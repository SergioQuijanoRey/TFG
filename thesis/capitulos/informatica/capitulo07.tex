\chapter{Conclusiones y futuras líneas de trabajo} \label{ich:conclusiones}

El objetivo del trabajo era el de estudiar dos variantes \textit{online} de \textit{Triplet Loss} para tratar de resolver una tarea de reconocimiento facial invariante a cambios en la edad. En este sentido, identificamos un problema en el diseño de estas variantes, proponemos una solución original a dicho problema y comprobamos su enorme eficacia experimentalmente. Por lo tanto, este objetivo ha sido alcanzado de forma satisfactoria.

Este proceso se ha desarrollado de la siguiente manera. En un primer experimento sobre los conjuntos de datos \textit{CACD} y \textit{FG-Net} obtenemos malos resultados, tal y como mostramos en la \tableref{table:estado_del_arte_y_mi_modelo}. Pese a que el conjunto \textit{FG-Net} presente una gran dificultad, los resultados están lejos de ser competentes. El estado del arte obtiene valores de \textit{Rank@1 accuracy} por encima del 90\% mientras que nuestro modelo no llega al 1\%. Además, en un segundo experimento sobre \textit{MNIST}, volvemos a obtener malos resultados, como mostramos en la \tableref{table:resultados_mnist_mal}. Por ello sospechamos de la existencia de un fallo en el diseño de las variantes que estamos estudiando. Descartamos que estos resultados estén causados por un fallo por nuestra parte explorando bases de código de terceros, en la \sectionref{isubsec:experiemntacion_base_codigo_externa}, y gracias a que nuestra extensa base de \textit{tests} que valida nuestras implementaciones, como explicamos en la \sectionref{isec:test_suite}.

Una vez hecho esto, nos centramos en estudiar la función de pérdida. En la \sectionref{isubsec:identificacion_problemas_propuesta_solucion} identificamos el problema de diseño que está ocasionando todos los problemas y proponemos una solución. Debido a la función de pérdida, el proceso de entrenamiento cae frecuentemente en un comportamiento degenerado. Dicha función tiene un término de margen para evitar que la red colapse todas las entradas al vector nulo. Pero durante el entrenamiento aprende a colapsar todas las entradas a un centro de gravedad, es decir, un vector fijo no nulo del espacio. Los procesos de entrenamiento y los resultados que mostramos en la experimentación previa respaldan nuestra teoría. Los comportamientos inusuales que se muestran quedan perfectamente explicados ahora. Principalmente el hecho de que la función de pérdida se quede atascada exactamente en el valor del margen, independientemente de que modifiquemos este valor.

Proponemos una solución original que consiste en introducir un término en la función de pérdida para penalizar las distancias ancla-negativo pequeñas. Validamos la efectividad de la propuesta experimentalmente en la \sectionref{isubsec:experimentacion_mnist_bien} y en la \sectionref{isubsec:experimentacion_cacd_bien}. Logramos mejoras significativas en las métricas que monitoreamos, incrementando su desempeño entre 2 y 30 veces, como muestran la \tableref{table:comparaciones_mnist_resultados} y la \tableref{table:comparaciones_cacd_resultados}. En concreto, para \textit{MNIST}, logramos mejoras del orden de magnitud de 10 veces para \textit{Rank@1 accuracy} y de 2 veces para \textit{Rank@5 accuracy}, tanto para entrenamiento como para \textit{test}. Y para \textit{CACD}, logramos mejoras en torno a 30 veces para entrenamiento tanto para \textit{Rank@1 accuracy} como para \textit{Rank@5 accuracy} y entorno a 10 veces para \textit{test}. Los resultados sobre \textit{CACD} quedan entorno a 0.37 para \textit{Rank@1} y 0.61 para \textit{Rank@5}, como muestra la \tableref{table:resultados_cacd_corregido}.

En definitiva, \textbf{hemos cumplido satisfactoriamente el objetivo planteado}. Hemos estudiado en profundidad las dos variantes de \textit{Triplet Loss} en un problema de reconocimiento facial invariante a la edad. En dicho estudio, hemos identificado un problema de diseño y propuesto una solución que supone grandes mejoras de rendimiento. Sin nuestra propuesta, estas técnicas no pueden ser aplicadas a ningún problema de cierta complejidad.

Además de la propuesta de solución original, durante el desarrollo del presente trabajo hemos adquirido un amplio conocimiento en ciertas áreas del aprendizaje automático, especialmente en el aprendizaje profundo. Este trabajo nos ha presentado una oportunidad única para estudiar en detalle algunos conceptos como \textit{embeddings} semánticos o distintas técnicas para entrenar redes siamesas. El desarrollo de una amplia base de código nos a permitido profundizar en la aplicación de patrones de diseño y código limpio poniendo en práctica los aprendizajes obtenidos a lo largo del doble grado. Es destacable haber aprendido en detalle uno de los \textit{frameworks} de referencia en el campo del aprendizaje automático: \textit{PyTorch}. En definitiva, ha resultado ser una experiencia de aprendizaje muy enriquecedora.

En base a todas las conclusiones obtenidas surgen varias líneas de trabajo muy interesantes. En primer lugar, gracias a disponer ahora sí de una función de pérdida que genera modelos con buen rendimiento, realizar una experimentación que busque mejorar los resultados obtenidos para el problema de reconocimiento facial invariante a la edad. Por ejemplo, probando con otras arquitecturas de red. Y en segundo lugar, probar y validar esta solución sobre otras áreas en las que se haya usado \textit{Triplet Loss} y en las que se hayan observado problemas parecidos a los que hemos enfrentado.
