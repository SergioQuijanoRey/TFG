% !TeX root = ../../libro.tex
% !TeX encoding = utf8

\chapter{Introducción}\label{ch:introduccion}

El objetivo de este trabajo, desde la perspectiva de las matemáticas, es \textbf{analizar las redes neuronales profundas, basadas en convoluciones, y explicar por qué en ciertos escenarios estas funcionan mejor que las redes neuronales no profundas}, conocidas en la literatura como redes neuronales \entrecomillado{shallow}.

En este escenario ocurre que para replicar redes profundas con un número polinomial de coeficientes, necesitamos que las redes \textit{shallow} tengan un número de coeficientes superior al polinomial (en nuestro modelo \textit{shallow}, el número de coeficientes será exponencial). En este escenario hablamos de \textbf{eficiencia en profundidad} o \textbf{\entrecomillado{depth efficiency}}.
\todo{Poner aquí una referencia a otra parte del paper sobre esto de tamaño exponencial}

El \textit{depth efficiency} es muy conocido en la práctica, pero pocos son los trabajos que han tratado de justificar matemáticamente por qué ocurre este fenómeno. Además, \textbf{en la mayoría de trabajos de este tipo, no se han tenido en cuenta propiedades fundamentales de las redes profundas ni de las redes convolucionales} \cite{matematicas:paper_depth_malo_01} \cite{matematicas:paper_depth_malo_02} \cite{matematicas:paper_depth_malo_03}. Por otro lado, suelen ser trabajos en los que \textbf{se muestran ejemplos concretos} de funciones implementables (de forma eficiente) con redes profundas pero no con redes \textit{shallow}, sin dar \textbf{ningún tipo de información sobre con qué frecuencia ocurre esto}. En base a esto, las \textbf{principales fortalezas de este trabajo respecto a otros} es que consideramos:
\todo{Aquí he copiado una frase de la introducción del trabajo, que quizás debería referenciar propiamente}

\begin{itemize}
    \item La diferencia jerárquica entre ambos tipos de redes
    \item Propiedades fundamentales de las redes convolucionales. Esto es:
        \begin{itemize}
            \item Compartición de los coeficientes en las convoluciones
            \item Localidad de la operación de convolución
            \item Uso de la operación de \textit{pooling}
        \end{itemize}
    \item Un estudio de lo frecuente que es tener funciones que sean implementables (de forma eficiente) por redes profundas, pero no por redes \textit{shallow}
\end{itemize}
\todo{Desarrollar más qué significan estas propiedades. En la introducción del paper se explica esto más o menos}

\section{Objetivos}

Los principales \textbf{objetivos} del presente trabajo son:

\begin{enumerate}
    \item Modelar matemáticamente la tarea de aprendizaje. Para esto, modelaremos los datos de entrada, la tarea a resolver y el espacio de hipótesis en el que buscaremos la mejor solución
    \item Modelar los dos tipos de redes neuronales (profundas y no profundas) a través de dos tipos de descomposiciones tensoriales:
        \begin{enumerate}
            \item Descomposición \textit{CP}, que equivale al uso de redes \textit{shallow}
            \item Descomposición \textit{HT}, que equivale al uso de redes profundas
        \end{enumerate}
    \item Demostrar dos resultados centrales que pondrán de manifiesto la superioridad de las redes profundas, en lo que se conoce como \textit{depth efficiency}, a través de las ya mencionadas descomposiciones tensoriales
    \todo{Debería hablar más sobre lo que dicen en concreto los resultados, aunque sea en un lenguaje sencillo y de forma resumida}
\end{enumerate}

Nos basaremos principalmente en el trabajo \cite{matematicas:principal}. Algunas de las herramientas que usaremos serán:

\begin{itemize}
    \item Teoría básica sobre tensores
    \item Teoría de la medida
    \item Álgebra de matrices
\end{itemize}

\section{Estructura del trabajo}

\todo{Escribir}

\endinput
