\chapter{Introducción}\label{ch:introduccion}

El objetivo de este trabajo, desde la perspectiva de las matemáticas, es \textbf{analizar las redes neuronales profundas, basadas en convoluciones, y explicar por qué en ciertos escenarios éstas funcionan mejor que las redes neuronales no profundas}, a las que llamaremos redes someras. En este escenario ocurre que para replicar redes profundas con un número polinomial de coeficientes, necesitamos que las redes someras tengan un número de coeficientes superior al polinomial, concretamente, exponencial. En este caso hablamos de \textbf{eficiencia en profundidad} o \textbf{\entrecomillado{depth efficiency}}.
\todo{Poner aquí una referencia a otra parte del paper sobre esto de tamaño exponencial}

La eficiencia en profundidad es muy conocida en la práctica, pero pocos son los trabajos que han tratado de justificar matemáticamente por qué ocurre este fenómeno. Además, como se comenta en \cite{matematicas:principal}, \textbf{en la mayoría de trabajos de este tipo, no se han tenido en cuenta propiedades fundamentales de las redes profundas ni de las redes convolucionales} \cite{matematicas:paper_depth_malo_01} \cite{matematicas:paper_depth_malo_02} \cite{matematicas:paper_depth_malo_03}. Por otro lado, suelen ser trabajos en los que \textbf{se muestran ejemplos concretos} de funciones implementables (de forma eficiente) con redes profundas pero no con redes someras, sin dar \textbf{ningún tipo de información sobre con qué frecuencia ocurre esto}. En base a esto, las \textbf{principales fortalezas de este trabajo respecto a otros} es que consideramos:
\todo{Comentarios de JMeri: este tipo de afirmaciones pueden llevar a preguntas del tribunal. Preparar bien las posibles respuestas}
\todo{Quizás yo añadir comentarios sobre esta afirmación para que se responda automáticamente al leer esto}

\begin{itemize}
	\item La diferencia jerárquica entre ambos tipos de redes.
	\item Propiedades fundamentales de las redes convolucionales: compartición de los coeficientes en las convoluciones, localidad de la operación de convolución y uso de la operación de \textit{pooling}.
	\item Un estudio de lo frecuente que es tener funciones que sean implementables (de forma eficiente) por redes profundas pero no por redes someras.
\end{itemize}
\todo{Desarrollar más qué significan estas propiedades. En la introducción del paper se explica esto más o menos}

\section{Objetivos}
\todo{No sé si me acaba de gustar que aquí haya un listado, quizás quedaría mejor redactado}

Los principales \textbf{objetivos} del presente trabajo son:

\begin{enumerate}
	\item Modelar matemáticamente la tarea de aprendizaje representando los datos de entrada, la tarea a resolver y el espacio de hipótesis en el que buscaremos la mejor solución.
	\item Modelar los dos tipos de redes neuronales (profundas y someras) a través de dos tipos de descomposiciones tensoriales:
	      \begin{itemize}
		      \item Descomposición \textit{CP}, que equivale al uso de redes someras  y que desarrollamos en la \sectionref{subs:descomposcion_cp}.
		      \item Descomposición \textit{HT}, que equivale al uso de redes profundas y que introducimos en la \sectionref{subs:descomposicion_ht}.
	      \end{itemize}
	\item Demostrar dos resultados centrales que pondrán de manifiesto la superioridad de las redes profundas, en lo que se conoce como eficiencia en profundidad, a través de las ya mencionadas descomposiciones tensoriales.
	      \begin{itemize}
		      \item El primer resultado central, el \propref{teorema:teorema_principal_especificacion}, dice que si tenemos un modelo profundo con un número polinomial de coeficientes, el modelo somero necesitará al menos un número exponencial de coeficientes para poder implementar el mismo modelo. Esto ocurre casi por doquier respecto al espacio de los coeficientes que determinan el modelo profundo.
		      \item El segundo resultado central, el \propref{corolario:corolario_principal_concreto}, indica que no es que un modelo somero no pueda realizar el modelo profundo con menos de un número exponencial de parámetros, sino que no es capaz ni de aproximar al modelo profundo con menos de un número exponencial de parámetros. De nuevo, esto ocurre casi por doquier respecto al espacio de los coeficientes que determinan el modelo profundo.
	      \end{itemize}
\end{enumerate}

Nos basaremos principalmente en el trabajo \cite{matematicas:principal}. Algunas de las herramientas que usaremos serán la teoría básica sobre tensores, las descomposiciones tensoriales \textit{CP} y \textit{HT}, la matrización de tensores, algunos resultados conocidos sobre la medida de Lebesgue y finalmente el álgebra de matrices básica.

\section{Estructura del trabajo}
\todo{No sé si me termina de gustar que ahora esto sea una lista}

Estructuraremos el trabajo de la siguiente manera:

\begin{itemize}
	\item En el \sectionref{ch:matematicas_fundamentales} introduciremos las herramientas básicas sobre las que fundamentaremos nuestro trabajo. En concreto, introduciremos el concepto de producto tensorial.
	\item En el \sectionref{ch:tarea_aprendizaje} indicaremos cómo modelizamos la tarea de aprendizaje. Modelizaremos un espacio de hipótesis general, para lo cual justificaremos la elección de ciertas funciones de puntuación, que serán clave en el desarrollo del trabajo, y la elección de ciertas funciones de representación. En la \sectionref{subs:capa_de_representacion} daremos un primer paso común en la modelización de ambas arquitecturas de aprendizaje automático.
	\item En el \sectionref{ch:modelizacion} desarrollaremos las dos modelizaciones (somera y profunda) a partir de dos descomposiciones tensoriales (\textit{CP} y \textit{HT}, respectivamente). Veremos en ambos casos cómo las modelizaciones realizadas se corresponden con arquitecturas usadas en la práctica del aprendizaje automático. Compararemos el número de parámetros que definen cada modelo, pues será fundamental en nuestros resultados teóricos.
	\item En el \sectionref{chapter:teoremas_y_demostraciones} introduciremos los dos resultados principales, veremos y demostraremos algunos resultados previos necesarios para las demostraciones, y probaremos el teorema y corolario principal.
	\item En el \sectionref{chapter:conclusiones_trabajo_futuro} comentaremos los resultados obtenidos a partir del presente trabajo e introduciremos posibles líneas de estudio en base a nuestros resultados.
\end{itemize}
