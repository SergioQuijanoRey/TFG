\chapter{Teoremas de la capacidad de las redes neuronales}

\section{Introducción a los teoremas}

Los teoremas principales de este trabajo se presentan en esta sección. Comenzamos por el siguiente:

\begin{teorema}[Darle nombre]

Sea $\mathcal{A}^y \in \espaciotensores{N}{M}$ dado por las ecuaciones \eqref{eq:descomposicion_ht}. Definamos $r := \min{r_0, M}$ y consideremos el espacio de todas las posibles configuraciones de parámetros de nuestro modelo $\{ \nv{a^{l, j, \gamma}} \}_{l, j, \gamma}$. En este espacio, el tensor generado $\mathcal{A}^y$ tendrá rango \textit{CP} de al menos $r^{N/2}$ casi por doquier. Es decir, el conjunto de tensores generados con rango \textit{CP} menor que $r^{N/2}$ tiene medida cero. El resultado se mantiene si forzamos la compartición de coeficientes en \eqref{eq:descomposicion_ht}. Es decir, haciendo $\nv{a^{l, \gamma}} \equiv \nv{a^{l, j, \gamma}}$ y considerando el espacio de configuraciones $\{ \nv{a^{l, \gamma}}  \}_{l, \gamma}$.

\end{teorema}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

\begin{observacion}

    Casi todos los tensores que podemos generar con un modelo \textit{HT} tienen rango \textit{CP} de al menos $r^{N/2}$.

\end{observacion}

\begin{observacion}
    $\mathcal{A}^y$ claramente es realizable, por ser el resultado de un modelo \textit{HT}
\end{observacion}

\begin{observacion}

    Que $\mathcal{A}^y$ tenga rango \textit{CP} igual a $r^{N/2}$ implica que en la ecuación \eqref{eq:cp_decomp}, el número de sumandos deberá ser al menos $r^{N/2}$. Luego, conforme el tamaño de la imagen $N$ aumenta, el número de parámetros del modelo \textit{CP} crece exponencialmente. Conforme el número de canales $M$ de la imagen crece, el número de parámetros del modelo \textit{CP} crece pero de forma potencial.

\end{observacion}

A partir de este teorema, obtenemos el siguiente corolario:

\begin{corolario}[Darle nombre] \label{corol:corol_principal}

Dado un conjunto de funciones de representación $\{f_{\theta_d}: d \in \deltaset{M} \}$ linealmente independientes, aleatorizando los pesos de un modelo \textit{HT} \eqref{eq:descomposicion_ht} a partir de una distribución de probabilidad continua, induce funciones de puntuación $h_y$ que con probabilidad uno no pueden ser aproximadas arbitrariamente bien (en el sentido $L^2$) por un modelo \textit{CP} con menos de $r := \min \{r_0, M \}^{N/2}$ canales. Este resultado se mantiene forzando la compartición de coeficientes en el modelo \textit{HT} mientras que dejamos el modelo \textit{CP} sin restricciones.

\end{corolario}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

Es decir, salvo un conjunto de medida nula, todos los tensores realizables por una descomposición \textit{HT} no pueden realizarse, ni siquiera aproximarse, por una descomposición \textit{CP} con menos de un número exponencial de parámetros.

El \customref{corol:corol_principal} introduce una mejora muy grande respecto a los trabajos previos. En estos, se aportan ejemplos de funciones realizables por el modelo \textit{HT} que no son realizables por un modelo \textit{CP} con un número de parámetros sustancialmente superior al número de parámetros del modelo \textit{HT}. Es decir, establecen la \textit{depth efficiency} de ejemplos concretos de funciones. Nuestro corolario establece la \textit{depth efficiency} casi por doquier.

Estos resultados también mejoran los trabajos previos sobre análisis tensorial. El trabajo que introduce la descomposición \textit{HT} \cite{matematicas:descomposicion_ht} da ejemplos específicos de tensores realizables por el modelo \textit{HT} y no por un modelo \textit{CP} con un número exponencial de parámetros. Nuestro corolario indica que esto no ocurre en ejemplos aislados, sino en casi todos los tensores realizables por un modelo \textit{HT}.

Teniendo en cuenta que cualquier tensor realizable por un modelo \textit{CP} puede ser realizado por un modelo \textit{HT} con un aumento polinomial del número de coeficientes (véase \customref{msubs:parametros_modelo_ht}), esto implica que la descomposición \textit{HT} es asintóticamente más eficiente que la \textit{CP}.
\todo{Aquí no he definido lo que entendemos por eficiencia. Creo que sería \textit{depth efficiency}}

\section{Lemas previos}

En esta sección introduciremos algunos resultados que serán usados en las demostraciones del teorema \customref{} y el corolario \customref{}. Empezamos con los siguientes resultados de teoría de la medida de Lebesgue:

\begin{lema}[Propiedades fundamentales de la medida] \label{lema:prop_fundamentales_medida} Las siguientes propiedades sobre la medida de Lebesgue son ciertas:

\begin{enumerate}
    \item La unión contable de conjuntos de medida nula es un conjunto de medida nula
    \item Si $A \subseteq \R^{N_1}$ tiene medida nula, entonces $A \times R^{N_2} \subseteq R^{N_1 + N_2}$ también tiene medida nula
    \item Sea $p \in \R^d[x]$, es decir, un polinomio sobre los números reales con $d$ variables. Si $p$ no es el polinomio idénticamente nulo, entonces el conjunto de ceros del polinomio es un conjunto de medida nula \cite{informatica:zeros_of_polynomial}
\end{enumerate}

\end{lema}

Usaremos la siguiente \textbf{matrización} de un tensor:

\begin{definicion}[Matrización de un tensor]

Sea $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$ un tensor de orden $N$ y dimensión $M_i, i \in \deltaset{N}$ en cada modo. Por simplicidad, supondremos que el orden del tensor es par. Denotaremos por $[\mathcal{A}]$ a la \textbf{matrización} del tensor $\mathcal{A}$. $[\mathcal{A}]$ es una matriz donde las filas corresponden a modos impares del tensor, y las columnas corresponden a los modos pares. Por lo tanto, $[\mathcal{A}]$ tiene $M_1 \cdot M_3 \cdot \ldots \cdot M_{N-1}$ filas y $M_2 \cdot M_4 \cdot \ldots \cdot M_N$ columnas. Asignaremos el valor $\mathcal{A}_{d_1, \ldots, d_N}$ en $[\mathcal{A}]$ en la posición:

\begin{itemize}
    \item Fila $1 + \sum_{i = 1}^{N/2} (d_{2i - 1} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j - 1}$
    \item Columna $1 + \sum_{i = 1}^{N/2} (d_{2i} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j}$
\end{itemize}

\end{definicion}

Con esto, tenemos que saber cómo se comporta la matrización con el producto tensorial. Esto lo introduce la siguiente proposición:

\begin{definicion}[Producto de Kronecker]

    Sean $A \in \espaciomatrices{N}{M}$ y  $B \in \espaciomatrices{P}{Q}$ dos matrices cualesquiera. Se define su producto de Kronecker $A \odot B$ como la matriz en $\espaciomatrices{N \cdot P}{M \cdot Q}$ dada por:

    \begin{equation}
        (A \odot B)_{i,j} := (A_{i, j} B)_{i, j}
    \end{equation}

    Donde estamos usando una notación por bloques.
\end{definicion}

\begin{ejemplo}[Producto de Kronecker con dos matrices sencillas]

Sean las siguientes matrices:

    \begin{equation}
        A := \begin{pmatrix}
            a_{11} & a_{12} \\
            a_{21} & a_{22} \\
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        B := \begin{pmatrix}
            b_{11} & b_{12} \\
            b_{21} & b_{22} \\
        \end{pmatrix}
    \end{equation}

    Entonces, la matriz $A \odot B$ se computa de la siguiente forma:

    \begin{equation}
        (A \odot B) = \begin{pmatrix}
            a_{11}B & a_{12}B \\
            a_{21}B & a_{22}B \\
        \end{pmatrix} =
        \begin{pmatrix}
            a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
            a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
            a_{21} b_{11} & a_{11} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
            a_{21} b_{21} & a_{11} b_{22} & a_{22} b_{21} & a_{22} b_{22} \\
        \end{pmatrix}
    \end{equation}

\end{ejemplo}

\begin{observacion}
    El anterior ejemplo nos muestra cómo con el producto de Kronecker podemos operar dos matrices cualesquiera, independientemente de sus dimensiones.
\end{observacion}

Vemos ahora tres propiedades fundamentales:

\begin{proposicion}[Propiedades fundamentales] Las siguientes afirmaciones son ciertas:

    \begin{enumerate}
        \item Matrización y producto tensorial: Sean $\mathcal{A}, \mathcal{B}$ dos tensores. Entonces se verifica que $[\mathcal{A} \otimes \mathcal{B}] = [\mathcal{A}] \odot [\mathcal{B}]$ \label{prop:prop_fundamentales_primera}
        \item Rangos matriciales y producto de Kronecker: sean $A, B$ dos matrices. Entonces se verifica que $rank(A \odot B) = rank(A) \cdot rank(B)$
        \item Linealidad de la matrización: sean $\mathcal{A}_1, \ldots, \mathcal{A}_N$ tensores del mismo orden y mismas dimensiones, y $\alpha_1, \ldots, \alpha_N \in \R$, entonces $[\sum_{i = 1}^N \alpha_i \cdot \mathcal{A}_i] = \sum_{i = 1}^N \alpha_i \cdot [\mathcal{A}_i]$
    \end{enumerate}

\end{proposicion}

\begin{observacion} A partir de \customref{prop:prop_fundamentales_primera} es directo ver que el producto de Kronecker verifica las mismas propiedades que el producto tensorial (véase \customref{prop:tensores_propiedades}).
\end{observacion}

\begin{proposicion}[Rango matricial de la descomposición \textit{CP}]

    \begin{equation}
        rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = Z
    \end{equation}

    Es decir, un tensor de orden $2^L$ dado como descomposición \textit{CP} con $Z$ términos tiene una matrización de rango matricial como mucho $Z$.

\end{proposicion}

\begin{proof}

    Empezamos viendo la siguiente igualdad:

    \begin{equation}
    \begin{split}
        rank([\nv{v_1^{(z)}}  \otimes \ldots \otimes \nv{v_{2^L}^{(z)}}]) &= \ldots \text{  agrupo dos a dos   } \ldots = \\
        \ldots &= \prod_{i = 1}^{2^{L/2}} rank([\nv{v_{2i - 1}^{(z)}} \otimes\nv{v_{2i}^{(z)}}]) = \ldots   \\
        \ldots &= \prod_{i = 1}^{2^{L/2}} rank(\nv{v_{2i - 1}^{(z)}} \nv{v_{2i}^{(z)}}^T) = \ldots \\
        \ldots &= \prod_{i = 1}^{2^{L/2}} 1 = 1
    \end{split}
    \end{equation}

    Usamos esto para calcular nuestra ecuación

    \begin{equation}
    \begin{split}
        rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) &\encima{=}{linealidad} rank(\sum_{z = 1}^Z \lambda_z [\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) \leq \ldots \\
        \ldots &\leq \sum_{z = 1}^Z rank([\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = \sum_{z = 1}^Z 1 = Z
    \end{split}
    \end{equation}
    \todo{No sé de donde sale la desigualdad!}
    \todo{No estoy explicando qué propiedades fundamentales que acabo de introducir uso en la prueba}
\end{proof}

\begin{proposicion}
    Dado $r \in \R$, el siguiente conjunto es cerrado:

    \begin{equation}
        \conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r }
    \end{equation}
\end{proposicion}

\begin{proof}
    \todo{Importante, es parte importante del trabajo}
\end{proof}



\section{Demostración de los teoremas}

Necesitaremos probar dos lemas que serán fundamentales en la prueba del teorema.

\begin{lema}

    Sean $M, N \in \N$, $\nv{x} \in \R^{2MN + N}$ y definimos tres funciones que llevan $\nv{x}$ a una matriz:

    \begin{enumerate}
        \item $A(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los primeros $MN$ elementos de $\nv{x}$
        \item $B(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los siguientes $MN$ elementos de $\nv{x}$
        \item $D(\nv{x}) \in \espaciomatrices{N}{N}$ es una matriz diagonal con los últimos $N$ elementos de $\nv{x}$ en la diagonal
    \end{enumerate}

    Definimos con ello la matriz $U(\nv{x}) := A(\nv{x}) D(\nv{x}) B(\nv{x})^T \in \espaciomatrices{M}{M}$. Entonces el conjunto de puntos $\nv{x} \in \R^{2MN + N}$ para los cuales el rango de $U(\nv{x})$ es distinto de $r := \min \{M, N\}$ tienen medida nula.

    El resultado sigue siendo cierto si consideramos $\nv{x} \in \R^{MN+N}$ y cambiamos la asignación de $B(x)$ por $B(x) := A(x)$.
\end{lema}

\begin{proof}
    Sin pérdida de generalidad, supondré que $M \leq N$, luego $r = M$. Como $U(\nv{x}) \in \espaciomatrices{M}{M}$ es claro que

    \begin{equation}
        rank(U(\nv{x})) \leq M \leq r := \min \{N, M \}; \dspace \forall \nv{x} \in \R^{2MN + N}
    \end{equation}

    Por tanto, queda probar que $rank(U(x)) \geq r; \dspace \text{cpd } x \in \R^{2MN + N}$. Definimos $U_r(x)$ como la submatriz superior izquierda $r \times r$ de $U(x)$.

    Si fuese $U_r(x)$ no singular para cierto $x$, claramente sabríamos que $r = rank(U_r(x)) \leq rank(U(x))$. Por tanto, buscamos probar que $U_r(x)$ es no singular salvo un conjunto de medida nula, es decir, que el conjunto $\conjunto{x \in \R^{2MN + N} : det(U_r(x)) = 0 }$ tiene medida nula.

    Para ver esto tenemos en cuenta que $det(U_r(\nv{x}))$ es un polinomio con entradas en $x \in \R^{2MN + N}$. Por \customref{lema:prop_fundamentales_medida} sabemos que entonces o bien es el polinomio idénticamente nulo o bien el conjunto de sus ceros tiene medida nula. Basta ver por tanto que no es el polinomio idénticamente nulo.

    Para ello basta ver que $\exists \nv{x_0} \in \R^{2MN + N}$ de forma que $det(U_r(\nv{x})) \neq 0$. Para ello, tomamos $x_0$ de forma que $D(x_0) = \mathbb{I}_d$ y que $A(\nv{x_0}) = B(\nv{x_0}) = (\delta_{i,j})_{i,j}$. Es decir, que $A(\nv{x_0}), B(\nv{x_0})$ son matrices, no necesariamente cuadradas, con unos en la diagonal y ceros en el resto de posiciones. En esta situación tenemos:

    \begin{equation}
    \begin{split}
        U_r(\nv{x_0}) &= \begin{pmatrix}
            \mathbb{I}d_{r \times r} & \vline & 0 \\
        \end{pmatrix}_{M \times N}
        \mathbb{I}d_{N \times N}
        \begin{pmatrix}
            \mathbb{I}d_{r \times r} & \vline & 0 \\
        \end{pmatrix}^T_{N \times M} = \ldots \\
        \ldots &= \begin{pmatrix}
            \mathbb{I}d_{r \times r} & \vline & 0 \\
        \end{pmatrix}_{M \times N}
        \mathbb{I}d_{N \times N}
        \begin{pmatrix}
            0 \\
            \hline
            \mathbb{I}d_{r \times r}
        \end{pmatrix}{N \times M} = \ldots \\
        \ldots &= \begin{pmatrix}
            \mathbb{I}d_{r \times r} & \vline & 0 \\
        \end{pmatrix}_{M \times N}
        \begin{pmatrix}
            0 \\
            \hline
            \mathbb{I}d_{r \times r}
        \end{pmatrix}_{N \times M} = \ldots \\
        \ldots &= \mathbb{I}d_M
    \end{split}
    \end{equation}

    Por tanto, es claro que

    \begin{equation}
        det(U_r(\nv{x_0})) = det(\mathbb{I}d_M) = 1 \neq -1
    \end{equation}

    como buscábamos probar.
\end{proof}

Veamos ahora el segundo lema:

\begin{lema}
    Sean $p$ funciones continuas de $\R^d$ a $\espaciomatrices{M}{N}$, que llevan un punto $\nv{y} \in \R^d$ a las matrices $A_1(\nv{y}), \ldots, A_p(\nv{y})$. Supongamos que con estas funciones el siguiente conjunto tiene medida nula:

    \begin{equation}
        \conjunto{\nv{y} \in \R^d : rank(A_i(\nv{y})) < r, \dspace \forall i \in \deltaset{p}}
    \end{equation}

    Definimos la función

    \begin{equation}
    \begin{split}
        A: \R^P \times R^d &\to \espaciomatrices{M}{N} \\
        (x, y) &\mapsto \sum_{i = 1}^p x_i A_i(\nv{y})
    \end{split}
    \end{equation}

    Entonces el conjunto de puntos $(x, y) \in \R^p \times \R^d$ para los cuales $A(x, y) < r$ forman un conjunto de medida nula
\end{lema}

\begin{proof}

    Definimos el conjunto

    \begin{equation}
        S := \conjunto{(\nv{x}, \nv{y}) \in  \R^p \times \R^d: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p \times \R^d
    \end{equation}

    y lo que buscamos es probar que tiene medida nula.

    Como $A(x, y)$ es continua, y $\conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r}$ es cerrado, entonces sabemos que $S$ es cerrado, y por tanto, medible.

    Ahora buscamos calcular la medida de $S$. Empezamos definiendo, para cada $y \in \R^d$, el siguiente conjunto marginal:

    \begin{equation}
        S^{\nv{y}} := \conjunto{x \in \R^p: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p
    \end{equation}

    Con esto, basta probar que la medida de $S^{\nv{y}}$ es nula casi por doquier. Para ello definimos también

    \begin{equation}
        \mathcal{C} := \conjunto{\nv{y} \in \R^d: rank(A_i(y)) < r, \forall i \in \deltaset{p}}
    \end{equation}

    Por las hipótesis del lema, sabemos que $\mathcal{C}$ tiene medida nula. Luego queda ver que para $\nv{y_0} \in S - \mathcal{C}$ el conjunto $S^{\nv{y_0}}$ tiene medida nula.

    \begin{equation}
    \begin{split}
        \nv{y_o} \in S - \mathcal{C} \encima{\then}{\nv{y_o} \notin \mathcal{C}} \substack{\exists i \in \deltaset{p}: \\ rank(A_i(\nv{y_o})) \geq r}
    \end{split}
    \end{equation}

    Sin pérdida de generalidad, supongo que $i = 1$ y que la submatriz $r \times r$ superior izquierda de $A_1(\nv{y_0})$ es no singular. Viendo $\nv{y_o}$ como un valor fijo, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y})$ es un polinomio en $\nv{x}$. Además, no es el polinomio idénticamente nulo, porque tomando $\nv{x} = (1, \ldots, 0)$ tenemos que

    \begin{equation}
        A(\nv{x}, \nv{y_0}) = \sum_{i = 1}^{p} x_i A_i(\nv{y_0}) = A_1(\nv{y_0})
    \end{equation}

    que no es singular, luego su determinante no se anula. Por tanto, usando \customref{lema:prop_fundamentales_medida}, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y})$ es un polinomio cuyo conjunto de ceros tiene medida nula. Y por lo tanto, el conjunto $S^{\nv{y_0}}$ tiene medida nula, porque casi todas las matrices $A(\nv{x}, \nv{y_0})$ tienen al menos rango $r$, como acabamos de ver.

    Ahora, usaremos el teorema de Fubini y el teorema de la convergencia monótona para demostrar lo que buscamos. $\mathbb{1}_A$ denotará la función indicadora del subconjunto $A$. Definimos los siguientes conjuntos:

    \begin{equation}
    \begin{split}
        S_n &:= S \interseccion [-n, n]^{p+d} \\
        \R_n &:= \R \interseccion [-n, n]^{p+d} = [-n, n]^{p+d}
    \end{split}
    \end{equation}

    Todos los conjuntos con los que estamos trabajando son medibles. Al intersecar con $[-n, n]^{p+d}$, además, tendrán medida finita. Aplicamos ahora el teorema de Fubini para desarrollar:

    \begin{equation}
    \begin{split}
        \int_{(\nv{x}, \nv{y}) \in \R^{p+d}} \mathbb{1}_{S_n} &= \int_{(\nv{x},\nv{y}) \in \R_n^{p+d}} \mathbb{1}_{S} \encima{=}{Fubini} \int_{\nv{y} \in \R_n^d} \int_{\nv{x} \in \R_n^p} \mathbb{1}_{S^y} = \ldots \\
        \ldots &= \comentardebajo{\int_{\nv{y} \in \R_n^d \interseccion \mathcal{y}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(1)} + \comentardebajo{\int_{\nv{y} \in \R_n^d - \mathcal{C}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(2)} = \ldots \\
        \ldots &= 0 + 0 = 0
    \end{split}
    \end{equation}
    \todo{Habría que comprobar los supuestos de Fubini}

    Donde hemos usado:

    \begin{itemize}
        \item (1): hemos visto que la medida del conjunto $\mathcal{C}$ es cero
        \item (2): hemos visto que si $\nv{y_0} \in \R_n^d$, entonces el conjunto $S^{y_0}$ tiene medida nula
    \end{itemize}

    Y con ello hemos probado que $\int \mathbb{1}_{S_n} = 0$, sea cual sea el valor de $n \in \N$. Usamos ahora el \textbf{teorema de convergencia monótona} para calcular:
    \todo{Comprobar explícitamente los supuestos del TCM}

    \begin{equation}
        \int \mathbb{1}_S = \int \lim_{n \to \infty} \mathbb{1}_{S_n} \encima{=}{T.C.M} \lim_{n \to \infty} \int \mathbb{1}_{S_n} = \lim_{n \to \infty} 0  = 0
    \end{equation}

    Por lo tanto, la medida del conjunto $S$ es cero, como queríamos probar.

\end{proof}
