\chapter{Teoremas de la capacidad de las redes neuronales} \label{chapter:teoremas_y_demostraciones}

\section{Introducción a los resultados principales}

En esta sección introduciremos los dos resultados principales del trabajo. En el resto de secciones trabajaremos en demostrar estos dos resultados.

El teorema principal del trabajo es el siguiente:

\begin{teorema}[Rango \textit{CP} exponencial de un modelo \textit{HT}] \label{teorema:teorema_principal_introduccion}

Sea $\mathcal{A}^y \in \espaciotensores{N}{M}$ dado por las ecuaciones \eqref{eq:descomposicion_ht}. Definamos $r := \min \conjunto{r_0, M}$ y consideremos el espacio de todas las posibles configuraciones de parámetros de nuestro modelo \textit{HT} $\conjunto{ \nv{a^{l, j, \gamma}}}_{l, j, \gamma}$. En este espacio, el tensor generado $\mathcal{A}^y$ tendrá rango \textit{CP} de al menos $r^{N/2}$ casi por doquier. Es decir, el conjunto de parámetros del modelo \textit{HT} con los que el modelo tiene rango \textit{CP} menor que $r^{N/2}$ tiene medida nula. El resultado se mantiene si forzamos la compartición de coeficientes en \eqref{eq:descomposicion_ht}. Es decir, haciendo $\nv{a^{l, \gamma}} \equiv \nv{a^{l, j, \gamma}}$ y considerando el espacio de configuraciones $\{ \nv{a^{l, \gamma}}  \}_{l, \gamma}$.

\end{teorema}

En resumen, casi todos los tensores $\mathcal{A}^y$ que podemos generar con un modelo \textit{HT} tienen rango \textit{CP} de al menos $r^{N/2}$. Esto implica que en la ecuación de la descomposición \textit{CP} \eqref{eq:cp_decomp}, el número de sumandos deberá ser al menos $r^{N/2}$. Luego, conforme el tamaño de la imagen $N$ aumenta, el número de parámetros del modelo \textit{CP} crece exponencialmente. Y cuando el número de canales $M$ aumenta, el número de paráemtros del modelo \textit{CP} crece de forma potencial.

A partir de este teorema, obtenemos el corolario, que es el segundo resultado principal en nuestro trabajo:

\begin{corolario}[Incapacidad del modelo \textit{CP} para aproximar eficientemente el modelo \textit{HT}] \label{corol:corol_principal}

    Dado un conjunto de funciones de representación linealmente independientes

    \begin{equation}
        \conjunto{f_{\theta_d}: \; d \in \deltaset{M}}
    \end{equation}

     aleatorizar los pesos de un modelo \textit{HT} \eqref{eq:descomposicion_ht} a partir de una distribución de probabilidad continua induce funciones de puntuación $h_y$ que con probabilidad uno no pueden ser aproximadas arbitrariamente bien (en el sentido $L^2$) por un modelo \textit{CP} con menos de $r := \min \{r_0, M \}^{N/2}$ canales. Este resultado se mantiene forzando la compartición de coeficientes en el modelo \textit{HT} mientras que dejamos el modelo \textit{CP} sin restricciones.

\end{corolario}

El teorema anterior nos afirmaba que casi todos los tensores realizables por una descomposición \textit{HT} no pueden realizarse eficientemente por un modelo \textit{CP}. El corolario añade a este hecho que ni siquiera pueden aproximarse eficientemente (menos de un número exponencial de coeficientes) por una descomposición \textit{CP}.

El corolario \customref{corol:corol_principal} introduce una mejora muy grande respecto a los trabajos previos. En estos, se aportan ejemplos de funciones realizables por el modelo \textit{HT} que no son realizables por un modelo \textit{CP} con un número de parámetros sustancialmente superior al número de parámetros del modelo \textit{HT}. Es decir, establecen la \textit{depth efficiency} de ejemplos concretos de funciones. Nuestro corolario establece la \textit{depth efficiency} casi por doquier.

Estos resultados también mejoran los trabajos previos sobre análisis tensorial. El trabajo que introduce la descomposición \textit{HT} \cite{matematicas:descomposicion_ht} da ejemplos específicos de tensores realizables por el modelo \textit{HT} y no por un modelo \textit{CP} con un número exponencial de parámetros. Nuestro corolario indica que esto no ocurre únicamente en ejemplos aislados, sino en casi todos los tensores realizables por un modelo \textit{HT}.

Teniendo en cuenta que cualquier tensor realizable por un modelo \textit{CP} puede ser realizado por un modelo \textit{HT} con un aumento polinomial del número de coeficientes (véase \customref{msubs:parametros_modelo_ht}), esto implica que la descomposición \textit{HT} es asintóticamente más eficiente que la \textit{CP}.

\section{Lemas previos} \label{sec:lemas_previos}

En esta sección introduciremos algunos resultados que serán usados en las demostraciones del teorema \customref{teorema:teorema_principal_especificacion} y el corolario \customref{corolario:corolario_principal_concreto}. Empezamos con los siguientes resultados de teoría de la medida de Lebesgue:

\begin{lema}[Propiedades fundamentales de la medida] \label{lema:prop_fundamentales_medida} Las siguientes propiedades sobre la medida de Lebesgue son ciertas:

\begin{enumerate}
    \item La unión contable de conjuntos de medida nula es un conjunto de medida nula
    \item Si $A \subseteq \R^{N_1}$ tiene medida nula, entonces $A \times R^{N_2} \subseteq R^{N_1 + N_2}$ también tiene medida nula
    \item Sea $p \in \R^d[x]$, es decir, un polinomio sobre los números reales con $d$ variables. Si $p$ no es el polinomio idénticamente nulo, entonces el conjunto de ceros del polinomio es un conjunto de medida nula \cite{informatica:zeros_of_polynomial}
\end{enumerate}

\end{lema}

Usaremos la siguiente \textbf{matrización} de un tensor:

\begin{definicion}[Matrización de un tensor]

Sea $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$ un tensor de orden $N$ y dimensión $M_i$ en cada modo $i \in \deltaset{N}$. Por simplicidad, supondremos que el orden del tensor es par. Denotaremos por $[\mathcal{A}]$ a la \textbf{matrización} del tensor $\mathcal{A}$, que será una matriz donde las filas corresponden a modos impares del tensor, y las columnas corresponden a los modos pares. Por lo tanto, $[\mathcal{A}]$ tiene $M_1 \cdot M_3 \cdot \ldots \cdot M_{N-1}$ filas y $M_2 \cdot M_4 \cdot \ldots \cdot M_N$ columnas. Asignaremos el valor de $\mathcal{A}_{d_1, \ldots, d_N}$ en $[\mathcal{A}]$ en la posición:

\begin{itemize}
    \item Fila $1 + \sum_{i = 1}^{N/2} (d_{2i - 1} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j - 1}$
    \item Columna $1 + \sum_{i = 1}^{N/2} (d_{2i} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j}$
\end{itemize}

\end{definicion}

Queremos saber cómo se comporta la matrización con el producto tensorial. Para ello, será fundamental la siguiente herramienta:

\begin{definicion}[Producto de Kronecker]

    Sean $A \in \espaciomatrices{N}{M}$ y  $B \in \espaciomatrices{P}{Q}$ dos matrices cualesquiera. Se define su producto de Kronecker $A \odot B$ como la matriz en $\espaciomatrices{N \cdot P}{M \cdot Q}$ dada por:

    \begin{equation}
        (A \odot B)_{i,j} := (A_{i, j} B)_{i, j}
    \end{equation}

    Donde estamos usando una notación por bloques.
\end{definicion}

\begin{ejemplo}[Producto de Kronecker con dos matrices sencillas]

Sean las siguientes matrices:

    \begin{equation}
        A := \begin{pmatrix}
            a_{11} & a_{12} \\
            a_{21} & a_{22} \\
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        B := \begin{pmatrix}
            b_{11} & b_{12} \\
            b_{21} & b_{22} \\
        \end{pmatrix}
    \end{equation}

    Entonces, la matriz $A \odot B$ se computa de la siguiente forma:

    \begin{equation}
        (A \odot B) = \begin{pmatrix}
            a_{11}B & a_{12}B \\
            a_{21}B & a_{22}B \\
        \end{pmatrix} =
        \begin{pmatrix}
            a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
            a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
            a_{21} b_{11} & a_{11} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
            a_{21} b_{21} & a_{11} b_{22} & a_{22} b_{21} & a_{22} b_{22} \\
        \end{pmatrix}
    \end{equation}

\end{ejemplo}

\begin{observacion}
    El anterior ejemplo nos muestra cómo con el producto de Kronecker podemos operar dos matrices cualesquiera, independientemente de sus dimensiones.
\end{observacion}

Vemos ahora tres propiedades fundamentales que tratan sobre la matrización, producto de Kronecker y el producto tensorial

\begin{proposicion}[Propiedades fundamentales de la matrización y producto de Kronecker] Las siguientes afirmaciones son ciertas:

    \begin{enumerate}
        \item Matrización y producto tensorial: Sean $\mathcal{A}, \mathcal{B}$ dos tensores. Entonces se verifica que $[\mathcal{A} \otimes \mathcal{B}] = [\mathcal{A}] \odot [\mathcal{B}]$ \label{prop:prop_fundamentales_primera}
        \item Rangos matriciales y producto de Kronecker: sean $A, B$ dos matrices. Entonces se verifica que $rank(A \odot B) = rank(A) \cdot rank(B)$
        \item Linealidad de la matrización: sean $\mathcal{A}_1, \ldots, \mathcal{A}_N$ tensores del mismo orden y mismas dimensiones, y $\alpha_1, \ldots, \alpha_N \in \R$, entonces $[\sum_{i = 1}^N \alpha_i \cdot \mathcal{A}_i] = \sum_{i = 1}^N \alpha_i \cdot [\mathcal{A}_i]$
    \end{enumerate}

\end{proposicion}

\begin{observacion} A partir de \customref{prop:prop_fundamentales_primera} es directo ver que el producto de Kronecker verifica las mismas propiedades que el producto tensorial (véase \customref{prop:tensores_propiedades}).
\end{observacion}

La siguiente proposición será clave a la hora de desarrollar nuestras demostraciones:

\begin{proposicion}[Rango matricial de la descomposición \textit{CP}] \label{prop:rango_matricial_descomp_cp}

    \begin{equation}
        rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = Z
    \end{equation}

    Es decir, un tensor de orden $2^L$ dado como descomposición \textit{CP} con $Z$ términos tiene una matrización de rango matricial como mucho $Z$.

\end{proposicion}

\begin{proof}

    Empezamos viendo la siguiente igualdad:

    \begin{equation}
        \begin{split}
            rank([\nv{v_1^{(z)}}  \otimes \ldots \otimes \nv{v_{2^L}^{(z)}}]) &= \ldots \text{  agrupo dos a dos   } \ldots = \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} rank([\nv{v_{2i - 1}^{(z)}} \otimes\nv{v_{2i}^{(z)}}]) = \ldots   \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} rank(\nv{v_{2i - 1}^{(z)}} \nv{v_{2i}^{(z)}}^T) = \ldots \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} 1 = 1
        \end{split}
    \end{equation}

    Usamos esto para calcular nuestra ecuación

    \begin{equation}
        \begin{split}
            rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) &\encima{=}{linealidad} rank(\sum_{z = 1}^Z \lambda_z [\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) \leq \ldots \\
            \ldots &\leq \sum_{z = 1}^Z rank([\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = \sum_{z = 1}^Z 1 = Z
        \end{split}
    \end{equation}
    \todo{No sé de donde sale la desigualdad!}
    \todo{No estoy explicando qué propiedades fundamentales que acabo de introducir uso en la prueba}
\end{proof}

La siguiente proposición será necesaria para ver un detalle técnico en la prueba de \customref{lema:segundo_lema}:

\begin{proposicion}
    Dado $r \in \R$, el siguiente conjunto es cerrado:

    \begin{equation}
        \conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r }
    \end{equation}
\end{proposicion}

\begin{proof}
    Defino $\zeta := \conjunto{M \in \espaciomatrices{M}{N}: \; rank(M) < r}$ y busco probar que todas las sucesiones de elementos de $\zeta$ que convergen lo hacen a un elemento de $\zeta$. Es decir

    \begin{equation}
        \begin{split}
            \conjunto{M_n}_{n \in \N} \subseteq \zeta \\
            \conjunto{M_n} \to M \in \espaciomatrices{M}{N} \\
        \end{split}
        \then M \in \zeta
    \end{equation}

    Sea $A_n$ un menor cualquiera de orden $r$ de $M_n$. Sea $A$ el menor de orden $r$ de $M$ asociado a las mismas entradas que el orden que hemos tomado en $M_n$. Entonces es claro que

    \begin{equation}
        \conjunto{A_n} \to A
    \end{equation}

    Como el determinante es una función continua, es claro entonces que

    \begin{equation}
        \conjunto{0}_{n \in \N} = \conjunto{det(A_n)} \to det(A)
    \end{equation}

    Y con ello, hemos probado que $det(A) = 0$. Como podemos hacer esto con todos los posibles menores de orden $r$, hemos probado que $M$ tiene rango menor que $r$, y por tanto $M \in \zeta$.
\end{proof}

\section{Demostración del teorema principal}

Los dos siguientes \textbf{lemas son centrales} en la prueba del teorema principal, y por ello los presentamos aquí y no en \customref{sec:lemas_previos}. Veamos el primero:

\begin{lema}[Primer lema fundamental, rango de una matrización de la forma $A D B^T$] \label{lema:primer_lema}

    Sean $M, N \in \N$, $\nv{x} \in \R^{2MN + N}$ y definimos tres funciones $A, B, D$ que llevan $\nv{x}$ a una matriz de la siguiente forma:

    \begin{enumerate}
        \item $A(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los primeros $MN$ elementos de $\nv{x}$
        \item $B(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los siguientes $MN$ elementos de $\nv{x}$
        \item $D(\nv{x}) \in \espaciomatrices{N}{N}$ es una matriz diagonal con los últimos $N$ elementos de $\nv{x}$ en la diagonal
    \end{enumerate}

    Definimos con ello la función $U$ que lleva $\nv{x}$ a la matriz $U(\nv{x}) := A(\nv{x}) D(\nv{x}) B(\nv{x})^T \in \espaciomatrices{M}{M}$. Entonces el conjunto de puntos $\nv{x} \in \R^{2MN + N}$ para los cuales el rango de $U(\nv{x})$ es distinto de $r := \min \{M, N\}$ tienen medida nula.

    El resultado sigue siendo cierto si consideramos $\nv{x} \in \R^{MN+N}$ y cambiamos la asignación de $B(x)$ por $B(x) := A(x)$.
\end{lema}

\begin{proof}
    \textbf{Caso General}

    Sin pérdida de generalidad, supondré que $M \leq N$, luego $r = M$. Como $U(\nv{x}) \in \espaciomatrices{M}{M}$ es claro que

    \begin{equation}
        rank(U(\nv{x})) \leq M \leq r := \min \{N, M \}; \dspace \forall \nv{x} \in \R^{2MN + N}
    \end{equation}

    Por tanto, queda probar que $rank(U(x)) \geq r; \dspace \text{c.p.d. } x \in \R^{2MN + N}$. Definimos $U_r(x)$ como la submatriz superior izquierda $r \times r$ de $U(x)$.

    Si fuese $U_r(x)$ no singular para cierto $x$, claramente sabríamos que $r = rank(U_r(x)) \leq rank(U(x))$. Por tanto, buscamos probar que $U_r(x)$ es no singular salvo un conjunto de medida nula, es decir, que el conjunto $\conjunto{x \in \R^{2MN + N} : det(U_r(x)) = 0 }$ tiene medida nula.

    Para ver esto tenemos en cuenta que $det(U_r(\nv{x}))$ es un polinomio con entradas en $x \in \R^{2MN + N}$. Por \customref{lema:prop_fundamentales_medida} sabemos que entonces o bien es el polinomio idénticamente nulo o bien el conjunto de sus ceros tiene medida nula. Basta ver por tanto que no es el polinomio idénticamente nulo.

    Para ello es suficiente probar que $\exists \nv{x_0} \in \R^{2MN + N}$ con el que se verifique que $det(U_r(\nv{x})) \neq 0$. Tomamos $x_0$ de forma que $D(x_0) = \mathbb{I}_d$ y que $A(\nv{x_0}) = B(\nv{x_0}) = (\delta_{i,j})_{i,j}$. Es decir, que $A(\nv{x_0}), B(\nv{x_0})$ son matrices, no necesariamente cuadradas, con unos en la diagonal y ceros en el resto de posiciones. Por cómo definimos las matrices $A(\nv{x})$, $B(\nv{x})$ y $D(\nv{x})$, es trivial realizar esta elección de $\nv{x_0}$. En esta situación tenemos:

    \begin{equation}
        \begin{split}
            U_r(\nv{x_0}) &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \mathbb{I}d_{N \times N}
            \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}^T_{N \times M} = \ldots \\
            \ldots &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \mathbb{I}d_{N \times N}
            \begin{pmatrix}
                0 \\
                \hline
                \mathbb{I}d_{r \times r}
            \end{pmatrix}_{N \times M} = \ldots \\
            \ldots &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \begin{pmatrix}
                0 \\
                \hline
                \mathbb{I}d_{r \times r}
            \end{pmatrix}_{N \times M} = \ldots \\
            \ldots &= \mathbb{I}d_M
        \end{split}
    \end{equation}

    Por tanto, es claro que

    \begin{equation}
        det(U_r(\nv{x_0})) = det(\mathbb{I}d_M) = 1 \neq 0
    \end{equation}

    como buscábamos probar.

    \textbf{Caso específico}. Más adelante, usaremos este caso cuando estudiemos las modelizaciones imponiendo compartición de parámetros.

    La asignación ahora es, dado $\nv{x} \in \R^{MN + N}$:

    \begin{itemize}
        \item $A(\nv{x})$ contiene los primeros $MN$ elementos de $\nv{x}$
        \item $D(\nv{x})$ contiene los últimos $N$ elementos de $\nv{x}$ en la diagonal
        \item $U(\nv{x}) := A(\nv{x}) \; D(\nv{x}) \; A(\nv{x})^T$
    \end{itemize}

    Es claro que $U(\nv{x}) \in \espaciomatrices{M}{M}$. A partir de esto, podemos repetir análogamente todo el razonamiento, y en el último paso tomar de forma todavía más sencilla $\nv{x_0} \in \R^{MN + N}$ para que $D(\nv{x}) = \mathbb{I}_d$ y que $A(\nv{x}) = (\delta_{i, j})_{i, j}$.
\end{proof}


Veamos ahora el segundo lema clave en la prueba del teorema principal:

\begin{lema}[Segundo lema fundamental] \label{lema:segundo_lema}
    Sean $p$ funciones continuas de $\R^d$ a $\espaciomatrices{M}{N}$, que llevan un punto $\nv{y} \in \R^d$ a las matrices $A_1(\nv{y}), \ldots, A_p(\nv{y})$. Supongamos que con estas funciones el siguiente conjunto tiene medida nula:

    \begin{equation}
        \conjunto{\nv{y} \in \R^d : rank(A_i(\nv{y})) < r, \dspace \forall i \in \deltaset{p}}
    \end{equation}

    Definimos la función

    \begin{equation}
        \begin{split}
            A: \R^P \times \R^d &\to \espaciomatrices{M}{N} \\
            (\nv{x}, \nv{y}) &\mapsto \sum_{i = 1}^p x_i A_i(\nv{y})
        \end{split}
    \end{equation}

    Entonces el conjunto de puntos $(\nv{x}, \nv{y}) \in \R^p \times \R^d$ para los cuales $A(\nv{x}, \nv{y}) < r$ forman un conjunto de medida nula
\end{lema}

\begin{proof}

    Definimos el conjunto

    \begin{equation}
        S := \conjunto{(\nv{x}, \nv{y}) \in  \R^p \times \R^d: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p \times \R^d
    \end{equation}

    y lo que buscamos es probar que tiene medida nula.

    Como $A(x, y)$ es continua, y hemos probado que $\conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r}$ es cerrado, entonces sabemos que $S$ es cerrado, y por tanto, medible.

    Ahora buscamos calcular la medida de $S$. Empezamos definiendo, para cada $y \in \R^d$, el siguiente conjunto marginal:

    \begin{equation}
        S^{\nv{y}} := \conjunto{x \in \R^p: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p
    \end{equation}

    que también es medible.

    Con esto, basta probar que la medida de $S^{\nv{y}}$ es nula casi por doquier (respecto $\nv{y} \in \R^d$). Porque en tal caso, usando \customref{lema:prop_fundamentales_medida}, sabremos que:

    \begin{enumerate}
        \item $S^{\nv{y}} \times \R^d$ tiene medida nula para casi todo $\nv{y} \in \R^d$
        \item $S$ tiene medida nula por ser $S \subseteq S^{\nv{y}} \times \R^d$
    \end{enumerate}

    Para probar que $S^{\nv{y}}$ tiene medida nula casi por doquier, usaremos el siguiente conjunto:

    \begin{equation}
        \mathcal{C} := \conjunto{\nv{y} \in \R^d: rank(A_i(\nv{y})) < r, \forall i \in \deltaset{p}}
    \end{equation}

    que es el conjunto de funciones que aparece en el enunciado del lema, por lo que sabemos que $\mathcal{C}$ tiene medida nula. Si tomamos $\nv{y_0} \in \mathcal{C}$ es claro que $S^{\nv{y_0}} = \R^p$. Pero esto no supone un problema puesto que $\mathcal{C}$ es de medida nula. Queda ver qué pasa en $S - \mathcal{C}$, buscando que en dicho conjunto $S^{\nv{y_0}}$ tenga medida nula:

    \begin{equation}
        \begin{split}
            \nv{y_o} \in S - \mathcal{C} \encima{\then}{\nv{y_o} \notin \mathcal{C}} \substack{\exists i \in \deltaset{p}: \\ rank(A_i(\nv{y_o})) \geq r}
        \end{split}
    \end{equation}

    Sin pérdida de generalidad, supongo que $i = 1$ y que la submatriz $r \times r$ superior izquierda de $A_1(\nv{y_0})$ es no singular. Viendo $\nv{y_o}$ como un valor fijo, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y_0})$ es un polinomio en $\nv{x}$. Además, no es el polinomio idénticamente nulo, porque tomando $\nv{x} = (1, \ldots, 0)$ tenemos que

    \begin{equation}
    \begin{split}
        A(\nv{x}, \nv{y_0}) &= \sum_{i = 1}^{p} x_i A_i(\nv{y_0}) = A_1(\nv{y_0}) \then \ldots \\
        \ldots \then det(A(\nv{x}, \nv{y_0})) &= det(A_1(\nv{y_0})) \neq 0
    \end{split}
    \end{equation}

    Por tanto, usando \customref{lema:prop_fundamentales_medida}, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y})$ es un polinomio cuyo conjunto de ceros tiene medida nula. Y por lo tanto, el conjunto $S^{\nv{y_0}}$ tiene medida nula, porque casi todas las matrices $A(\nv{x}, \nv{y_0})$ tienen al menos rango $r$, como acabamos de ver. Con esto, hemos probado que la medida de $S^{\nv{y_0}}$ es nula.

    Ahora, usaremos el teorema de Fubini y el teorema de la convergencia monótona para demostrar que el conjunto $S$ tiene medida nula. $\mathbb{1}_A$ denotará la función indicadora del subconjunto $A$. Definimos los siguientes conjuntos:

    \begin{equation}
        \begin{split}
            S_n &:= S \interseccion [-n, n]^{p+d} \\
            \R_n &:= \R \interseccion [-n, n]^{p+d} = [-n, n]^{p+d}
        \end{split}
    \end{equation}

    Todos los conjuntos con los que estamos trabajando son medibles. Al intersecar con $[-n, n]^{p+d}$, además, tendrán medida finita. Por tanto, la función indicadora sobre estos conjuntos medibles de medida finita será integrable \footnotemark. Y con ello, estamos en condiciones de aplicar ahora el teorema de Fubini para desarrollar:

    \footnotetext{Porque claramente $\int |\mathbb{1}_{S_n}| = \int \mathbb{1}_{S_n} = \lambda(S_n) < \infty$ }

    \begin{equation}
        \begin{split}
            \int_{(\nv{x}, \nv{y}) \in \R^{p+d}} \mathbb{1}_{S_n} &= \int_{(\nv{x},\nv{y}) \in \R_n^{p+d}} \mathbb{1}_{S} \encima{=}{Fubini} \int_{\nv{y} \in \R_n^d} \int_{\nv{x} \in \R_n^p} \mathbb{1}_{S^y} = \ldots \\
            \ldots &= \comentardebajo{\int_{\nv{y} \in \R_n^d \interseccion \mathcal{C}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(1)} + \comentardebajo{\int_{\nv{y} \in \R_n^d - \mathcal{C}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(2)} = \ldots \\
            \ldots &= 0 + 0 = 0
        \end{split}
    \end{equation}
    \todo{Habría que comprobar los supuestos de Fubini}

    Donde hemos usado:

    \begin{itemize}
        \item (1): hemos visto que la medida del conjunto $\mathcal{C}$ es cero
        \item (2): hemos visto que si $\nv{y} \not\in \mathcal{C}$, entonces el conjunto $S^{y}$ tiene medida nula
    \end{itemize}

    Y con ello hemos probado que $\int \mathbb{1}_{S_n} = 0$, sea cual sea el valor de $n \in \N$. Es claro que, como $S$ es un conjunto medible, $\mathbb{1}_{S_n}$ es una sucesión de funciones medibles creciente, y por tanto, podemos aplicar el \textbf{teorema de convergencia monótona} para calcular:

    \begin{equation}
        \int \mathbb{1}_S = \int \lim_{n \to \infty} \mathbb{1}_{S_n} \encima{=}{T.C.M} \lim_{n \to \infty} \int \mathbb{1}_{S_n} = \lim_{n \to \infty} 0  = 0
    \end{equation}

    Por lo tanto, la medida del conjunto $S$ es cero, como queríamos probar.

\end{proof}

Estamos ya en condiciones de probar el teorema principal del trabajo. Recordamos su enunciado y pasamos a ver la prueba:

\begin{teorema}[Rango \textit{CP} exponencial de un modelo \textit{HT}] \label{teorema:teorema_principal_especificacion}
    Sea $\mathcal{A}^y \in \espaciotensores{N}{M}$ dado por las ecuaciones \eqref{eq:descomposicion_ht}. Definamos $r := \min \conjunto{r_0, M}$ y consideremos el espacio de todas las posibles configuraciones de parámetros de nuestro modelo \textit{HT} $\conjunto{ \nv{a^{l, j, \gamma}}}_{l, j, \gamma}$. En este espacio, el tensor generado $\mathcal{A}^y$ tendrá rango \textit{CP} de al menos $r^{N/2}$ casi por doquier. Es decir, el conjunto de parámetros del modelo \textit{HT} con los que el modelo tiene rango \textit{CP} menor que $r^{N/2}$ tiene medida nula. El resultado se mantiene si forzamos la compartición de coeficientes en \eqref{eq:descomposicion_ht}. Es decir, haciendo $\nv{a^{l, \gamma}} \equiv \nv{a^{l, j, \gamma}}$ y considerando el espacio de configuraciones $\{ \nv{a^{l, \gamma}}  \}_{l, \gamma}$.
\end{teorema}

\begin{proof}
    \textbf{Caso 1: sin compartición de coeficientes}

    Como hemos hecho al introducir el modelo \textit{HT}, denotamos por conveniencia $\phi^{L, 1, 1}:= A^y$ y consideramos que $r_L = 1$.
    \todo{En la página 7 del \textit{paper} dicen que por completitud, $r_L = Y$}

    Probamos por inducción en $l = 1, \ldots, L$ que, en casi todo punto respecto al conjunto $\conjunto{\nv{a^{l, j, \gamma}}}_{l, j, \gamma}$, el rango \textit{CP} de los tensores $\conjunto{\phi^{l, j, \gamma}: j \in \deltaset{N/2^l}, \gamma \in \deltaset{r_l}}$ es al menos $r^{2^{l/2}}$.

    Por \customref{prop:rango_matricial_descomp_cp}, bastará probar que los rangos de las matrizaciones $[\phi^{l, j, \gamma}]$ tienen al menos rango matricial $r^{2^{l/2}}$ en casi todo punto (respecto al conjunto de los parámetros que definen el modelo \textit{HT}: $\conjunto{ \nv{a^{l, j, \gamma}}}_{l, j, \gamma}$). Comenzamos la inducción viendo el \textbf{caso $l = 1$}.

    Por definición, tenemos que para unos valores concretos de $j, \gamma$:

    \begin{equation}
        \phi^{1, j, \gamma} = \sum_{\alpha = 1}^{r_0} \comentardebajo{a_{\alpha}^{1, j, \gamma}}{\text{diagonal } D} \comentarencima{\varphi^{0, 2j - 1, \alpha}}{A \text{ por columnas}} \otimes \comentardebajo{\varphi^{0, 2j, \alpha}}{B \text{ por columnas}}
    \end{equation}

    Definimos tres matrices $A, B, D$:

    \begin{itemize}
        \item $A \in \espaciomatrices{M}{r_0}$ con columnas $\conjunto{\varphi^{0, 2j - 1, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $B \in \espaciomatrices{M}{r_o}$ con columnas $\conjunto{\varphi^{0, 2j, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $D \in \espaciomatrices{r_0}{r_0}$ con valores en la diagonal $a_{\alpha}^{1, j, \gamma}, \; \alpha \in \deltaset{r_0}$
    \end{itemize}

    Con ello podemos escribir $[\phi^{1, j, \gamma}] = A D B^T$. Aplicando \customref{lema:primer_lema}, sabemos que $rank([\phi^{1, j, \gamma}]) = r = \min \conjunto{r_0, M}$ en casi todo punto, respecto al conjunto de todos los parámetros que conforman $\phi^{1, j, \gamma}$. Estos parámetros son:
    \todo{Hay que probar o desarrollar mejor esta matrización}

    \begin{itemize}
        \item $\conjunto{\varphi^{0, 2j - 1, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $\conjunto{\varphi^{0, 2j, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $\conjunto{\nv{a^{1, j, \alpha}} : \alpha \in \deltaset{r_0}}$ donde $\nv{a^{1, j, \alpha}} := (a^{1, j, \alpha}_1, \ldots,  a^{1, j, \alpha}_{r_0})$
    \end{itemize}

    Por tanto, falta ver que dicha afirmación se mantiene para casi todo punto respecto al conjunto de todos los parámetros del modelo \textit{HT}, no solo los que determinan $\phi^{1, j, \gamma}$. Pero esto es directo usando \customref{lema:prop_fundamentales_medida}.

    Ahora sabemos que $rank([\phi^{1, j, \gamma}]) = r$ en casi todo punto (respecto al conjunto de todos los parámetros que definen el modelo), para valores concretos de $j \in \deltaset{N/2}, \gamma \in r_1$. Veamos que esto implica que la igualdad sigue siendo válida para cualquier $j, \gamma$ de forma conjunta:

    \begin{equation}
    \begin{split}
        \phi_1^\gamma &:= \conjunto{j \in \deltaset{N/2}: rank([\phi^{1, j, \gamma}] \neq r} \text{ tiene medida } 0 \then \phi_1 := \union_{\gamma \in \deltaset{r_1}} \phi_1^\gamma \text{ tiene medida } 0 \then \ldots \\
        \ldots &\then \phi_1 \times \R \text{ tiene medida } 0 \then \ldots \\
        \ldots &\then \phi := \conjunto{(j, \gamma) \in \deltaset{N/2} \times \deltaset{r_1}: rank([\phi^{1, j, \gamma}]) \neq r} \subseteq \phi_1 \times \R \text{ tiene medida } 0
    \end{split}
    \end{equation}

    Esto prueba el caso $l = 1$.

    \textbf{Hipótesis de inducción}: supuesto que es cierto para $l - 1$, ¿esto implica que sea cierto para $l$?.

    Usando la hipótesis de inducción sabemos que

    \begin{equation}
        rank(\phi^{l-1, j, \gamma}) \geq r^{\frac{2^{l - 1}}{2}},
        \; \forall j \in \deltaset{N/2^{l-1}}, \forall \gamma \in \deltaset{r_{l-1}}
    \end{equation}

    en casi todo punto (respecto al conjunto de todos los parámetros del modelo). Fijados unos valores concretos para $j \in \deltaset{N/2^{l-1}}$, $\gamma \in \deltaset{r_{l-1}}$, tenemos que:

    \begin{equation}
    \begin{split}
        \phi^{l, j, \gamma} &:= \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot \phi^{l-1, 2j-1, \alpha} \otimes \phi^{l-1, 2j, \alpha} \then \\
        &\then [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot [\phi^{l-1, 2j-1, \alpha}] \odot [\phi^{l-1, 2j, \alpha}]
    \end{split}
    \end{equation}

    Ahora definimos $M_\alpha := [\phi^{l-1, 2j-1, \alpha}] \odot [\phi^{l-1, 2j, \alpha}]$, $\forall \alpha \in \deltaset{r_{l-1}}$ y aplicamos la hipótesis de inducción junto al hecho de que $rank(A \odot B) = rank(A) \cdot rank(B)$:

    \begin{equation}
    \begin{split}
        \substack{rank([\phi^{l-1, 2j-1, \alpha}]) \geq r^{\frac{2^{l-1}}{2}} c.p.d. \\ rank([\phi^{l-1, 2j-1, \alpha}]) \geq r^{\frac{2^{l-1}}{2}} c.p.d.} \then         rank(M_\alpha) \geq r^{\frac{2^{l-1}}{2}} \cdot r^{\frac{2^{l-1}}{2}} = r^{\frac{2^{l-1}}{2} + \frac{2^{l-1}}{2}} =
        r^{\frac{2 \cdot 2^{l - 1}}{2}} = r^{\frac{2^{l}}{2}} \; \text{c.p.d.} \\
    \end{split}
    \end{equation}

    Ahora, usando la definición de $M_\alpha$, calculamos:

    \begin{equation}
        [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot M_\alpha
    \end{equation}

    Como $\conjunto{M_\alpha: \; \alpha \in \deltaset{r_{l-1}}}$ no depende de $\nv{a^{l, j, \gamma}}$ porque solo codifica las entradas tensoriales del nivel anterior, podemos aplicar \customref{lema:segundo_lema}:

    \begin{equation}
        \substack{
            rank(M_\alpha) \geq r^{\frac{2^l}{2}}\\
            \conjunto{\nv{a^{0, j, \gamma}}} \to M_\alpha \text{ continuo } \\
            \text{por ser productos tensoriales, sumas, matrizaciones}
        } \then
        [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_\alpha^{l,j,\gamma} M_\alpha \text{ tiene rango al menos } r^{\frac{2^l}{2}} \text{ c.p.d. }
    \end{equation}

    Como $rank([\phi^{l, j, \gamma}]) \geq r^{\frac{2^l}{2}}$ casi por doquier para $(j, \gamma) \in \deltaset{N/2^l} \times \deltaset{r_{l-1}}$, podemos hacer un argumento análogo al anterior para extender esta propiedad a todo el espacio.

    Esto concluye la prueba para el caso no compartido

    \textbf{Caso 2: Compartición de coeficientes}

    Es totalmente análogo al caso 1. En la inducción, en el paso $l = 1$, usamos la versión del lema \customref{lema:primer_lema} en la que $A(\nv{x}) = B(\nv{x})$
\end{proof}

\section{Prueba del corolario principal}

Para demostrar el corolario, necesitaremos primero probar el siguiente lema. De nuevo, es tan relevante que lo presentamos aquí y no con el resto de corolarios.

\begin{lema}[Tercer lema fundamental] \label{lema:lema_previo_corolario}
    Sean $f_{\theta_1}, \ldots, f_{\theta_M} \in L^2(\R^s)$ un conjunto de funciones linealmente independientes. Dado un tensor $\mathcal{A} \in \espaciotensores{N}{M}$ definimos la siguiente función:

    \begin{equation}
    \begin{split}
        h(\mathcal{A}): (\R^s)^N &\to \R \\
        (\nv{x_1}, \ldots, \nv{x_N}) &\mapsto \sum_{d_1, \ldots, d_N = 1}^M \mathcal{A}_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i})
    \end{split}
    \end{equation}

    que es la función de puntuación introducida en \eqref{eq:puntuacion_general} dado el tensor de coeficientes $\mathcal{A}$. Sea $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda} \subseteq \espaciotensores{N}{M}$ una familia de tensores, y $\mathcal{A}^* \in \espaciotensores{N}{M}$ un tensor que no esté en esa familia. Supongamos que $rank([\mathcal{A}^\lambda]) < rank([\mathcal{A}^*])$, $\forall \lambda \in \Lambda$. Entonces la distancia en $L^2((\R^s)^N)$ entre $h(\mathcal{A}^*)$ y el conjunto $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva. Es decir, $\exists \epsilon > 0$ tal que

    \begin{equation}
        \int |h(\mathcal{A}^\lambda) - h(\mathcal{A}^*)|^2 > \epsilon; \; \forall \lambda \in \Lambda
    \end{equation}
\end{lema}

\begin{proof}
    Como la familia de funciones $f_{\theta_1}, \ldots, f_{\theta_M} \in L^2(\R^s)$ es linealmente independiente, sabemos que la familia de funciones producto inducida

    \begin{equation}
        \zeta = \conjunto{\prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i}): d_1, \ldots d_N \in \deltaset{M}}
    \end{equation}

    también es linealmente independiente (\customref{prop:conservacion_totalidad_indp_lineal_func_prod}).

    Consideramos ahora una sucesión de funciones $\conjunto{h^{(t)}}_{t \in \N}$ en $span\conjunto{\zeta}$. Para cada $t \in \N$ denotamos por $\mathcal{A}^{(t)}$ al tensor de coeficientes de $h^{(t)}$ en la \entrecomillado{base} dada por $\zeta$, gracias a que nuestra familia de funciones $\zeta$ es linealmente independiente y claramente total en $span \conjunto{\zeta}$. Es decir, $\mathcal{A}^{(t)} \in \espaciotensores{N}{M}$ viene determinado por la siguiente igualdad:

    \begin{equation}
        h^{(t)}(\nv{x_1}, \ldots, \nv{x_N}) = \sum_{d_1 = 1, \ldots, d_N = 1}^M \mathcal{A}^{(t)}_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i})
    \end{equation}

    En caso de que $\conjunto{h^{(t)}}_{t \in \N}$ no converja a $h(\mathcal{A}^*)$ nuestra proposición es trivial. Por lo tanto, supongamos que $\conjunto{h^{(t)}}_{t \in \N}$ converge a $h(\mathcal{A}^*)$ en $L^2((\R^s)^N)$, es decir:

    \begin{equation}
        \lim_{t \to \infty} d_{L^2}(h^{(t)}, h(\mathcal{A}^*)) = 0
    \end{equation}

    O lo que es lo mimso:

    \begin{equation}
        \lim_{t \to \infty} \int | h^{(t)} - h(\mathcal{A}^*)|^2 = 0
    \end{equation}

    Sabemos, por \customref{prop:convergencia_norma_coeficientes}, que en un espacio de Hilbert de dimensión finita, como es nuestro caso, convergencia en norma implica convergencia en los coeficientes de representación. Como hemos supuesto la convergencia en norma, con lo anterior tenemos que:

    \begin{equation}
        \mathcal{A}^{(t)}_{d_1, \ldots, d_N} \encima{\longmapsto}{t \mapsto \infty} \mathcal{A}^*_{d_1, \ldots, d_N},
        \; \forall d_1, \ldots, d_N \in \deltaset{M}
    \end{equation}

    Luego en el espacio $\espaciotensores{N}{M}$ el tensor $\mathcal{A}^*$ pertenece al cierre de $\conjunto{\mathcal{A}^{(t)}: t \in \N}$.

    Con todo esto, para probar que la distancia en $L^2((\R^s)^N)$ entre $h(\mathcal{A}^*)$ y $\conjunto{h(\mathcal{A}^\lambda): \lambda \in \Lambda}$ es estrictamente positiva bastará con ver que la distancia en $\espaciotensores{N}{M}$ entre $\mathcal{A}^*$ y $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva. Comprobemos que basta con realizar la comprobación en $\espaciotensores{N}{M}$ que acabamos de comentar:

    \begin{equation}
    \begin{split}
        h^{(t)} \encima{\longmapsto}{t \mapsto \infty} h(\mathcal{A}^*) &\iif \sum \mathcal{A}^{(t)}_{d_1, \ldots, d_N} \prod f_{\theta_{d_i}}(\nv{x_i}) \encima{\longmapsto}{t \mapsto \infty} \sum \mathcal{A}^*_{d_1, \ldots, d_N} \prod f_{\theta_{d_i}}(\nv{x_i}) \iif \ldots \\
        \ldots &\iif \mathcal{A}^{(t)} \encima{\longmapsto}{t \mapsto \infty} \mathcal{A}^* \iif \ldots \\
        \ldots &\iif \lim_{t \to \infty} d(A^{(t)}, \mathcal{A}^*) = 0
    \end{split}
    \end{equation}

    Es claro entonces que

    \begin{equation}
        d(A^{(t)}, \mathcal{A}^*) \not\longto 0 \then h^{(t)} \not\longto h(\mathcal{A}^*)
    \end{equation}

    Que la distancia en $\espaciotensores{N}{M}$ entre $\mathcal{A}^*$ y $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva equivale a su vez a que la distancia entre $[\mathcal{A}^*]$ y $\conjunto{[\mathcal{A}^\lambda]: \lambda \in \Lambda}$ sea estrictamente positiva, porque la matrización no degenera distancias \footnotemark.

    \footnotetext{Esto es, si dos tensores $\mathcal{A}$, $\mathcal{B}$ están a una distancia estrictamente positiva, las matrices $[\mathcal{A}]$, $[\mathcal{B}]$ no pueden estar a una distancia nula}

    Ahora, esto último es claro puesto que:

    \begin{equation}
        rank([\mathcal{A}^\lambda]) < rank([\mathcal{A}^*]), \; \forall \lambda \in \Lambda \then d(\mathcal{A}^*, \conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda} > 0
    \end{equation}
    \todo{Obvio pero habŕa que demostrarlo. No conocemos $\Lambda$ y podríamos tener una sucesión de distancias que converge a cero}
\end{proof}

Estamos ya en condiciones de probar el corolario fundamental de este trabajo:

\begin{corolario}[Incapacidad del modelo \textit{CP} para aproximar eficientemente el modelo \textit{HT}] \label{corolario:corolario_principal_concreto}
    Dado un conjunto de funciones de representación $\conjunto{f_{\theta_d}: d \in \deltaset{M}}$ linealmente independientes, aleatorizar los pesos de un modelo \textit{HT} con una distribución de probabilidad continua induce una función de puntuación $h_y$ que, con probabilidad uno, no puede ser aproximada arbitrariamente bien (en el sentido $L^2$ ) por un modelo \textit{CP} con menos de $r := \min \conjunto{r_0, M}^{\frac{N}{2}}$ canales en la capa oculta. El resultado se mantiene aunque forcemos la compartición de parámetros en el modelo \textit{HT} mientras que dejamos al modelo \textit{CP} en su forma general.
\end{corolario}

\begin{observacion}
    En \customref{teorema:teorema_principal_especificacion} hemos visto que un tensor generado por un modelo \textit{HT} tiene rango \textit{CP} de al menos $\min \conjunto{r_0, M}^{\frac{N}{2}}$  en casi todo punto respecto al espacio de parámetros que definen el modelo \textit{HT}, $\conjunto{\nv{a^{l, j, \gamma}}}_{l, j, \gamma}$. Es decir, que el modelo \textit{CP} requerirá un tamaño exponencial: $Z \geq \min \conjunto{r_0, M}^{\frac{N}{2}}$.

    Pero el corolario da un paso más. Partimos de que las funciones de representación son linealmente independientes (ya hemos visto que en la práctica del \textit{deep learning} esto es lo común). El corolario afirma que no es que sea \entrecomillado{prácticamente imposible} realizar el coeficiente de tensores con un modelo \textit{CP} con menos de un número exponencial de canales en la capa oculta, sino que es \entrecomillado{prácticamente imposible} aproximar la función de puntuación con menos de dicho número exponencial de canales en la capa oculta.
\end{observacion}

\begin{proof}

    En la prueba de \customref{teorema:teorema_principal_especificacion}, para mostrar la separación entre coeficientes del modelo \textit{HT} y el modelo \textit{CP} usamos como herramienta el rango de la matrización.

    En concreto, habíamos encontrado constantes $Rank^{HT}, Rank^{CP} \in \N$ con $Rank^{HT} > Rank^{CP}$ de forma que el modelo \textit{HT} tenía rango mayor o igual que $Rank^{HT}$ mientras que el modelo \textit{CP} tenía rango menor o igual que $Rank^{CP}$.

    Usando este hecho, aplicamos el lema \customref{lema:lema_previo_corolario}:

    \begin{itemize}
        \item Considero como $\mathcal{A}^*$ el tensor de coeficientes generado por el modelo \textit{HT}. Por tanto, $h(\mathcal{A}^*)$ es la función de puntuación que dicho modelo implementa como una red profunda
        \item Considero como $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ el conjunto de tensores generados por un modelo \textit{CP} de tamaño $Z < \min \conjunto{r_0, M}^{\frac{N}{2}}$
    \end{itemize}

    El teorema \customref{teorema:teorema_principal_especificacion} nos dice con probabilidad 1 que $\mathcal{A}^*$ no puede ser generado por un modelo \textit{CP} con menos de $\min \conjunto{r_0, M}^{\frac{N}{2}}$ canales en la capa oculta. Luego $\mathcal{A}^* \notin \conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$.
    \todo{Relacionar el \entrecomillado{con probabilidad 1} con que nuestro teorema habla de medida nula}

    Además, ya sabemos que $rank([\mathcal{A}^\lambda]) < \frac{N}{2} \leq rank([\mathcal{A}^*]), \; \forall \lambda \in \Lambda$.

    Por lo tanto, se cumplen todos los supuestos del lema \customref{lema:lema_previo_corolario}, luego la distancia $L^2$ entre $h(\mathcal{A}^*)$ y $\conjunto{h(\mathcal{A}^\lambda): \lambda \in \Lambda}$ es estrictamente positiva. Es decir, $\exists \epsilon_0 > 0:$

    \begin{equation}
        d_{L^2}(h(\mathcal{A}^*), h(\mathcal{A}^\lambda)) \geq \epsilon_0, \; \forall \lambda \in \Lambda
    \end{equation}

    Por tanto, no podemos aproximar arbitrariamente bien, estamos limitados por la constante $\epsilon_0$. Es decir, no es cierto que dada $h_y$ inducida al tomar coeficientes aleatorios en un modelo \textit{HT}, podamos aproximar arbitrariamente bien con un modelo \textit{CP} con menos de $\min \conjunto{r_0, M}^{\frac{N}{2}} = r^{\frac{N}{2}}$ canales en la capa oculta.

    De ser cierto, tendríamos que $\forall \epsilon > 0$, $\exists \mathcal{A}_\epsilon \in \Omega$ de forma que $d_{L^2}(h(\mathcal{A}^*), h(\mathcal{A}_\epsilon)) < \epsilon$ donde

    \begin{equation}
        \Omega := \conjunto{\mathcal{A} \in \espaciotensores{N}{M}: \mathcal{A} \text{ es realizable por un modelo \textit{CP} con menos de } r^{\frac{N}{2}} \text{ canales en la capa oculta}}
    \end{equation}

    Pero acabamos de ver que esto es falso $\forall \epsilon < \epsilon_0$.
\end{proof}
