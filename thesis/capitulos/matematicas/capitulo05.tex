\chapter{Teoremas de la capacidad de las redes neuronales}

\section{Introducción a los teoremas}

Los teoremas principales de este trabajo se presentan en esta sección. Comenzamos por el siguiente:

\begin{teorema}[Darle nombre]

Sea $\mathcal{A}^y \in \espaciotensores{N}{M}$ dado por las ecuaciones \eqref{eq:descomposicion_ht}. Definamos $r := \min{r_0, M}$ y consideremos el espacio de todas las posibles configuraciones de parámetros de nuestro modelo $\{ \nv{a^{l, j, \gamma}} \}_{l, j, \gamma}$. En este espacio, el tensor generado $\mathcal{A}^y$ tendrá rango \textit{CP} de al menos $r^{N/2}$ casi por doquier. Es decir, el conjunto de tensores generados con rango \textit{CP} menor que $r^{N/2}$ tiene medida cero. El resultado se mantiene si forzamos la compartición de coeficientes en \eqref{eq:descomposicion_ht}. Es decir, haciendo $\nv{a^{l, \gamma}} \equiv \nv{a^{l, j, \gamma}}$ y considerando el espacio de configuraciones $\{ \nv{a^{l, \gamma}}  \}_{l, \gamma}$.

\end{teorema}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

\begin{observacion}

    Casi todos los tensores que podemos generar con un modelo \textit{HT} tienen rango \textit{CP} de al menos $r^{N/2}$.

\end{observacion}

\begin{observacion}
    $\mathcal{A}^y$ claramente es realizable, por ser el resultado de un modelo \textit{HT}
\end{observacion}

\begin{observacion}

    Que $\mathcal{A}^y$ tenga rango \textit{CP} igual a $r^{N/2}$ implica que en la ecuación \eqref{eq:cp_decomp}, el número de sumandos deberá ser al menos $r^{N/2}$. Luego, conforme el tamaño de la imagen $N$ aumenta, el número de parámetros del modelo \textit{CP} crece exponencialmente. Conforme el número de canales $M$ de la imagen crece, el número de parámetros del modelo \textit{CP} crece pero de forma potencial.

\end{observacion}

A partir de este teorema, obtenemos el siguiente corolario:

\begin{corolario}[Darle nombre] \label{corol:corol_principal}

Dado un conjunto de funciones de representación $\{f_{\theta_d}: d \in \deltaset{M} \}$ linealmente independientes, aleatorizando los pesos de un modelo \textit{HT} \eqref{eq:descomposicion_ht} a partir de una distribución de probabilidad continua, induce funciones de puntuación $h_y$ que con probabilidad uno no pueden ser aproximadas arbitrariamente bien (en el sentido $L^2$) por un modelo \textit{CP} con menos de $r := \min \{r_0, M \}^{N/2}$ canales. Este resultado se mantiene forzando la compartición de coeficientes en el modelo \textit{HT} mientras que dejamos el modelo \textit{CP} sin restricciones.

\end{corolario}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

Es decir, salvo un conjunto de medida nula, todos los tensores realizables por una descomposición \textit{HT} no pueden realizarse, ni siquiera aproximarse, por una descomposición \textit{CP} con menos de un número exponencial de parámetros.

El \customref{corol:corol_principal} introduce una mejora muy grande respecto a los trabajos previos. En estos, se aportan ejemplos de funciones realizables por el modelo \textit{HT} que no son realizables por un modelo \textit{CP} con un número de parámetros sustancialmente superior al número de parámetros del modelo \textit{HT}. Es decir, establecen la \textit{depth efficiency} de ejemplos concretos de funciones. Nuestro corolario establece la \textit{depth efficiency} casi por doquier.

Estos resultados también mejoran los trabajos previos sobre análisis tensorial. El trabajo que introduce la descomposición \textit{HT} \cite{matematicas:descomposicion_ht} da ejemplos específicos de tensores realizables por el modelo \textit{HT} y no por un modelo \textit{CP} con un número exponencial de parámetros. Nuestro corolario indica que esto no ocurre en ejemplos aislados, sino en casi todos los tensores realizables por un modelo \textit{HT}.

Teniendo en cuenta que cualquier tensor realizable por un modelo \textit{CP} puede ser realizado por un modelo \textit{HT} con un aumento polinomial del número de coeficientes (véase \customref{msubs:parametros_modelo_ht}), esto implica que la descomposición \textit{HT} es asintóticamente más eficiente que la \textit{CP}.
\todo{Aquí no he definido lo que entendemos por eficiencia. Creo que sería \textit{depth efficiency}}

\section{Lemas previos}

En esta sección introduciremos algunos resultados que serán usados en las demostraciones del teorema \customref{} y el corolario \customref{}. Empezamos con los siguientes resultados de teoría de la medida de Lebesgue:

\begin{lema}[Propiedades fundamentales de la medida] \label{lema:prop_fundamentales_medida} Las siguientes propiedades sobre la medida de Lebesgue son ciertas:

\begin{enumerate}
    \item La unión contable de conjuntos de medida nula es un conjunto de medida nula
    \item Si $A \subseteq \R^{N_1}$ tiene medida nula, entonces $A \times R^{N_2} \subseteq R^{N_1 + N_2}$ también tiene medida nula
    \item Sea $p \in \R^d[x]$, es decir, un polinomio sobre los números reales con $d$ variables. Si $p$ no es el polinomio idénticamente nulo, entonces el conjunto de ceros del polinomio es un conjunto de medida nula \cite{informatica:zeros_of_polynomial}
\end{enumerate}

\end{lema}

Usaremos la siguiente \textbf{matrización} de un tensor:

\begin{definicion}[Matrización de un tensor]

Sea $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$ un tensor de orden $N$ y dimensión $M_i, i \in \deltaset{N}$ en cada modo. Por simplicidad, supondremos que el orden del tensor es par. Denotaremos por $[\mathcal{A}]$ a la \textbf{matrización} del tensor $\mathcal{A}$. $[\mathcal{A}]$ es una matriz donde las filas corresponden a modos impares del tensor, y las columnas corresponden a los modos pares. Por lo tanto, $[\mathcal{A}]$ tiene $M_1 \cdot M_3 \cdot \ldots \cdot M_{N-1}$ filas y $M_2 \cdot M_4 \cdot \ldots \cdot M_N$ columnas. Asignaremos el valor $\mathcal{A}_{d_1, \ldots, d_N}$ en $[\mathcal{A}]$ en la posición:

\begin{itemize}
    \item Fila $1 + \sum_{i = 1}^{N/2} (d_{2i - 1} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j - 1}$
    \item Columna $1 + \sum_{i = 1}^{N/2} (d_{2i} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j}$
\end{itemize}

\end{definicion}

Con esto, tenemos que saber cómo se comporta la matrización con el producto tensorial. Esto lo introduce la siguiente proposición:

\begin{definicion}[Producto de Kronecker]

    Sean $A \in \espaciomatrices{N}{M}$ y  $B \in \espaciomatrices{P}{Q}$ dos matrices cualesquiera. Se define su producto de Kronecker $A \odot B$ como la matriz en $\espaciomatrices{N \cdot P}{M \cdot Q}$ dada por:

    \begin{equation}
        (A \odot B)_{i,j} := (A_{i, j} B)_{i, j}
    \end{equation}

    Donde estamos usando una notación por bloques.
\end{definicion}

\begin{ejemplo}[Producto de Kronecker con dos matrices sencillas]

Sean las siguientes matrices:

    \begin{equation}
        A := \begin{pmatrix}
            a_{11} & a_{12} \\
            a_{21} & a_{22} \\
        \end{pmatrix}
    \end{equation}

    \begin{equation}
        B := \begin{pmatrix}
            b_{11} & b_{12} \\
            b_{21} & b_{22} \\
        \end{pmatrix}
    \end{equation}

    Entonces, la matriz $A \odot B$ se computa de la siguiente forma:

    \begin{equation}
        (A \odot B) = \begin{pmatrix}
            a_{11}B & a_{12}B \\
            a_{21}B & a_{22}B \\
        \end{pmatrix} =
        \begin{pmatrix}
            a_{11} b_{11} & a_{11} b_{12} & a_{12} b_{11} & a_{12} b_{12} \\
            a_{11} b_{21} & a_{11} b_{22} & a_{12} b_{21} & a_{12} b_{22} \\
            a_{21} b_{11} & a_{11} b_{12} & a_{22} b_{11} & a_{22} b_{12} \\
            a_{21} b_{21} & a_{11} b_{22} & a_{22} b_{21} & a_{22} b_{22} \\
        \end{pmatrix}
    \end{equation}

\end{ejemplo}

\begin{observacion}
    El anterior ejemplo nos muestra cómo con el producto de Kronecker podemos operar dos matrices cualesquiera, independientemente de sus dimensiones.
\end{observacion}

Vemos ahora tres propiedades fundamentales:

\begin{proposicion}[Propiedades fundamentales] Las siguientes afirmaciones son ciertas:

    \begin{enumerate}
        \item Matrización y producto tensorial: Sean $\mathcal{A}, \mathcal{B}$ dos tensores. Entonces se verifica que $[\mathcal{A} \otimes \mathcal{B}] = [\mathcal{A}] \odot [\mathcal{B}]$ \label{prop:prop_fundamentales_primera}
        \item Rangos matriciales y producto de Kronecker: sean $A, B$ dos matrices. Entonces se verifica que $rank(A \odot B) = rank(A) \cdot rank(B)$
        \item Linealidad de la matrización: sean $\mathcal{A}_1, \ldots, \mathcal{A}_N$ tensores del mismo orden y mismas dimensiones, y $\alpha_1, \ldots, \alpha_N \in \R$, entonces $[\sum_{i = 1}^N \alpha_i \cdot \mathcal{A}_i] = \sum_{i = 1}^N \alpha_i \cdot [\mathcal{A}_i]$
    \end{enumerate}

\end{proposicion}

\begin{observacion} A partir de \customref{prop:prop_fundamentales_primera} es directo ver que el producto de Kronecker verifica las mismas propiedades que el producto tensorial (véase \customref{prop:tensores_propiedades}).
\end{observacion}

\begin{proposicion}[Rango matricial de la descomposición \textit{CP}] \label{prop:rango_matricial_descomp_cp}

    \begin{equation}
        rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = Z
    \end{equation}

    Es decir, un tensor de orden $2^L$ dado como descomposición \textit{CP} con $Z$ términos tiene una matrización de rango matricial como mucho $Z$.

\end{proposicion}

\begin{proof}

    Empezamos viendo la siguiente igualdad:

    \begin{equation}
        \begin{split}
            rank([\nv{v_1^{(z)}}  \otimes \ldots \otimes \nv{v_{2^L}^{(z)}}]) &= \ldots \text{  agrupo dos a dos   } \ldots = \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} rank([\nv{v_{2i - 1}^{(z)}} \otimes\nv{v_{2i}^{(z)}}]) = \ldots   \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} rank(\nv{v_{2i - 1}^{(z)}} \nv{v_{2i}^{(z)}}^T) = \ldots \\
            \ldots &= \prod_{i = 1}^{2^{L/2}} 1 = 1
        \end{split}
    \end{equation}

    Usamos esto para calcular nuestra ecuación

    \begin{equation}
        \begin{split}
            rank([\sum_{z = 1}^Z \lambda_z \nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) &\encima{=}{linealidad} rank(\sum_{z = 1}^Z \lambda_z [\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) \leq \ldots \\
            \ldots &\leq \sum_{z = 1}^Z rank([\nv{v_1}^{(z)} \otimes \ldots \otimes \nv{v_{2^L}}^{(z)}]) = \sum_{z = 1}^Z 1 = Z
        \end{split}
    \end{equation}
    \todo{No sé de donde sale la desigualdad!}
    \todo{No estoy explicando qué propiedades fundamentales que acabo de introducir uso en la prueba}
\end{proof}

\begin{proposicion}
    Dado $r \in \R$, el siguiente conjunto es cerrado:

    \begin{equation}
        \conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r }
    \end{equation}
\end{proposicion}

\begin{proof}
    \todo{Importante, es parte importante del trabajo}
\end{proof}



\section{Demostración de los teoremas}

Necesitaremos probar dos lemas que serán fundamentales en la prueba del teorema.

\begin{lema}[Rango de una matrización de la forma $A D B^T$] \label{lema:primer_lema}

    Sean $M, N \in \N$, $\nv{x} \in \R^{2MN + N}$ y definimos tres funciones que llevan $\nv{x}$ a una matriz:

    \begin{enumerate}
        \item $A(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los primeros $MN$ elementos de $\nv{x}$
        \item $B(\nv{x}) \in \espaciomatrices{M}{N}$ contiene los siguientes $MN$ elementos de $\nv{x}$
        \item $D(\nv{x}) \in \espaciomatrices{N}{N}$ es una matriz diagonal con los últimos $N$ elementos de $\nv{x}$ en la diagonal
    \end{enumerate}

    Definimos con ello la matriz $U(\nv{x}) := A(\nv{x}) D(\nv{x}) B(\nv{x})^T \in \espaciomatrices{M}{M}$. Entonces el conjunto de puntos $\nv{x} \in \R^{2MN + N}$ para los cuales el rango de $U(\nv{x})$ es distinto de $r := \min \{M, N\}$ tienen medida nula.

    El resultado sigue siendo cierto si consideramos $\nv{x} \in \R^{MN+N}$ y cambiamos la asignación de $B(x)$ por $B(x) := A(x)$.
\end{lema}

\begin{proof}
    Sin pérdida de generalidad, supondré que $M \leq N$, luego $r = M$. Como $U(\nv{x}) \in \espaciomatrices{M}{M}$ es claro que

    \begin{equation}
        rank(U(\nv{x})) \leq M \leq r := \min \{N, M \}; \dspace \forall \nv{x} \in \R^{2MN + N}
    \end{equation}

    Por tanto, queda probar que $rank(U(x)) \geq r; \dspace \text{cpd } x \in \R^{2MN + N}$. Definimos $U_r(x)$ como la submatriz superior izquierda $r \times r$ de $U(x)$.

    Si fuese $U_r(x)$ no singular para cierto $x$, claramente sabríamos que $r = rank(U_r(x)) \leq rank(U(x))$. Por tanto, buscamos probar que $U_r(x)$ es no singular salvo un conjunto de medida nula, es decir, que el conjunto $\conjunto{x \in \R^{2MN + N} : det(U_r(x)) = 0 }$ tiene medida nula.

    Para ver esto tenemos en cuenta que $det(U_r(\nv{x}))$ es un polinomio con entradas en $x \in \R^{2MN + N}$. Por \customref{lema:prop_fundamentales_medida} sabemos que entonces o bien es el polinomio idénticamente nulo o bien el conjunto de sus ceros tiene medida nula. Basta ver por tanto que no es el polinomio idénticamente nulo.

    Para ello basta ver que $\exists \nv{x_0} \in \R^{2MN + N}$ de forma que $det(U_r(\nv{x})) \neq 0$. Para ello, tomamos $x_0$ de forma que $D(x_0) = \mathbb{I}_d$ y que $A(\nv{x_0}) = B(\nv{x_0}) = (\delta_{i,j})_{i,j}$. Es decir, que $A(\nv{x_0}), B(\nv{x_0})$ son matrices, no necesariamente cuadradas, con unos en la diagonal y ceros en el resto de posiciones. En esta situación tenemos:

    \begin{equation}
        \begin{split}
            U_r(\nv{x_0}) &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \mathbb{I}d_{N \times N}
            \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}^T_{N \times M} = \ldots \\
            \ldots &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \mathbb{I}d_{N \times N}
            \begin{pmatrix}
                0 \\
                \hline
                \mathbb{I}d_{r \times r}
            \end{pmatrix}{N \times M} = \ldots \\
            \ldots &= \begin{pmatrix}
                \mathbb{I}d_{r \times r} & \vline & 0 \\
            \end{pmatrix}_{M \times N}
            \begin{pmatrix}
                0 \\
                \hline
                \mathbb{I}d_{r \times r}
            \end{pmatrix}_{N \times M} = \ldots \\
            \ldots &= \mathbb{I}d_M
        \end{split}
    \end{equation}

    Por tanto, es claro que

    \begin{equation}
        det(U_r(\nv{x_0})) = det(\mathbb{I}d_M) = 1 \neq -1
    \end{equation}

    como buscábamos probar.
    \todo{No he visto la parte en la que $Ax = Bx$}
\end{proof}

Veamos ahora el segundo lema:

\begin{lema} \label{lema:segundo_lema}
    Sean $p$ funciones continuas de $\R^d$ a $\espaciomatrices{M}{N}$, que llevan un punto $\nv{y} \in \R^d$ a las matrices $A_1(\nv{y}), \ldots, A_p(\nv{y})$. Supongamos que con estas funciones el siguiente conjunto tiene medida nula:

    \begin{equation}
        \conjunto{\nv{y} \in \R^d : rank(A_i(\nv{y})) < r, \dspace \forall i \in \deltaset{p}}
    \end{equation}

    Definimos la función

    \begin{equation}
        \begin{split}
            A: \R^P \times R^d &\to \espaciomatrices{M}{N} \\
            (x, y) &\mapsto \sum_{i = 1}^p x_i A_i(\nv{y})
        \end{split}
    \end{equation}

    Entonces el conjunto de puntos $(x, y) \in \R^p \times \R^d$ para los cuales $A(x, y) < r$ forman un conjunto de medida nula
\end{lema}

\begin{proof}

    Definimos el conjunto

    \begin{equation}
        S := \conjunto{(\nv{x}, \nv{y}) \in  \R^p \times \R^d: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p \times \R^d
    \end{equation}

    y lo que buscamos es probar que tiene medida nula.

    Como $A(x, y)$ es continua, y $\conjunto{A \in \espaciomatrices{M}{N}: rank(A) < r}$ es cerrado, entonces sabemos que $S$ es cerrado, y por tanto, medible.

    Ahora buscamos calcular la medida de $S$. Empezamos definiendo, para cada $y \in \R^d$, el siguiente conjunto marginal:

    \begin{equation}
        S^{\nv{y}} := \conjunto{x \in \R^p: rank(A(\nv{x}, \nv{y})) < r} \subseteq \R^p
    \end{equation}

    Con esto, basta probar que la medida de $S^{\nv{y}}$ es nula casi por doquier. Para ello definimos también

    \begin{equation}
        \mathcal{C} := \conjunto{\nv{y} \in \R^d: rank(A_i(y)) < r, \forall i \in \deltaset{p}}
    \end{equation}

    Por las hipótesis del lema, sabemos que $\mathcal{C}$ tiene medida nula. Luego queda ver que para $\nv{y_0} \in S - \mathcal{C}$ el conjunto $S^{\nv{y_0}}$ tiene medida nula.

    \begin{equation}
        \begin{split}
            \nv{y_o} \in S - \mathcal{C} \encima{\then}{\nv{y_o} \notin \mathcal{C}} \substack{\exists i \in \deltaset{p}: \\ rank(A_i(\nv{y_o})) \geq r}
        \end{split}
    \end{equation}

    Sin pérdida de generalidad, supongo que $i = 1$ y que la submatriz $r \times r$ superior izquierda de $A_1(\nv{y_0})$ es no singular. Viendo $\nv{y_o}$ como un valor fijo, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y})$ es un polinomio en $\nv{x}$. Además, no es el polinomio idénticamente nulo, porque tomando $\nv{x} = (1, \ldots, 0)$ tenemos que

    \begin{equation}
        A(\nv{x}, \nv{y_0}) = \sum_{i = 1}^{p} x_i A_i(\nv{y_0}) = A_1(\nv{y_0})
    \end{equation}

    que no es singular, luego su determinante no se anula. Por tanto, usando \customref{lema:prop_fundamentales_medida}, el determinante de la submatriz $r \times r$ superior izquierda de $A(\nv{x}, \nv{y})$ es un polinomio cuyo conjunto de ceros tiene medida nula. Y por lo tanto, el conjunto $S^{\nv{y_0}}$ tiene medida nula, porque casi todas las matrices $A(\nv{x}, \nv{y_0})$ tienen al menos rango $r$, como acabamos de ver.

    Ahora, usaremos el teorema de Fubini y el teorema de la convergencia monótona para demostrar lo que buscamos. $\mathbb{1}_A$ denotará la función indicadora del subconjunto $A$. Definimos los siguientes conjuntos:

    \begin{equation}
        \begin{split}
            S_n &:= S \interseccion [-n, n]^{p+d} \\
            \R_n &:= \R \interseccion [-n, n]^{p+d} = [-n, n]^{p+d}
        \end{split}
    \end{equation}

    Todos los conjuntos con los que estamos trabajando son medibles. Al intersecar con $[-n, n]^{p+d}$, además, tendrán medida finita. Aplicamos ahora el teorema de Fubini para desarrollar:

    \begin{equation}
        \begin{split}
            \int_{(\nv{x}, \nv{y}) \in \R^{p+d}} \mathbb{1}_{S_n} &= \int_{(\nv{x},\nv{y}) \in \R_n^{p+d}} \mathbb{1}_{S} \encima{=}{Fubini} \int_{\nv{y} \in \R_n^d} \int_{\nv{x} \in \R_n^p} \mathbb{1}_{S^y} = \ldots \\
            \ldots &= \comentardebajo{\int_{\nv{y} \in \R_n^d \interseccion \mathcal{y}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(1)} + \comentardebajo{\int_{\nv{y} \in \R_n^d - \mathcal{C}} \int_{\nv{x} \in \R^p_n} \mathbb{1}_{S^y}}{(2)} = \ldots \\
            \ldots &= 0 + 0 = 0
        \end{split}
    \end{equation}
    \todo{Habría que comprobar los supuestos de Fubini}

    Donde hemos usado:

    \begin{itemize}
        \item (1): hemos visto que la medida del conjunto $\mathcal{C}$ es cero
        \item (2): hemos visto que si $\nv{y} \in \R_n^d$, entonces el conjunto $S^{y}$ tiene medida nula
    \end{itemize}

    Y con ello hemos probado que $\int \mathbb{1}_{S_n} = 0$, sea cual sea el valor de $n \in \N$. Usamos ahora el \textbf{teorema de convergencia monótona} para calcular:
    \todo{Comprobar explícitamente los supuestos del TCM}

    \begin{equation}
        \int \mathbb{1}_S = \int \lim_{n \to \infty} \mathbb{1}_{S_n} \encima{=}{T.C.M} \lim_{n \to \infty} \int \mathbb{1}_{S_n} = \lim_{n \to \infty} 0  = 0
    \end{equation}

    Por lo tanto, la medida del conjunto $S$ es cero, como queríamos probar.

\end{proof}

Estamos ya en condiciones de probar el teorema principal del trabajo:

\begin{teorema}
\end{teorema}
\begin{proof}
    \textbf{Caso 1: sin compartición de coeficientes}

    \todo{Escribir en algun lado que es suficiente con mostrar que las matrizaciones tienen cierto rango}
    Como hemos hecho al introducir el modelo \textit{HT}, denotamos por conveniencia $\phi^{L, 1, 1}:= A^y$ y consideramos que $r_L = 1$.
    \todo{En la página 7 del \textit{paper} dicen que por completitud, $r_L = Y$}

    Probamos por inducción en $l = 1, \ldots, L$ que, en casi todo punto respecto al conjunto $\conjunto{\nv{a^{l, j, \gamma}}}_{l, j, \gamma}$, el rango \textit{CP} de los tensores $\conjunto{\phi^{l, j, \gamma}: j \in \deltaset{N/2^l}, \gamma \in \deltaset{r_l}}$ es al menos $r^{2^{l/2}}$.

    Por \customref{prop:rango_matricial_descomp_cp}, bastará probar que los rangos de las matrizaciones $[\phi^{l, j, \gamma}]$ tienen al menos rango matricial $r^{2^{l/2}}$ en casi todo punto. Comenzamos la inducción viendo el \textbf{caso $l = 1$}.

    Por definición, tenemos que para unos valores concretos de $j, \gamma$:

    \begin{equation}
        \phi^{1, j, \gamma} = \sum_{\alpha = 1}^{r_0} \comentardebajo{a_{\alpha}^{1, j, \gamma}}{\text{diagonal } D} \comentarencima{\varphi^{0, 2j - 1, \alpha}}{A \text{ por columnas}} \otimes \comentardebajo{\varphi^{0, 2j, \alpha}}{B \text{ por columnas}}
    \end{equation}

    Definimos tres matrices $A, B, D$:

    \begin{itemize}
        \item $A \in \espaciomatrices{M}{r_0}$ con columnas $\conjunto{\varphi^{0, 2j - 1, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $B \in \espaciomatrices{M}{r_o}$ con columnas $\conjunto{\varphi^{0, 2j, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $D \in \espaciomatrices{r_0}{r_0}$ con valores en la diagonal $a_{\alpha}^{1, j, \gamma}, \; \alpha \in \deltaset{r_0}$
    \end{itemize}

    Con ello podemos escribir $[\phi^{1, j, \gamma}] = A D B^T$. Aplicando \customref{lema:primer_lema}, sabemos que $rank([\phi^{1, j, \gamma}]) = r := \min \conjunto{r_0, M}$ en casi todo punto, respecto al conjunto de todos los parámetros que conforman $\phi^{1, j, \gamma}$. Estos parámetros son:

    \begin{itemize}
        \item $\conjunto{\varphi^{0, 2j - 1, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $\conjunto{\varphi^{0, 2j, \alpha}: \alpha \in \deltaset{r_0}}$
        \item $\conjunto{\nv{a^{1, j, \alpha}} : \alpha \in \deltaset{r_0}}$ donde $\nv{a^{1, j, \alpha}} := (a^{1, j, \alpha}_1, \ldots,  a^{1, j, \alpha}_{r_0})$
    \end{itemize}

    Por tanto, falta ver que dicha afirmación se mantiene para casi todo punto respecto al conjunto de todos los parámetros del modelo, no solo los que determinan $\phi^{1, j, \gamma}$. Pero esto es directo usando \customref{lema:prop_fundamentales_medida}.

    Ahora sabemos que $rank([\phi^{1, j, \gamma}]) = r$ en casi todo punto (respecto al conjunto de todos los parámetros que definen el modelo), para valores concretos de $j \in \deltaset{N/2}, \gamma \in r_1$. Veamos que esto implica que la igualdad sigue siendo válida para cualquier $j, \gamma$ de forma conjunta:

    \begin{equation}
    \begin{split}
        \phi_1^\gamma &:= \conjunto{j \in \deltaset{N/2}: rank([\phi^{1, j, \gamma}] \neq r} \text{ tiene medida } 0 \then \phi_1 := \union_{\gamma \in \deltaset{r_1}} \phi_1^\gamma \text{ tiene medida } 0 \then \ldots \\
        \ldots &\then \phi_1 \times \R \text{ tiene medida } 0 \then \ldots \\
        \ldots &\then \phi := \conjunto{(j, \gamma) \in \deltaset{N/2} \times \deltaset{r_1}: rank([\phi^{1, j, \gamma}]) \neq r} \subseteq \phi_1 \times \R \text{ tiene medida } 0
    \end{split}
    \end{equation}

    Esto prueba el caso $l = 1$.

    \textbf{Hipótesis de inducción}: supuesto que es cierto para $l - 1$, ¿esto implica que sea cierto para $l$?.

    Usando la hipótesis de inducción sabemos que

    \begin{equation}
        rank(\phi^{l-1, j', \gamma'}) \geq r^{2^{\frac{l - 1}{2}}},
        \; \forall j' \in \deltaset{N/2^{l-1}}, \forall \gamma' \in \deltaset{r_{l-1}}
    \end{equation}

    en casi todo punto (respecto al conjunto de todos los parámetros del modelo). Fijados unos valores concretos para $j' \in \deltaset{N/2^{l-1}}$, $\gamma' \in \deltaset{r_{l-1}}$, tenemos que:

    \begin{equation}
    \begin{split}
        \phi^{l, j, \gamma} &:= \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot \phi^{l-1, 2j-1, \alpha} \otimes \phi^{l-1, 2j, \alpha} \then \\
        &\then [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot [\phi^{l-1, 2j-1, \alpha}] \odot [\phi^{l-1, 2j, \alpha}]
    \end{split}
    \end{equation}

    Ahora definimos $M_\alpha := [\phi^{l-1, 2j-1, \alpha}] \odot [\phi^{l-1, 2j, \alpha}]$, $\forall \alpha \in \deltaset{r_{l-1}}$ y aplicamos la hipótesis de inducción junto al hecho de que $rank(A \odot B) = rank(A) \cdot rank(B)$:

    \begin{equation}
    \begin{split}
        \substack{rank([\phi^{l-1, 2j-1, \alpha}]) \geq r^{\frac{2^{l-1}}{2}} cpd \\ rank([\phi^{l-1, 2j-1, \alpha}]) \geq r^{\frac{2^{l-1}}{2}} cpd} \then         rank(M_\alpha) \geq r^{\frac{2^{l-1}}{2}} \cdot r^{\frac{2^{l-1}}{2}} = r^{\frac{2^{l-1}}{2} + \frac{2^{l-1}}{2}} =
        r^{\frac{2 \cdot 2^{l - 1}}{2}} = r^{\frac{2^{l}}{2}} \\
    \end{split}
    \end{equation}
    \todo{Corregir los exponentes r elevado a algo antes de este punto. estos son los correctos creo}

    Ahora, usando la definición de $M_\alpha$, calculamos:

    \begin{equation}
        [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_{\alpha}^{l, j, \gamma} \cdot M_\alpha
    \end{equation}

    Como $\conjunto{M_\alpha}_\alpha$ no depende de $\nv{a^{l, j, \gamma}}$ porque solo codifica las entradas tensoriales del nivel anterior, podemos aplicar \customref{lema:segundo_lema}:

    \begin{equation}
    \begin{split}
        \substack{
            rank(M_\alpha) \geq r^{\frac{2^l}{2}}\\
            \conjunto{\nv{a^{0, j, \gamma}}} \to M_\alpha \text{ continuo } \\
            \text{por ser productos tensoriales, sumas, matrizaciones}
        } \then \\
        \then [\phi^{l, j, \gamma}] = \sum_{\alpha = 1}^{r_{l-1}} a_\alpha^{l,j,\gamma} M_\alpha \text{ tiene rango al menos } r^{\frac{2^l}{2}} \text{ c.p.d. }
    \end{split}
    \end{equation}

    Como $rank([\phi^{l, j, \gamma}]) \geq r^{\frac{2^l}{2}}$ casi por doquier para $(j, \gamma) \in \deltaset{N/2^l} \times \deltaset{r_{l-1}}$, podemos hacer un argumento análogo al anterior para extender esta propiedad a todo el espacio.

    Esto concluye la prueba para el caso no compartido

    \textbf{Caso 2: Compartición de coeficientes}

    Es totalmente análogo al caso 1. En la inducción, en el paso $l = 1$, usamos la versión del lema \customref{lema:primer_lema} en la que $A(\nv{x}) = B(\nv{x})$
\end{proof}

Para demostrar el corolario, necesitaremos primero probar el siguiente lema:

\begin{lema}
    Sean $f_{\theta_1}, \ldots, f_{\theta_M} \in L^2(\R^s)$ un conjunto de funciones linealmente independientes. Dado un tensor $\mathcal{A} \in \espaciotensores{N}{M}$ definimos la siguiente función:

    \begin{equation}
    \begin{split}
        h(\mathcal{A}): (\R^s)^N &\to \R \\
        (\nv{x_1}, \ldots, \nv{x_N}) &\mapsto \sum_{d_1, \ldots, d_N = 1}^M \mathcal{A}_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i})
    \end{split}
    \end{equation}

    Que es la función de puntuación introducida en \eqref{eq:puntuacion_general}. Sea $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda} \subseteq \espaciotensores{N}{M}$ una familia de tensores, y $\mathcal{A}^*$ un tensor que no esté en esa familia. Supongamos que $rank([\mathcal{A}^\lambda]) < rank([\mathcal{A}^*])$, $\forall \lambda \in \Lambda$. Entonces la distancia en $L^2((\R^s)^N)$ entre $h(\mathcal{A}^*)$ y el conjunto $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva. Es decir, $\forall \epsilon > 0$ se tiene que

    \begin{equation}
        \int |h(\mathcal{A}^\lambda) - h(\mathcal{A}^*)|^2 > \epsilon; \; \forall \lambda \in \Lambda
    \end{equation}
\end{lema}

\begin{proof}
    \todo{Revisar esta prueba porque doy algunos pasos de los que no estoy nada seguro}
    Como la familia de funciones $f_{\theta_1}, \ldots, f_{\theta_M} \in L^2(\R^s)$ son linealmente independientes, sabemos que la familia de funciones producto inducida

    \begin{equation}
        \zeta = \conjunto{\prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i}): d_1, \ldots d_N \in \deltaset{M}}
    \end{equation}

    también es linealmente independiente (\customref{prop:conservacion_totalidad_indp_lineal_func_prod}).

    Consideramos ahora una secuencia de funciones $\conjunto{h^{(t)}}_{t \in \N}$ que vivan en $span \zeta$. Para cada $t \in \N$ denotamos por $\mathcal{A}^{(t)}$ al tensor de coeficientes de $h^{(t)}$ en la \entrecomillado{base} dada por $\zeta$, gracias a que nuestra familia de funciones $\zeta$ es linealmente independiente. Es decir, $\mathcal{A}^{(t)} \in \espaciotensores{N}{M}$ viene dado por:

    \todo{No estoy seguro de la frase \entrecomillado{gracias a que nuestra familia de funciones $\zeta$ es linealmente independiente}, repasarlo bien}

    \begin{equation}
        h^{(t)}(\nv{x_1}, \ldots, \nv{x_N}) = \sum_{d_1 = 1, \ldots, d_N = 1}^M \mathcal{A}^{(t)}_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i})
    \end{equation}

    En el caso que $\conjunto{h^{(t)}}_{t \in \N}$ no converge a $h(\mathcal{A}^*)$ nuestra proposición es trivial. Por lo tanto, supongamos que $\conjunto{h^{(t)}}_{t \in \N}$ converge a $h(\mathcal{A}^*)$ en $L^2((\R^s)^N)$, es decir:

    \begin{equation}
        \lim_{t \to \infty} d_{L^2}(h^{(t)}, h(\mathcal{A}^*)) = 0
    \end{equation}

    O lo que es lo mimso:

    \begin{equation}
        \lim_{t \to \infty} \int | h^{(t)} - h(\mathcal{A}^*)|^2 = 0
    \end{equation}

    En un espacio de Hilbert de dimensión finita, como es nuestro caso, convergencia en norma implica convergencia en los coeficientes de representación. Como hemos supuesto la convergencia en norma, con lo anterior tenemos que:
    \todo{Tendría que justificar esto de alguna forma}

    \begin{equation}
        \mathcal{A}^{(t)} \encima{\longmapsto}{t \mapsto \infty} \mathcal{A}^*_{d_1, \ldots, d_N},
        \; \forall d_1, \ldots, d_N \in \deltaset{M}
    \end{equation}

    Luego en el espacio $\espaciotensores{N}{M}$ el tensor $\mathcal{A}^*$ pertenece al cierre de $\conjunto{\mathcal{A}^{(t)}: t \in \N}$. Por tanto, para probar que la distancia en $L^2((\R^s)^N)$ entre $h(\mathcal{A}^*)$ y $\conjunto{h(\mathcal{A}^\lambda): \lambda \in \Lambda}$ es estrictamente positiva bastará con ver que la distancia en $\espaciotensores{N}{M}$ entre $\mathcal{A}^*$ y $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva. Veamos esto

    \begin{equation}
    \begin{split}
        h^{(t)} \encima{\longmapsto}{t \mapsto \infty} h(\mathcal{A}^*) &\iif \sum \mathcal{A}^{(t)}_{d_1, \ldots, d_N} \prod f_{\theta_{d_i}}(\nv{x_i}) = \sum \mathcal{A}^*_{d_1, \ldots, d_N} \prod f_{\theta_{d_i}}(\nv{x_i}) \then \ldots \\
        \ldots &\then \mathcal{A}^{(t)} \encima{\longmapsto}{t \mapsto \infty} \mathcal{A}^* \then \ldots \\
        \ldots &\then \lim_{t \to \infty} d(A^{(t)}, \mathcal{A}^*) = 0
    \end{split}
    \end{equation}

    Y por contrarrecíproco, es claro entonces que

    \begin{equation}
        d(A^{(t)}, \mathcal{A}^*) \not\longto 0 \then h^{(t)} \not\longto h(\mathcal{A}^*)
    \end{equation}

    Que la distancia en $\espaciotensores{N}{M}$ entre $\mathcal{A}^*$ y $\conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda}$ es estrictamente positiva equivale a su vez a que la distancia entre $[\mathcal{A}^*]$ y $\conjunto{[\mathcal{A}^\lambda]: \lambda \in \Lambda}$ es estrictamente positiva.

    \todo{Entiendo la equivalencia, y es intuitivo, pero no veo la conexión entre el argumento anterior y la equivalencia}
    \todo{También habrá que ver que la matrización conserva distancias}

    Ahora, esto último es claro puesto que:

    \begin{equation}
        rank([\mathcal{A}^\lambda]) < rank([\mathcal{A}^*]), \; \forall \lambda \in \Lambda \then d(\mathcal{A}^*, \conjunto{\mathcal{A}^\lambda: \lambda \in \Lambda} > 0
    \end{equation}
    \todo{Obvio pero habŕa que demostrarlo. No conocemos $\Lambda$ y podríamos tener una sucesión de distancias que converge a cero}
    \todo{Para que usamos que la familia de funciones sea linealmente independiente?}
\end{proof}

Estamos ya en condiciones de enunciar el corolario:

\begin{corolario}
\end{corolario}
