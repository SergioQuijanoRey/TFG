\chapter{Teoremas de la capacidad de las redes neuronales}

\section{Introducción a los teoremas}

Los teoremas principales de este trabajo se presentan en esta sección. Comenzamos por el siguiente:

\begin{teorema}[Darle nombre]

Sea $\mathcal{A}^y \in \espaciotensores{N}{M}$ dado por las ecuaciones \eqref{eq:descomposicion_ht}. Definamos $r := \min{r_0, M}$ y consideremos el espacio de todas las posibles configuraciones de parámetros de nuestro modelo $\{ \nv{a^{l, j, \gamma}} \}_{l, j, \gamma}$. En este espacio, el tensor generado $\mathcal{A}^y$ tendrá rango \textit{CP} de al menos $r^{N/2}$ casi por doquier. Es decir, el conjunto de tensores generados con rango \textit{CP} menor que $r^{N/2}$ tiene medida cero. El resultado se mantiene si forzamos la compartición de coeficientes en \eqref{eq:descomposicion_ht}. Es decir, haciendo $\nv{a^{l, \gamma}} \equiv \nv{a^{l, j, \gamma}}$ y considerando el espacio de configuraciones $\{ \nv{a^{l, \gamma}}  \}_{l, \gamma}$.

\end{teorema}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

\begin{observacion}

    Casi todos los tensores que podemos generar con un modelo \textit{HT} tienen rango \textit{CP} de al menos $r^{N/2}$.

\end{observacion}

\begin{observacion}
    $\mathcal{A}^y$ claramente es realizable, por ser el resultado de un modelo \textit{HT}
\end{observacion}

\begin{observacion}

    Que $\mathcal{A}^y$ tenga rango \textit{CP} igual a $r^{N/2}$ implica que en la ecuación \eqref{eq:cp_decomp}, el número de sumandos deberá ser al menos $r^{N/2}$. Luego, conforme el tamaño de la imagen $N$ aumenta, el número de parámetros del modelo \textit{CP} crece exponencialmente. Conforme el número de canales $M$ de la imagen crece, el número de parámetros del modelo \textit{CP} crece pero de forma potencial.

\end{observacion}

A partir de este teorema, obtenemos el siguiente corolario:

\begin{corolario}[Darle nombre] \label{corol:corol_principal}

Dado un conjunto de funciones de representación $\{f_{\theta_d}: d \in \deltaset{M} \}$ linealmente independientes, aleatorizando los pesos de un modelo \textit{HT} \eqref{eq:descomposicion_ht} a partir de una distribución de probabilidad continua, induce funciones de puntuación $h_y$ que con probabilidad uno no pueden ser aproximadas arbitrariamente bien (en el sentido $L^2$) por un modelo \textit{CP} con menos de $r := \min \{r_0, M \}^{N/2}$ canales. Este resultado se mantiene forzando la compartición de coeficientes en el modelo \textit{HT} mientras que dejamos el modelo \textit{CP} sin restricciones.

\end{corolario}
\todo{Este teorema está muy mal escrito en el \textit{paper}}

Es decir, salvo un conjunto de medida nula, todos los tensores realizables por una descomposición \textit{HT} no pueden realizarse, ni siquiera aproximarse, por una descomposición \textit{CP} con menos de un número exponencial de parámetros.

El \customref{corol:corol_principal} introduce una mejora muy grande respecto a los trabajos previos. En estos, se aportan ejemplos de funciones realizables por el modelo \textit{HT} que no son realizables por un modelo \textit{CP} con un número de parámetros sustancialmente superior al número de parámetros del modelo \textit{HT}. Es decir, establecen la \textit{depth efficiency} de ejemplos concretos de funciones. Nuestro corolario establece la \textit{depth efficiency} casi por doquier.

Estos resultados también mejoran los trabajos previos sobre análisis tensorial. El trabajo que introduce la descomposición \textit{HT} \cite{matematicas:descomposicion_ht} da ejemplos específicos de tensores realizables por el modelo \textit{HT} y no por un modelo \textit{CP} con un número exponencial de parámetros. Nuestro corolario indica que esto no ocurre en ejemplos aislados, sino en casi todos los tensores realizables por un modelo \textit{HT}.

Teniendo en cuenta que cualquier tensor realizable por un modelo \textit{CP} puede ser realizado por un modelo \textit{HT} con un aumento polinomial del número de coeficientes (véase \customref{msubs:parametros_modelo_ht}), esto implica que la descomposición \textit{HT} es asintóticamente más eficiente que la \textit{CP}.
\todo{Aquí no he definido lo que entendemos por eficiencia. Creo que sería \textit{depth efficiency}}

\section{Lemas previos}

En esta sección introduciremos algunos resultados que serán usados en las demostraciones del teorema \customref{} y el corolario \customref{}. Empezamos con los siguientes resultados de teoría de la medida de Lebesgue:

\begin{lema}[Propiedades fundamentales de la medida] Las siguientes propiedades sobre la medida de Lebesgue son ciertas:

\begin{enumerate}
    \item La unión contable de conjuntos de medida nula es un conjunto de medida nula
    \item Si $A \subseteq \R^{N_1}$ tiene medida nula, entonces $A \times R^{N_2} \subseteq R^{N_1 + N_2}$ también tiene medida nula
    \item Sea $p \in \R^d[x]$, es decir, un polinomio sobre los números reales con $d$ variables. Si $p$ no es el polinomio idénticamente nulo, entonces el conjunto de ceros del polinomio es un conjunto de medida nula.
\end{enumerate}

\end{lema}

Usaremos la siguiente \textbf{matrización} de un tensor:

\begin{definicion}[Matrización de un tensor]

Sea $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$ un tensor de orden $N$ y dimensión $M_i, i \in \deltaset{N}$ en cada modo. Por simplicidad, supondremos que el orden del tensor es par. Denotaremos por $[\mathcal{A}]$ a la \textbf{matrización} del tensor $\mathcal{A}$. $[\mathcal{A}]$ es una matriz donde las filas corresponden a modos impares del tensor, y las columnas corresponden a los modos pares. Por lo tanto, $[\mathcal{A}]$ tiene $M_1 \cdot M_3 \cdot \ldots \cdot M_{N-1}$ filas y $M_2 \cdot M_4 \cdot \ldots \cdot M_N$ columnas. Asignaremos el valor $\mathcal{A}_{d_1, \ldots, d_N}$ en $[\mathcal{A}]$ en la posición:

\begin{itemize}
    \item Fila $1 + \sum_{i = 1}^{N/2} (d_{2i - 1} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j - 1}$
    \item Columna $1 + \sum_{i = 1}^{N/2} (d_{2i} - 1) \; \prod_{j = i + 1}^{N/2} M_{2j}$
\end{itemize}

\end{definicion}

Con esto, tenemos que saber cómo se comporta la matrización con el producto tensorial. Esto lo introduce la siguiente proposición:

\begin{definicion}[Producto de Kronecker]

\end{definicion}

\begin{proposicion}[Matrización y producto tensorial]

    Sean $\mathcal{A}, \mathcal{B}$ dos tensores. Entonces se verifica:

    \begin{equation}
        [\mathcal{A} \otimes \mathcal{B}] = [\mathcal{A}] \odot [\mathcal{B}]
    \end{equation}

\end{proposicion}

También tenemos que saber cómo se comporta el producto de Kronecker entre matrices y el rango matricial:

\begin{proposicion}[Rangos matriciales y producto de Kronecker]

    Sean $A, B$ dos matrices. Entonces se verifica que:

    \begin{equation}
        rank(A \odot B) = rank(A) \cdot rank(B)
    \end{equation}

\end{proposicion}

\section{Demostración de los teoremas}
