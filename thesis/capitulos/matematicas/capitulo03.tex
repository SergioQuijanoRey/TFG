\chapter{Modelización de las redes neuronales} \label{ch:modelizacion}

A partir de las herramientas matemáticas que hemos introducido en \customref{ch:matematicas_fundamentales}, buscamos desarrollar una modelización matemática de las redes neuronales con las que se suele trabajar en la práctica. Para que sea una buena modelización, debería cumplirse que:

\begin{itemize}
    \item Sea lo más parecida a los modelos que se usan en la práctica
    \item Permita obtener resultados interesantes
\end{itemize}

Usando descomposiciones tensoriales, modelaremos dos tipos de redes:

% TODO -- no se que nombre le dan a cada una de las redes en el paper
\begin{itemize}
    % TODO -- hay que darle un nombre a esto
    \item Redes neuronales no profundas, a partir
    \item Redes convolucionales profundas, a partir de los \textit{circuitos convolucionales aritméticos}
\end{itemize}

Creemos que la modelización es muy cercana a las redes usadas en la práctica. Principalmente, porque tiene en cuenta las tres propiedades características de una red convolucional:

\begin{enumerate}
    \item Localidad
    \item Compartición de parámetros, que junto a la localidad, da lugar a la convolución
    \item \textit{Pooling}
\end{enumerate}

Además, los \textit{circuitos convolucionales aritméticos} son equivalentes a las redes conocidas como \textit{SimNets}, lo que reafirma el hecho de que la modelización es muy buena.

\section{Tarea de aprendizaje}

La tarea de aprendizaje que consideramos es la de clasificación. Durante todo el desarrollo, pensaremos en la clasificación de imágenes. En este caso, el enfoque que más éxito ha tenido históricamente es el uso de redes convolucionales profundas.

Dado un elemento $X = (\mathbf{x_1}, \ldots, \mathbf{x_N})$, donde $\mathbf{x_i} \in \R^s \ \forall i \in \deltaset{N}$, queremos clasificarlo en alguna de las etiquetas $\mathcal{Y} = \{1, \ldots, Y \}$.

Con esto, podemos ver que los datos de entrada viven en el espacio

$$\mathcal{X} := \R^s \times \overset{N}{\ldots} \times \R^s = (R^s)^N$$

Esta representación de los datos de entrada es natural en muchas situaciones. En el caso de las imágenes, podemos considerar cada vector $\mathbf{x_i}$ como un conjunto de \textit{pixels} de la imagen. Idealmente, cada vector de \textit{pixels} debería contener un vecindario de \textit{pixels}, es decir, \textit{pixels} adyacentes. Podemos incluso considerar \textit{pixels} que aparezcan en más de un vector (es decir, vectores $\mathbf{x_i}$ que se intersequen). Por ejemplo, podemos considerar $\mathbf{x_i}$ como la fila o columna $i$-ésima de la imagen.

Para decidir la etiqueta de un elemento, consideramos $Y$ funciones de puntuación

$$\{h_y: \mathcal{X} \to \R \dspace / \dspace y \in \mathcal{Y} \}$$

Con esto, dado un elemento $X \in \mathcal{X}$, lo clasificaremos buscando la etiqueta cuya función de puntuación sea máxima, es decir:

$$label(X) := \underset{y \in \mathcal{Y}}{argmax} \dspace h_y(X)$$

Por tanto, nuestro \textbf{espacio de hipótesis} $\Gamma$ es el conjunto de funciones $\mathcal{X} \to \R$. Tanto en la práctica con modelos de \textit{machine learning}, como en nuestras dos modelizaciones, trabajamos en un subconjunto $\Gamma_{restringido} \subset \Gamma$ de funciones de puntuación, implementables o bien por el modelo de \textit{machine learning} o bien por nuestra modelización teórica.

\section{Espacio de hipótesis general}

\subsection{Justificación para la representación de las funciones de puntuación} \label{sec:justificacion_func_repr}

% TODO -- desarrollar el apéndice 6 donde se justifica que esta ecuación es lo
% suficientemente general
TODO -- desarrollar el apéndice 6 donde se justifica que esta ecuación es lo suficientemente general

\subsection{Representación de las funciones de puntuación} \label{sec:repr_funciones_puntuacion}

Por todo esto, las funciones de puntuación vendrán dadas de la forma:

\begin{equation} \label{eq:puntuacion_general}
    h_y(\mathbf{x_1}, \ldots, \mathbf{x_N}) = \sum_{d_1, \ldots, d_N = 1}^{M} \mathcal{A}^y_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\mathbf{x_i})
\end{equation}

Explicamos ahora algunos detalles sobre esta ecuación, que será central en nuestro trabajo.

En primer lugar, usamos la notación $\sum_{d_1, \ldots, d_N = 1}^{M}$ para denotar $\sum_{d_1 = 1}^{M} \sum_{d_2 = 1}^{M} \ldots \sum_{d_N = 1}^{M}$

Las funciones $f_{\theta_1}, \ldots, f_{\theta_M}: \R^s \to \R$ son las \textbf{funciones de representación}. Cada una de estas funciones se selecciona de una familia paramétrica

$$\mathcal{F} = \{ f_{\theta}: \R^s \to \R / \theta \in \Theta \}$$

Algunas funciones de representación usuales son:

\begin{itemize}
    \item \textit{Wavelets}
    \item Funciones de base radial (\textit{RBF}), normalmente la Gaussiana
    \item Neuronas
\end{itemize}

% TODO -- no he justificado que haya funciones de representacion linealmente independientes
El tensor $\mathcal{A}^y$ será el \textbf{tensor de coeficientes}. Por la sumatoria en \eqref{eq:puntuacion_general}, es claro que tiene orden $N$ y dimensión $M$ en cada modo. Por lo comentado en \customref{sec:justificacion_func_repr}, normalmente consideramos funciones de representación linealmente independientes.

Estamos usando las mismas funciones de representación $f_{\theta_1}, \ldots, f_{\theta_M}: \R^s \to \R$ para todas las funciones de representación $h_y, y \in \mathcal{Y}$, lo único que cambia entre las distintas funciones de puntuación es el tensor de coeficientes $\mathcal{A}^y$.

La tarea de aprendizaje ahora será aprender los valores de los parámetros $\theta_1, \ldots, \theta_M$ y los valores de los tensores de coeficientes $\mathcal{A}^1, \ldots, \mathcal{A}^Y$.

\begin{observacion}
    Notar que en la ecuación \refeq{eq:puntuacion_general}, los vectores de entrada $\mathbf{x_i}$ solo participan en el productorio que involucra computar $f_{\theta_{d_i}}(\mathbf{x_i})$.

    Por tanto, podemos considerar como paso inicial el cómputo de los valores

    $$\{f_{\theta_d}(x_i) / d \in \deltaset{M},\ i \in \deltaset{N} \}$$

    % Lo de los M canales lo tengo mas dudoso, porque no es necesariamente igual
    % que en una capa convolucional de machine learning
    Una vez que hayamos computado esos $M \cdot N$ valores, ya no necesitamos los valores $\mathbf{x_i}$ para nada más. Con esto, es natural considerar que nuestro modelo tenga una primera capa que compute esos valores. Podemos considerar dicha capa como una \textbf{primera capa convolucional} con $M$ canales, a la que llamaremos \textbf{capa de representación}.

    Y como ya hemos comentado, estamos usando las mismas funciones de representación para las $Y$ funciones de puntuación. Por tanto, en todas las funciones de puntuación, la capa de representación será la misma.
\end{observacion}

\begin{observacion}
    Si consideramos el espacio de funciones generado por la base

    $$\{(\mathbf{x_1}, \ldots, \mathbf{x_N}) \mapsto \prod_{i = 1}^{N} f_{\theta_{d_i}} (\mathbf{x_i}) / d_i \in \deltaset{M} \forall i \in \deltaset{N}\}$$

    entonces el tensor $\mathcal{A}^y$ nos da los coeficientes en dicha base de la función de puntuación $h_y$.
\end{observacion}

Como ya hemos dicho, los tensores de coeficientes tienen orden $N$ y dimensión $M$ en cada modo. Por tanto, cada tensor tiene $M^N$ valores reales, que deberemos aprender. Esto supone un reto, lo que \textbf{motiva uso de los dos modelos}, que se basarán en descomposiciones tensoriales para que el aprendizaje de los tensores de coeficientes sea más factible.

\section{Modelo CP}

Como ya hemos comentado en \customref{sec:repr_funciones_puntuacion}, buscaremos descomponer el tensor de coeficientes $\mathcal{A}^y$ en la ecuación \eqref{eq:puntuacion_general}, para que el aprendizaje de sus coeficientes sea computacionalmente factible.

La idea más simple es aplicar la \textit{descomposición CP}, que introducimos en \eqref{eq:cp_decomp}, en la ecuación \eqref{eq:puntuacion_general}. Usaremos una descomposición conjunta para los tensores:

\begin{equation} \label{eq:cp_decomp_conjunta}
    \mathcal{A}^y = \sum_{z = 1}^Z a_z^y \cdot \boldsymbol{\omega}^{z, 1} \otimes \ldots \otimes \boldsymbol{\omega}^{z, N}
\end{equation}

Desarrollemos los elementos que participan en esa ecuación. En primer lugar, sabemos por \eqref{eq:cp_decomp} que $a_z^y \in \R$, y por tanto, podemos considerar $\mathbf{a}^y := (a_1^y, \ldots, a_Z^y)^T \in \R^Z$. En segundo lugar, y de nuevo, conforme a \eqref{eq:cp_decomp}, tenemos los vectores $\boldsymbol{\omega^{z, i}} \in \R^M$ con $z \in \deltaset{Z}, i \in \deltaset{N}$. Decimos que la \textbf{descomposición es conjunta} porque los vectores $\boldsymbol{\omega}^{z, i}$ son los mismos para todos los valores de $y \in \mathcal{Y}$. Solo cambian los coeficientes $\mathbf{a}^y$.

\begin{proposicion}
    Tomando $Z = M^N$ en la ecuación \refeq{eq:cp_decomp_conjunta}, la descomposición es universal. Esto es, podemos fijar un conjunto de vectores $\{\boldsymbol{\omega}^{z, i} \in \R^M / z \in \deltaset{Z}, i \in \deltaset{N} \}$ de forma que cualquier tensor $\mathcal{A}^y$ de orden $N$ y dimensión $M$ en cada modo puede ser representado. Es decir:

    \begin{equation}
        \forall \mathcal{A}^y \in \esptensores{N}{M}, \exists \mathbf{a}^y \in \R^Z: \text{la ecuación \refeq{eq:cp_decomp_conjunta} se verifica}
    \end{equation}
\end{proposicion}

\begin{proof}
    La idea de la demostración es muy sencilla. Tenemos que expresar un tensor arbitrario $\mathcal{A}$ de orden $N$ y dimensión $M$ en cada modo (es decir, de $M^N$ elementos) como una suma de $M^N$ elementos. Por tanto, en cada sumando, buscamos generar un elemento del tensor y colocarlo en los índices apropiados.

    Para que la demostración sea más clara, y como $Z = M^N$, para cada $z \in \deltaset{Z}$ podemos expresar $z = (i_1, \ldots, i_N)$ con $i_j \in \deltaset{M}$, y también podemos expresar la sumatoria como:

    \begin{equation}
        \mathcal{A} = \sum_{i_1 = 1, \ldots, i_N = 1}^M a_{(i_1, \ldots, i_N)} \cdot \boldsymbol{\omega}^{(i_1, \ldots, i_N), 1} \otimes \ldots \otimes \boldsymbol{\omega}^{(i_1, \ldots, i_N), N}
    \end{equation}

    Para llevar a cabo la idea de usar cada sumando para colocar un elemento de $\mathcal{A}$ en sus índices correctos, haremos que en cada sumando se verifique

    \begin{itemize}
        \item $a_{i_1, \ldots, i_N} = \mathcal{A}_{i_1, \ldots, i_N}$. Esto es inmediato y no necesita más explicación
        \item $\boldsymbol{\omega}^{(i_1, \ldots, i_N), 1} \otimes \ldots \otimes \boldsymbol{\omega}^{(i_1, \ldots, i_N), N}$ genere el tensor de orden $N$ y dimensión $M$ en cada modo que sea cero en todas las entradas salvo en el índice $(i_1, \ldots, i_N)$, al que podemos denotar $\mathcal{B}^{(i_1, \ldots, i_N)}$
    \end{itemize}

    Ahora, veamos cómo podemos generar el tensor $\mathcal{B}^{(i_1, \ldots, i_N)}$ como producto tensorial de ciertos vectores. La propiedad fundamental que queremos que ese tensor cumpla es:

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)}_{j_1, \ldots, j_N} =
        \begin{cases}
            1 & \text{si } i_1 = j_1, \ldots, i_N = j_N \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation}

    Definimos $\vec{\delta}_{i, N}$ como el vector de longitud $N$, con todas las entradas nulas salvo la entrada de la posición $i$, en la que ponemos un uno. Por tanto, es claro que:

    \begin{equation}
        (\vec{\delta}_{i, N})_k =  \delta_{i, k}
    \end{equation}

    Definimos

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)} := \vec{\delta}_{i_1, N} \otimes \ldots \otimes \vec{\delta}_{i_N, N}
    \end{equation}

    y por tanto se verifica que:

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)}_{j_1, \ldots, j_N} = (\vec{\delta}_{i_1, N})_{j_1} \cdot \ldots \cdot (\vec{\delta}_{i_N, N})_{j_N} = \delta_{i_1, j_1} \cdot \ldots \cdot \delta_{i_N, j_N} =
        \begin{cases}
            1 & \text{si } i_1 = j_1, \ldots, i_N = j_N \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation}

    Para concluir la demostración, fijamos el conjunto de vectores de forma que:

    \begin{equation} \label{eq:cp_decomp_asignacion_vectores}
        \{\boldsymbol{\omega}^{z, i} \in \R^M / z \in \deltaset{Z}, i \in \deltaset{N} \} =
        \{ \vec{\delta}_{i_j, N} / j \in \deltaset{N}, i_j \in \deltaset{M} \}
    \end{equation}

\end{proof}

\begin{observacion}
    En \eqref{eq:cp_decomp_asignacion_vectores} vemos que estamos usando menos vectores que los que se consideran en el enunciado del teorema. Esto es porque estamos repitiendo estos vectores. Por ejemplo, todos los sumandos referentes al valor 1 del primer índice tiene como primer vector $\vec{\delta}_{1, N}$
\end{observacion}

\begin{observacion}
    Esta proposición nos sirve para tener una cota superior del número de sumandos necesarios para realizar la descomposición. No hemos ganado nada respecto al trabajo previo. Seguimos teniendo el problema de considerar $M^N$ elementos, problema que ya vimos en \customref{sec:justificacion_func_repr}
\end{observacion}

\endinput
