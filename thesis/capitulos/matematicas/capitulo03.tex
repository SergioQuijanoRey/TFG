\chapter{Modelización de la tarea de aprendizaje}

La tarea de aprendizaje que consideramos es la de clasificación. Durante todo el desarrollo, pensaremos en la \textbf{clasificación de imágenes}. En este caso, el enfoque que más éxito ha tenido históricamente es el uso de redes convolucionales profundas y, más actualmente, el uso del modelo \entrecomillado{transformer}.

Dado un elemento $X = (\nv{x_1}, \ldots, \nv{x_N})$, donde $\nv{x_i} \in \R^s \ \forall i \in \deltaset{N}$, queremos clasificarlo en alguna de las etiquetas $\mathcal{Y} = \{1, \ldots, Y \} = \deltaset{Y}$.

Con esto, podemos ver que los datos de entrada viven en el espacio

$$\mathcal{X} := \R^s \times \overset{N}{\ldots} \times \R^s = (R^s)^N$$

Esta representación de los datos de entrada es natural en muchas situaciones. En el caso de las imágenes, podemos considerar cada vector $\nv{x_i}$ como un conjunto de \textit{pixels} de la imagen. Idealmente, cada vector de \textit{pixels} debería contener un vecindario de \textit{pixels}, es decir, \textit{pixels} adyacentes. Podemos incluso considerar \textit{pixels} que aparezcan en más de un vector. Por ejemplo, podemos considerar $\nv{x_i}$ como la fila o columna $i$-ésima de la imagen.

Por ejemplo, el trabajo que presenta el conocido modelo  de deep learning \textit{VIT} propone una arquitectura \entrecomillado{transformer} a la que que se le pasa \entrecomillado{patches} de la imagen para poder explotar el mecanismo de atención: \entrecomillado{we split an image into patches and provide the sequence of linear embeddings of these patches as an input to a Transformer} \footnote{\cite{matematicas:vit}}
\todo{No sé si así está bien referenciado el trabajo al que me refiero}

Para decidir la etiqueta de un elemento, consideramos $Y$ \textbf{funciones de puntuación}:

$$\{h_y: \mathcal{X} \to \R \dspace / \dspace y \in \mathcal{Y} \}$$

Con esto, dado un elemento $X \in \mathcal{X}$, lo clasificaremos buscando la etiqueta cuya función de puntuación sea máxima, es decir:

$$\hat{y} := \underset{y \in \mathcal{Y}}{argmax} \dspace h_y(X)$$

Por tanto, nuestro \textbf{espacio de hipótesis} $\Gamma$ es el conjunto de funciones $\mathcal{X} \to \R$. Tanto en la práctica con modelos de \textit{machine learning}, como en nuestras dos modelizaciones, trabajamos en un subconjunto $\tilde{\Gamma} \subset \Gamma$ de funciones de puntuación, implementables o bien por el modelo de \textit{machine learning} o bien por nuestra modelización teórica.

\section{Espacio de hipótesis general}

\subsection{Justificación para la representación de las funciones de puntuación} \label{sec:justificacion_func_repr}

\todo{Desarrollar el apéndice 6 donde se justifica que esta ecuación es lo suficientemente general}

\subsection{Representación de las funciones de puntuación} \label{sec:repr_funciones_puntuacion}

Por todo esto, las funciones de puntuación vendrán dadas de la forma:

\begin{equation} \label{eq:puntuacion_general}
    h_y(\nv{x_1}, \ldots, \nv{x_N}) = \sum_{d_1, \ldots, d_N = 1}^{M} \mathcal{A}^y_{d_1, \ldots, d_N} \prod_{i = 1}^N f_{\theta_{d_i}}(\nv{x_i})
\end{equation}

Explicamos ahora algunos detalles sobre esta ecuación, que será central en nuestro trabajo.

En primer lugar, usamos la notación $\sum_{d_1, \ldots, d_N = 1}^{M}$ para denotar $\sum_{d_1 = 1}^{M} \sum_{d_2 = 1}^{M} \ldots \sum_{d_N = 1}^{M}$

Las funciones $f_{\theta_1}, \ldots, f_{\theta_M}: \R^s \to \R$ son las \textbf{funciones de representación}. Cada una de estas funciones se selecciona de una familia paramétrica

$$\mathcal{F} = \{ f_{\theta}: \R^s \to \R / \theta \in \Theta \}$$

Algunas funciones de representación usuales son:

\begin{itemize}
    \item \textit{Wavelets}
    \item Funciones de base radial (\textit{RBF}), normalmente la Gaussiana
    \item Neuronas
\end{itemize}

% TODO -- no he justificado que haya funciones de representacion linealmente independientes
El tensor $\mathcal{A}^y$ será el \textbf{tensor de coeficientes}. Por la sumatoria en \eqref{eq:puntuacion_general}, es claro que tiene orden $N$ y dimensión $M$ en cada modo. Es decir, $\mathcal{A} \in \espaciotensores{N}{M}$.

Por lo comentado en \customref{sec:justificacion_func_repr}, normalmente consideramos funciones de representación linealmente independientes.
\todo{Comentar, en base a lo que digamos en la sección que habla de totalidad, lind, que las funciones producto también seran totales, lind}
\todo{Habrá que decir que, en el bullet list de arriba, ya sabemos que son funciones lind y creo que totales}

La tarea de aprendizaje ahora será aprender los valores de los parámetros $\theta_1, \ldots, \theta_M$ y los valores de los tensores de coeficientes $\mathcal{A}^1, \ldots, \mathcal{A}^Y$.

\begin{observacion}

    Estamos usando las mismas funciones de representación $f_{\theta_1}, \ldots, f_{\theta_M}: \R^s \to \R$ para todas las funciones de representación $h_y, y \in \mathcal{Y}$, lo único que cambia entre las distintas funciones de puntuación es el tensor de coeficientes $\mathcal{A}^y$.


    Notar además que en la ecuación \refeq{eq:puntuacion_general}, los vectores de entrada $\nv{x_i}$ solo participan en el productorio que involucra computar $f_{\theta_{d_i}}(\mathbf{x_i})$.

    Por tanto, podemos considerar como paso inicial el cómputo de los valores

    $$\{f_{\theta_d}(\nv{x_i}) / d \in \deltaset{M},\ i \in \deltaset{N} \}$$

    % Lo de los M canales lo tengo mas dudoso, porque no es necesariamente igual
    % que en una capa convolucional de machine learning
    Una vez que hayamos computado esos $M \cdot N$ valores, ya no necesitamos los valores $\nv{x_i}$ para nada más. Con esto, es natural considerar que nuestro modelo tenga una primera capa que compute esos valores. Podemos considerar dicha capa como una \textbf{primera capa convolucional} con $M$ canales, a la que llamaremos \textbf{capa de representación}.

    Y como ya hemos comentado, estamos usando las mismas funciones de representación para las $Y$ funciones de puntuación. Por tanto, fijados los parámetros $\theta_i$, en todas las funciones de puntuación la capa de representación será la misma.
\end{observacion}

\begin{observacion}
    Sabemos, por lo comentado en \customref{sec:justificacion_func_repr}, que al trabajar con \textit{wavelets}, \textit{RBFs} y neuronas, podemos extraer un conjunto numerable de funciones que sea total y linealmente independiente. También gracias a \customref{sec:justificacion_func_repr}, que por tanto el siguiente espacio de funciones es total y linealmente independiente:

    $$\{(\nv{x_1}, \ldots, \nv{x_N}) \mapsto \prod_{i = 1}^{N} f_{\theta_{d_i}} (\nv{x_i}) \dspace / \dspace d_i \in \deltaset{M} \dspace \forall i \in \deltaset{N}\}$$

    Por tanto, actúa de forma parecida a una base de un espacio vectorial. Teniendo en cuenta este comportamiento similar, podemos pensar que el tensor $\mathcal{A}^y$ nos da los coeficientes de la función de puntuación $h_y$ en dicha base.

    \todo{Tengo que desarrollar bien esa sección para justificar todas estas cosas que estoy diciendo}
\end{observacion}

Como ya sabemos, como $\mathcal{A} \in \espaciotensores{N}{M}$, tenemos que optimizar el valor de $M^N$ valores reales a través del proceso de aprendizaje. Esto supone un reto, que \textbf{motiva el uso de los dos modelos}, que se basarán en descomposiciones tensoriales para que el aprendizaje del tensor de coeficientes sea más factible.
\todo{Tengo que comentar en la introducción que el número de elementos de un tensor N, M es M elevado a N}

% TODO -- me quedo aqui
\section{Modelo CP}

Como ya hemos comentado en \customref{sec:repr_funciones_puntuacion}, buscaremos descomponer el tensor de coeficientes $\mathcal{A}^y$ en la ecuación \eqref{eq:puntuacion_general}, para que el aprendizaje de sus coeficientes sea computacionalmente factible.

La idea más simple es aplicar la \textit{descomposición CP}, que introducimos en \eqref{eq:cp_decomp}, en la ecuación \eqref{eq:puntuacion_general}. Usaremos una descomposición conjunta para los tensores:

\begin{equation} \label{eq:cp_decomp_conjunta}
    \mathcal{A}^y = \sum_{z = 1}^Z a_z^y \cdot \boldsymbol{\omega}^{z, 1} \otimes \ldots \otimes \boldsymbol{\omega}^{z, N}
\end{equation}

Desarrollemos los elementos que participan en esa ecuación. En primer lugar, sabemos por \eqref{eq:cp_decomp} que $a_z^y \in \R$, y por tanto, podemos considerar $\mathbf{a}^y := (a_1^y, \ldots, a_Z^y)^T \in \R^Z$. En segundo lugar, y de nuevo, conforme a \eqref{eq:cp_decomp}, tenemos los vectores $\boldsymbol{\omega^{z, i}} \in \R^M$ con $z \in \deltaset{Z}, i \in \deltaset{N}$. Decimos que la \textbf{descomposición es conjunta} porque los vectores $\boldsymbol{\omega}^{z, i}$ son los mismos para todos los valores de $y \in \mathcal{Y}$. Solo cambian los coeficientes $\mathbf{a}^y$.

\begin{proposicion}
    Tomando $Z = M^N$ en la ecuación \refeq{eq:cp_decomp_conjunta}, la descomposición es universal. Esto es, podemos fijar un conjunto de vectores $\{\boldsymbol{\omega}^{z, i} \in \R^M / z \in \deltaset{Z}, i \in \deltaset{N} \}$ de forma que cualquier tensor $\mathcal{A}^y$ de orden $N$ y dimensión $M$ en cada modo puede ser representado. Es decir:

    \begin{equation}
        \forall \mathcal{A}^y \in \esptensores{N}{M}, \exists \mathbf{a}^y \in \R^Z: \text{la ecuación \refeq{eq:cp_decomp_conjunta} se verifica}
    \end{equation}
\end{proposicion}

\begin{proof}
    La idea de la demostración es muy sencilla. Tenemos que expresar un tensor arbitrario $\mathcal{A}$ de orden $N$ y dimensión $M$ en cada modo (es decir, de $M^N$ elementos) como una suma de $M^N$ elementos. Por tanto, en cada sumando, buscamos generar un elemento del tensor y colocarlo en los índices apropiados.

    Para que la demostración sea más clara, y como $Z = M^N$, para cada $z \in \deltaset{Z}$ podemos expresar $z = (i_1, \ldots, i_N)$ con $i_j \in \deltaset{M}$, y también podemos expresar la sumatoria como:

    \begin{equation}
        \mathcal{A} = \sum_{i_1 = 1, \ldots, i_N = 1}^M a_{(i_1, \ldots, i_N)} \cdot \boldsymbol{\omega}^{(i_1, \ldots, i_N), 1} \otimes \ldots \otimes \boldsymbol{\omega}^{(i_1, \ldots, i_N), N}
    \end{equation}

    Para llevar a cabo la idea de usar cada sumando para colocar un elemento de $\mathcal{A}$ en sus índices correctos, haremos que en cada sumando se verifique

    \begin{itemize}
        \item $a_{i_1, \ldots, i_N} = \mathcal{A}_{i_1, \ldots, i_N}$. Esto es inmediato y no necesita más explicación
        \item $\boldsymbol{\omega}^{(i_1, \ldots, i_N), 1} \otimes \ldots \otimes \boldsymbol{\omega}^{(i_1, \ldots, i_N), N}$ genere el tensor de orden $N$ y dimensión $M$ en cada modo que sea cero en todas las entradas salvo en el índice $(i_1, \ldots, i_N)$, al que podemos denotar $\mathcal{B}^{(i_1, \ldots, i_N)}$
    \end{itemize}

    Ahora, veamos cómo podemos generar el tensor $\mathcal{B}^{(i_1, \ldots, i_N)}$ como producto tensorial de ciertos vectores. La propiedad fundamental que queremos que ese tensor cumpla es:

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)}_{j_1, \ldots, j_N} =
        \begin{cases}
            1 & \text{si } i_1 = j_1, \ldots, i_N = j_N \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation}

    Definimos $\vec{\delta}_{i, N}$ como el vector de longitud $N$, con todas las entradas nulas salvo la entrada de la posición $i$, en la que ponemos un uno. Por tanto, es claro que:

    \begin{equation}
        (\vec{\delta}_{i, N})_k =  \delta_{i, k}
    \end{equation}

    Definimos

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)} := \vec{\delta}_{i_1, N} \otimes \ldots \otimes \vec{\delta}_{i_N, N}
    \end{equation}

    y por tanto se verifica que:

    \begin{equation}
        \mathcal{B}^{(i_1, \ldots, i_N)}_{j_1, \ldots, j_N} = (\vec{\delta}_{i_1, N})_{j_1} \cdot \ldots \cdot (\vec{\delta}_{i_N, N})_{j_N} = \delta_{i_1, j_1} \cdot \ldots \cdot \delta_{i_N, j_N} =
        \begin{cases}
            1 & \text{si } i_1 = j_1, \ldots, i_N = j_N \\
            0 & \text{en otro caso}
        \end{cases}
    \end{equation}

    Para concluir la demostración, fijamos el conjunto de vectores de forma que:

    \begin{equation} \label{eq:cp_decomp_asignacion_vectores}
        \{\boldsymbol{\omega}^{z, i} \in \R^M / z \in \deltaset{Z}, i \in \deltaset{N} \} =
        \{ \vec{\delta}_{i_j, N} / j \in \deltaset{N}, i_j \in \deltaset{M} \}
    \end{equation}

\end{proof}

\begin{observacion}
    En \eqref{eq:cp_decomp_asignacion_vectores} vemos que estamos usando menos vectores que los que se consideran en el enunciado del teorema. Esto es porque estamos repitiendo estos vectores. Por ejemplo, todos los sumandos referentes al valor 1 del primer índice tiene como primer vector $\vec{\delta}_{1, N}$
\end{observacion}

\begin{observacion}
    Esta proposición nos sirve para tener una cota superior del número de sumandos necesarios para realizar la descomposición. No hemos ganado nada respecto al trabajo previo. Seguimos teniendo el problema de considerar $M^N$ elementos, problema que ya vimos en \customref{sec:justificacion_func_repr}
\end{observacion}

\endinput
