\chapter{Conclusiones y futuras mejoras} \label{chapter:conclusiones_trabajo_futuro}

En este trabajo hemos conseguido modelar matemáticamente la tarea de aprendizaje de forma fiel a la dinámica que se aplica en escenarios reales de aprendizaje automático y describir dos modelos matemáticos que se asemejan muchísimo a las arquitecturas usadas en la práctica. Nuestra modelización tiene en cuenta propiedades fundamentales de las redes convolucionales profundas, como lo son la compartición de coeficientes y la localidad en las convoluciones y el uso de las operaciones de \textit{pooling}. Con estas modelizaciones hemos conseguido estudiar la mayor capacidad expresiva de las redes convolucionales profundas frente a las redes convolucionales no profundas y dar información precisa sobre cómo de frecuentemente ocurre este hecho. Por lo tanto, \textbf{consideramos que los objetivos propuestos inicialmente en el trabajo se han cumplido de forma satisfactoria}.

Sin embargo, nos hemos centrado en estudiar redes convolucionales profundas y no profundas, que se han usado intensivamente en tareas de visión por computador. No hemos estudiado otras arquitecturas más actuales usadas para visión, como pueden ser las \textit{redes generativas adversarias}, también conocidas como \textit{GANs} por sus siglas en inglés. O arquitecturas basadas en mecanismos de atención, como los \textit{Transformers}, que están teniendo mucho protagonismo tanto en tareas de visión como en otros ámbitos (principalmente en procesamiento de lenguaje natural o \textit{NLP}).

Y aunque hayamos realizado una modelización muy buena de las redes convolucionales profundas y no profundas, existen algunos elementos técnicos muy presentes en ciertas variantes de estas redes que no hemos considerado. Por ejemplo, en el modelo \textit{ResNet} se utilizan bloques residuales, normalización de \textit{batches} y conexiones de salto o \entrecomillado{skip connections}. Hemos usado esta arquitectura en la parte informática del presente trabajo, y describimos dicha arquitectura en detalle en la \sectionref{isec:explicacion_modelo}.

Finalizamos comentando algunas \textbf{mejoras y futuras líneas de trabajo}. En primer lugar, introducir elementos técnicos como capas residuales, capas de salto o normalización de \textit{batches} en nuestras modelizaciones, para estudiar cómo impactan estos elementos en la capacidad expresiva de las redes. En segundo lugar, estudiar posibles modelizaciones usando descomposiciones tensoriales para las arquitecturas que hemos mencionado previamente (\textit{GANs} y arquitecturas basadas en atención). Con ello, probar ciertos resultados que nos den información sobre la mayor potencia expresiva de estos modelos. Finalmente, nuestra modelización matemática induce un tipo de \textit{pooling} concreto, o bien un \textit{pooling} tipo producto global o bien un \textit{pooling} tipo producto, de tamaño de ventana dos y paso uno. Una profundización interesante es estudiar una modelización que emplee \textit{poolings} más comunes en la práctica del aprendizaje automático, como \textit{max pooling} o \textit{average pooling}, y estudiar cómo afecta este cambio a nuestros resultados.
