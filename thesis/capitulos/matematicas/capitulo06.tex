\chapter{Conclusiones y futuras mejoras} \label{chapter:conclusiones_trabajo_futuro}

En este trabajo hemos conseguido modelar matemáticamente la tarea de aprendizaje de forma fiel a la dinámica que se aplica en escenarios reales de aprendizaje automático y describir dos modelos matemáticos que se asemejan muchísimo a las arquitecturas usadas en la práctica. Nuestra modelización tiene en cuenta propiedades fundamentales de las redes convolucionales profundas, como lo son la compartición de coeficientes y la localidad en las convoluciones y el uso de las operaciones de \textit{pooling}. Con estas modelizaciones hemos conseguido estudiar la mayor capacidad expresiva de las redes convolucionales profundas frente a las redes convolucionales no profundas y dar información precisa sobre cómo de frecuente ocurre este hecho. Por lo tanto, \textbf{consideramos que los objetivos propuestos inicialmente en el trabajo se han cumplido de forma satisfactoria}.

Sin embargo, nos hemos centrado en estudiar redes convolucionales profundas y no profundas, que se han usado intensivamente en tareas de visión por computador. No hemos estudiado otras arquitecturas más actuales usadas para visión, como pueden ser las \textit{redes generativas adversarias}, también conocidas como \textit{GANs} por sus siglas en inglés. O arquitecturas basadas en mecanismos de atención, como los \textit{Transformers}, que están teniendo mucho protagonismo tanto en tareas de visión como en otros ámbitos (principalmente en procesamiento de lenguaje natural o \textit{NLP}).

Y aunque hayamos realizado una modelización muy buena de las redes convolucionales profundas y no profundas, existen algunos elementos técnicos muy presentes en ciertas variantes de estas redes que no hemos considerado. Por ejemplo, en el modelo \textit{ResNet} se utilizan bloques residuales, normalización de \textit{batches} y conexiones de salto o \entrecomillado{skip connections}. Hemos usado esta arquitectura en la parte informática del presente trabajo, y describimos dicha arquitectura en detalle en la \sectionref{isec:explicacion_modelo}.

Por lo tanto, algunas \textbf{mejoras y futuras líneas de trabajo} son:

\begin{itemize}
	\item Introducir elementos técnicos como capas residuales, capas de salto o normalización de \textit{batches} en nuestras modelizaciones. Estudiar cómo impactan estos elementos en la capacidad expresiva de las redes.
	\item Estudiar posibles modelizaciones usando descomposiciones tensoriales para las arquitecturas que hemos mencionado previamente (\textit{GANs} y arquitecturas basadas en atención).
	\item Probar ciertos resultados en base a estas modelizaciones que nos den información sobre su mayor potencia expresiva respecto a otros modelos.
\end{itemize}
