% !TeX root = ../../libro.tex
% !TeX encoding = utf8

\chapter{Herramientas matemáticas fundamentales} \label{ch:matematicas_fundamentales}

En esta sección vamos a introducir algunas de las herramientas matemáticas sobre las que se apoya nuestro trabajo. Estas herramientas son básicas y no aportan nada nuevo al estudio que hacemos sobre las redes neuronales, por lo que el lector puede pasar directamente a \customref{ch:tarea_aprendizaje}.

\section{Notación}

Seguiremos en parte la notación del \textit{paper} de referencia \cite{matematicas:principal}, aunque introducimos varios cambios por claridad, para no confundir en las fórmulas escalares, vectores y tensores.

Denotaremos a los vectores con flechas sobre las letras que los identifican, tal que $\nv{v} \in \R^N$. Las coordenadas de dicho vector se denotarán como $\nv{v_i}$ con $i \in \deltaset{n}$, donde $\deltaset{n} := \{1, \ldots, n\}$. También usaremos la notación $\doubledeltaset{n}{m} := \{n, \ldots, m\}$ donde $n \leq m$. En algunas ocasiones usaremos la notación $\nv{v_i^n}$ para indicar que estamos trabajando con el índice $i$ del vector $n$-ésimo de un conjunto de vectores.

Aunque más tarde definamos qué significan estos conceptos, introducimos ahora la notación usada respecto a los tensores.

Para denotar a los tensores usaremos tipografía caligráfica, por ejemplo, $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$. Cada una de las entradas de dicho tensor serán denotadas como $\mathcal{A}_{d_1, \ldots, d_N} \in \R$.

Al espacio de tensores de orden $N$ y dimensión $M$ en cada modo lo denotaremos por $\espaciotensores{N}{M}$. Al espacio de matrices de dimensiones $p, q$ lo denotaremos de la forma usual como  $\espaciomatrices{p}{q}$.

Al producto tensorial entre dos tensores $\mathcal{A}, \mathcal{B}$ lo denotaremos como $\mathcal{A} \otimes \mathcal{B}$. Dado un conjunto de vectores $\nv{v_1}, \ldots, \nv{v_N} \in \R^{M_1}, \ldots, \R^{M_N}$, denotaremos su producto tensorial $\nv{v_1} \otimes \ldots \otimes \nv{v_N}$ como $\otimes_{i = 1}^N \nv{v_i}$.

Al producto de Kronecker entre dos matrices $A, B$ lo denotaremos como $A \odot B$.

\section{Tensores}

\subsection{Definición del producto tensorial} \label{sec:deftensor}
\todo{Tengo que referenciar algún recurso sobre esto. Yo lo he visto a partir de unos vídeos}

Dados dos espacios vectoriales reales (aunque podría realizarse la construcción sobre otro cuerpo) $\mathbb{V}, \mathbb{W}$, queremos construir el espacio producto tensorial de estos espacios vectoriales, denotado como $\mathbb{V} \otimes \mathbb{W}$. Buscamos que este nuevo objeto matemático tenga propiedades similares a las del producto entre escalares, principalmente la propiedad distributiva y la propiedad asociativa. Especificaremos esto en \customref{sec:cociente_prod_formal}

\subsubsection{Producto formal de dos espacios vectoriales}

Para la construcción del producto tensorial de espacios vectoriales necesitaremos primero introducir el concepto de producto formal entre dos espacios vectoriales, que será fundamental en la construcción del objeto matemático que buscamos.

\begin{definicion}[Producto formal de dos espacios vectoriales]
    Sean $\mathbb{V}, \mathbb{W}$ dos espacios vectoriales reales. Se define su \textbf{producto formal} como:

    \begin{equation}
        \mathbb{V} \ast \mathbb{W} := span_{\mathbb{R}} \{v \ast w / v \in \mathbb{V}, w \in \mathbb{W} \}
    \end{equation}

    donde $*$ es un símbolo con el que no sabemos operar. Por tanto, ahora mismo no sabemos simplificar muchas expresiones en este espacio.

\end{definicion}

\begin{observacion}

    $span$ denota el conjunto formado por todas las combinaciones lineales finitas de los elementos del conjunto, es decir,

    \begin{equation}
        span_{\mathbb{R}}(A) := \{ \sum_{k = 1}^n \alpha_i a_i : n \in \N, \; \alpha_i \in \R, \; a_i \in A \}
    \end{equation}


\end{observacion}

Es claro que por ser $\mathbb{V}, \mathbb{W}$ espacios vectoriales, y estar tomando combinaciones lineales finitas, $\mathbb{V} \ast \mathbb{W}$ es un espacio vectorial.

Veamos ahora algunos ejemplos para familiarizarnos con este espacio.
\todo{\textbf{Para Javier}: no sé si estos ejemplos son necesarios. Los he puesto porque lo tenía apuntado en mis notas cuando estaba estudiando el producto tensorial, pero ahora no le veo demasiado el valor a dejarlo aquí}

\begin{ejemplo}[Producto formal sencillo] \label{ej:prod_formal}

Sea $\Omega := \R^2 \ast \R^3$. Entonces tenemos que:

\begin{equation}
    \Omega = span \conjunto{
        \begin{pmatrix}
            a \\
            b
        \end{pmatrix}
        \ast
        \begin{pmatrix}
            x \\
            y \\
            z
        \end{pmatrix}
        / a, b, x, y, z \in \R
    }
\end{equation}

Un elemento de dicho espacio puede ser

$$
2 \left[ \begin{pmatrix}1 \\ 2\end{pmatrix} \ast \begin{pmatrix}3 \\ 2 \\ 1 \end{pmatrix} \right]
- 3 \left[ \begin{pmatrix}1 \\ 0\end{pmatrix} \ast \begin{pmatrix}1 \\ 2 \\ 3 \end{pmatrix} \right]
$$

Notar que no sabemos cómo meter los escalares $2, -3$ en los elementos del producto formal.

Por cómo hemos definido el producto formal de dos espacios vectoriales, tenemos que $dim(\R^2 \ast \R^3) = \infty$

\end{ejemplo}

\subsubsection{Producto tensorial a partir del producto formal} \label{sec:cociente_prod_formal}

Para motivar el nuevo objeto que vamos a construir, hay que tener en cuenta que en general las siguientes igualdades no se cumplen:

\begin{enumerate}
    \item $c [\nv{v} \ast \nv{w}] = (c\nv{v}) \ast \nv{w}$
    \item $c[\nv{v} \ast \nv{w}] = \nv{v} \ast (c\nv{w})$
    \item $(\nv{v_1} + \nv{v_2}) \ast \nv{w} = \nv{v_1} \ast \nv{w} + \nv{v_2} \ast \nv{w}$
    \item $\nv{v} \ast (\nv{w_1} + \nv{w_2}) = \nv{v} \ast \nv{w_1} + \nv{v} \ast \nv{w_2}$
\end{enumerate}

Donde estamos tomando $\nv{v}, \nv{v_1}, \nv{v_2} \in \mathbb{V}, \nv{w}, \nv{w_1}, \nv{w_2} \in \mathbb{W}, c \in \R$

Estas igualdades representan las propiedades que queremos que se cumplan para que nuestro nuevo objeto matemático tenga un comportamiento similar al del producto entre escalares. En concreto, 1 y 2 nos dan la asociatividad, mientras que 3 y 4 nos dan la propiedad distributiva.

Como $\mathbb{V} \ast \mathbb{W}$ es un espacio vectorial, podemos usar el espacio cociente para introducir estas propiedades. Para ello definimos:

\begin{equation}
\begin{split}
    I &:= span\{ \\
        & \dspace (c\nv{v}) \ast \nv{w} - c(\nv{v} \ast \nv{w}), \\
        & \dspace \nv{v} \ast (c\nv{w}) - c(\nv{v} \ast \nv{w}), \\
        & \dspace (\nv{v_1} + \nv{v_2}) \ast \nv{w} - (\nv{v_1} \ast \nv{w} + \nv{v_2} \ast \nv{w}), \\
        & \dspace \nv{v} \ast (\nv{w_1} + \nv{w_2}) - (\nv{v} \ast \nv{w_1} + \nv{v} \ast \nv{w_2}): \\
        & \dspace \\
        & \dspace \nv{v}, \nv{v_1}, \nv{v_2} \in \mathbb{V}; \dspace \nv{w}, \nv{w_1}, \nv{w_2} \in \mathbb{W}; \dspace c \in \R \\
     \} &
\end{split}
\end{equation}

que claramente también es un espacio vectorial. Con esto, ya podemos definir el producto tensorial.

\begin{definicion}[Producto tensorial]
    Dados dos espacios vectoriales $\mathbb{V}, \mathbb{W}$, se define su \textbf{producto tensorial} $\mathbb{V} \otimes \mathbb{W}$ como:

    $$\mathbb{V} \otimes \mathbb{W} := (\mathbb{V} \ast \mathbb{W}) / I$$

    con lo que dados $\nv{v} \in \mathbb{V}, \nv{w} \in \mathbb{W}$, tenemos que

    \begin{equation}
        \nv{v} \otimes \nv{w} := \nv{v} \ast \nv{w} + I
    \end{equation}
\end{definicion}

A partir de esta definición, son directas las siguientes propiedades:

\begin{proposicion}[Propiedades del producto tensorial] \label{prop:tensores_propiedades}
    Sean $\nv{v}, \nv{v_1}, \nv{v_2} \in \mathbb{V}, \nv{w} \in \mathbb{W}, \lambda \in \R$, entonces son ciertas:
    \begin{enumerate}
        \item $\lambda [\nv{v} \otimes \nv{w}] = (\lambda \nv{v}) \otimes \nv{w}$
        \item $\lambda [\nv{v} \otimes \nv{w}] = \nv{v} \otimes (\lambda \nv{w})$
        \item $\nv{v} \otimes (\nv{w_1} + \nv{w_2}) = \nv{v} \otimes \nv{w_1} + \nv{v} \otimes \nv{w_2}$
        \item ($\nv{v_1} + \nv{v_2}) \otimes \nv{w} = \nv{v_1} \otimes \nv{w} + \nv{v_2} \otimes \nv{w}$
    \end{enumerate}
\end{proposicion}

\begin{proof} $\newline \newline$
    1.

    \begin{equation}
    \begin{split}
        (c\nv{v}) \otimes \nv{w} &\eqtext{def} (c\nv{v}) \ast \nv{w} + I = \ldots \ \text{usando que} \quad \nv{a} + I = \nv{a} + \nv{i} + I, \quad \forall \nv{i} \in I \\
        \ldots &= (c\nv{v}) \ast \nv{w} + (c(\nv{v} \ast \nv{w}) - c\nv{v} \ast \nv{w}) + I = \ldots \\
        \ldots &= \cancel{(c\nv{v}) \ast \nv{w}} + c(\nv{v} \ast \nv{w}) - \cancel{c\nv{v} \ast \nv{w}} + I = \ldots \\
        \ldots &= c(\nv{v} \ast \nv{w}) + I = c (\nv{v} \otimes \nv{w}) \\
    \end{split}
    \end{equation}

    El resto de propiedades se comprueban de forma análoga, introduciendo la propiedad que queremos probar gracias a que $\nv{a} + I = \nv{a} + \nv{i}$, $\forall \nv{i} \in I$ y operando con esto

\end{proof}

\begin{proposicion}
    Sean $\mathbb{V}$, $\mathbb{W}$ dos espacios vectoriales reales. Entonces el espacio producto tensorial $\mathbb{V} \otimes \mathbb{W}$ es un espacio vectorial real.
\end{proposicion}

\begin{proof}
    Al definir el producto tensorial como el cociente de $\mathbb{V} \ast \mathbb{W}$ (espacio vectorial) por $I$ (subespacio vectorial), claramente acabamos con un espacio vectorial.
\end{proof}


Ahora, enunciamos un importante teorema que nos ayudará a entender la naturaleza del producto vectorial:

\begin{teorema}[Base del espacio vectorial \textit{producto tensorial}] \label{th:base_prod_tensorial}
    Sean $\mathbb{B}_{\mathbb{V}} = \{\nv{v_1}, \ldots, \nv{v_n}\}$, $\mathbb{B}_{\mathbb{W}} = \{\nv{w_1}, \ldots, \nv{w_m}\}$ bases de $\mathbb{V}, \mathbb{W}$ respectivamente, entonces:

    $$\mathbb{B}_{\mathbb{V} \otimes \mathbb{W}} := \{\nv{v_i} \otimes \nv{w_j} / i \in \deltaset{n}, j \in \deltaset{m}\}$$

    es una base del espacio vectorial $\mathbb{V} \otimes \mathbb{W}$, y por lo tanto:

    $$dim(\mathbb{V} \otimes \mathbb{W}) = dim(\mathbb{V}) \cdot dim(\mathbb{W})$$
\end{teorema}

\begin{observacion}
    Al estar tomando bases vectoriales (finitas), esta proposición solo es válida en espacios vectoriales de dimensión finita.
\end{observacion}

\begin{observacion}
    Notar que podríamos haber usado este teorema como forma de definir el producto tensorial de dos espacios vectoriales. Sin embargo, limitaríamos esta construcción a espacios vectoriales que admitiesen una base.
\end{observacion}

Una consecuencia inmediata es que, en caso de que $\mathbb{V}$ y $\mathbb{W}$ admitan base, todo tensor $\gamma \in \mathbb{V} \otimes \mathbb{W}$ se puede escribir de la forma:

\begin{equation}
    \gamma = \sum_{\substack{\nv{v_i} \in \mathbb{V}\\ \nv{w_i} \in \mathbb{W}\\ i \in \deltaset{n}}} c_{i} \cdot \nv{v_i} \otimes \nv{w_i}, \dspace\dspace c_i \in \R \dspace \forall i \in \deltaset{n}
\end{equation}

La expresión anterior motiva la siguiente definición:

\begin{definicion}[Tensor puro]
    Un tensor $\gamma \in \mathbb{V} \otimes \mathbb{W}$ se dice puro cuando existen $\nv{v} \in \mathbb{V}$, $\nv{w} \in \mathbb{W}$ tales que $\gamma = \nv{v} \otimes \nv{w}$.
\end{definicion}

\begin{proposicion}
    Como consecuencia directa de \customref{th:base_prod_tensorial}, dados $\nv{v}, \nv{w} \in \mathbb{V}$, en general no es cierto que:

    \begin{equation}
        \nv{v} \otimes \nv{w} \neq \nv{w} \otimes \nv{v}
    \end{equation}

    \begin{observacion}
        En la anterior proposición estamos tomando el espacio $\mathbb{V} \otimes \mathbb{V}$ para que tenga sentido permutar los vectores $\nv{v}$ y $\nv{w}$ en el producto tensorial.
    \end{observacion}
\end{proposicion}

Veamos ahora otra propiedad interesante. Queremos que el producto tensorial se asemeje al producto entre escalares. Para ello, sería natural que $\nv{v} \otimes \nv{0_w} = \nv{0_{\mathbb{V} \otimes \mathbb{W}}}$.

\begin{proposicion}
    Sean $\mathbb{V}, \mathbb{W}$ espacios vectoriales sobre $\R$. Sean $\nv{v} \in \mathbb{V}, \nv{w} \in \mathbb{W}$. Entonces se verifica:

    \begin{enumerate}
        \item $\nv{v} \otimes \nv{0_\mathbb{W}} = \nv{0_{\mathbb{V} \otimes \mathbb{W}}}$
        \item $\nv{0_{\mathbb{V}}} \otimes \nv{w} = \nv{0_{\mathbb{V} \otimes \mathbb{W}}}$
    \end{enumerate}
\end{proposicion}
\begin{proof}
    Empezamos con la primera igualdad. Sabemos que en un espacio vectorial se verifica que:

    \begin{equation} \label{eq:dem_tensor_cero}
        \nv{v} + \nv{w} = \nv{w} \then \nv{v} = \nv{0}
    \end{equation}

    Veamos esto ahora con nuestro primer candidato a cero del espacio vectorial producto tensorial:

    \begin{equation}
    \begin{split}
        \nv{v} \otimes \nv{0_\mathbb{W}} + \nv{v} \otimes \nv{w} \eqtext{3.} \nv{v} \otimes (\nv{0_\mathbb{W}} + \nv{w}) = \nv{v} \otimes (\nv{w}) \then \nv{v} \otimes \nv{0_\mathbb{W}} = \nv{0_{\mathbb{V} \otimes \mathbb{W}}}
    \end{split}
    \end{equation}

    La demostración para $\nv{0_{\mathbb{V}}} \otimes w = \nv{0_{\mathbb{V} \otimes \mathbb{W}}}$ es completamente análoga.
\end{proof}

\subsubsection{Algunos ejemplos}
\todo{\textbf{Para Javier}: lo mismo que antes, creo que estos ejemplos ahora mismo no aportan demasiado valor y lían algo el desarrollo}

Con toda esta base podemos ver algunos ejemplos interesantes, con los que comprenderemos aún mejor la naturaleza del producto tensorial.

\begin{ejemplo}
    Sea $\Omega := \R^2 \otimes \R^3$. Usando \customref{th:base_prod_tensorial} sabemos que $dim(\Omega) = 2 \cdot 3 = 6$. Por ser un espacio vectorial real, de dimensión $6$, sabemos que $\Omega \cong \R^6$. El mismo teorema nos permite calcular una base del espacio, tomando las dos bases usuales de los espacios $\R^2$ y $\R^3$, que podríamos considerar la base usual para el producto tensorial:

    \begin{equation}
    \begin{split}
    \mathbb{B}_{\Omega} = \{& \\
        & \begin{pmatrix}1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},
        \begin{pmatrix}1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},
        \begin{pmatrix}1 \\ 0 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix}, \\
        & \begin{pmatrix}0 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 1 \\ 0 \\ 0 \end{pmatrix},
        \begin{pmatrix}0 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 1 \\ 0 \end{pmatrix},
        \begin{pmatrix}0 \\ 1 \end{pmatrix} \otimes \begin{pmatrix} 0 \\ 0 \\ 1 \end{pmatrix} \\
    & \}
    \end{split}
    \end{equation}
\end{ejemplo}

A partir de lo anterior, podemos ir abstrayendo ciertas propiedades resultantes de hacer el producto tensorial, siendo uno de los dos espacios un cierto $\R^N$. Empezamos con el siguiente ejemplo:

\begin{ejemplo}
    Sea $\mathbb{V}$ un espacio vectorial real y considero $\Omega := \R \otimes \mathbb{V}$.

    Comienzo considerando un tensor de la forma $\gamma = a(x \otimes \nv{u}) + b(y \otimes \nv{v})$ con $a, b \in \R$, $x, y \in \R$, $\nv{u}, \nv{v} \in \mathbb{V}$. Usando las propiedades de los tensores (\ref{prop:tensores_propiedades}), desarrollo esta expresión:

    \begin{equation}
    \begin{split}
        a(x \otimes \nv{u}) + b(y \otimes \nv{v}) &\eqtext{2.} \\
        &= x \otimes (a\nv{u}) + y \otimes (b\nv{v}) \eqtext{2} \ldots \text{ usando que }  x,y \in \R \ldots \\
        \ldots &= 1 \otimes ((ax) \nv{u}) + 1 \otimes ((by) \nv{v}) \eqtext{3.} \\
        &= 1 \otimes ((ax)\nv{u} + (by) \nv{v}) = \\
        &= 1 \otimes \nv{w} \quad \text{con } \nv{w} := (ax)\nv{u} + (by) \; \text{luego } \nv{w} \in \mathbb{V}
    \end{split}
    \end{equation}

    Esto sirve como prueba del siguiente resultado:

    \begin{proposicion}
        Sea $\gamma \in \R \otimes \mathbb{V}$ con $\mathbb{V}$ un espacio vectorial real. Entonces:

        \begin{equation}
            \gamma \text{ tensor puro} \implies \exists \nv{v} \in \mathbb{V}: \gamma = 1 \otimes \nv{v}
        \end{equation}
    \end{proposicion}

    Buscamos extender este resultado para un tensor cualquiera (no necesariamente puro) del espacio $\R \otimes \mathbb{V}$:

    \begin{proposicion} \label{prop:r_otimes_v_es_v}
        Sea $\gamma \in \R \otimes \mathbb{V}$ donde $\mathbb{V}$ es un espacio vectorial real de \textbf{dimensión finita}. Entonces $\exists v \in \mathbb{V}: \gamma = 1 \otimes v$
    \end{proposicion}
    \begin{proof}
        Como ahora consideramos que $\mathbb{V}$ tiene dimensión finita, podemos usar \ref{th:base_prod_tensorial} para expresar:

        \begin{equation}
            \gamma = \sum_{\substack{a_i \in \R\\ \nv{v_i} \in \mathbb{W}\\ i \in \deltaset{n}}} c_{i} \cdot a_i \otimes \nv{v_i}, \dspace\dspace c_{i} \in \R \dspace \forall i \in \deltaset{n}
        \end{equation}

        Usando ahora la proposición anterior en los tensores puros de la sumatoria, podemos re-escribir como:

        \begin{equation}
        \begin{split}
            \gamma &= \sum_{\substack{\nv{v_i} \in \mathbb{W}\\ i \in \deltaset{n}}} c_{i} \cdot 1 \otimes \nv{v_i} \eqtext{2.} \\
            & = \sum_{\substack{\nv{v_i} \in \mathbb{W}\\ i \in \deltaset{n}}} 1 \otimes (c_{i} \cdot \nv{v_i}) \eqtext{3.} \\
            & = 1 \otimes ( \sum_{\substack{\nv{v_i} \in \mathbb{W} \\ i \in \deltaset{n}}} c_{i} \cdot \nv{v_i} ) = \\
            & = 1 \otimes \nv{v} \dspace\dspace \text{con } \nv{v} := \sum_{\substack{\nv{v_i} \in \mathbb{W}\\ i \in \deltaset{n}}} c_{i} \nv{v_i} \implies \nv{v} \in \mathbb{V}
        \end{split}
        \end{equation}

    \end{proof}


    Con esto, podemos pasar al siguiente resultado que nos permitirá ver de una forma mucho más clara en que espacio nos encontramos:

    \begin{proposicion}
        Sea $\mathbb{V}$ un espacio vectorial de dimensión finita. Entonces $\R \otimes \mathbb{V} \isomorfismo{\text{vec}} \mathbb{V}$
    \end{proposicion}
    \begin{proof}
        Basta con considerar

        \begin{equation}
        \begin{split}
            \phi: \R \otimes \mathbb{V} &\to \mathbb{V} \\
            \nv{v} = 1 \otimes \nv{w} &\mapsto \nv{w}
        \end{split}
        \end{equation}

        Donde hemos usado \customref{prop:r_otimes_v_es_v} para expresar cualquier elemento del producto tensorial como $1 \otimes \nv{w}$ con $\nv{w} \in \mathbb{V}$. Veamos, aunque sea prácticamente inmediato, que $\phi$ es biyectiva y lineal:

        Inyectividad: $\phi(1 \otimes \nv{w_1}) = \phi(1 \otimes \nv{w_2}) \underset{def.\ \phi}{\iif} \nv{w_1} = \nv{w_2}$

        Sobreyectividad: $\nv{w} \in \mathbb{V} \then \phi(1 \otimes \nv{w}) = \nv{w}$

        Linealidad 1. $\phi(1 \otimes \nv{w_1} + 1 \otimes \nv{w_2}) \eqtext{3.} \phi(1 \otimes (\nv{w_1} + \nv{w_2})) = \nv{w_1} + \nv{w_2}$

        Linealidad 2. $\phi(\lambda (1 \otimes \nv{w})) \eqtext{2.} \phi(1 \otimes (\lambda \nv{w})) = \lambda \nv{w}$

    \end{proof}
\end{ejemplo}

\begin{ejemplo} \label{ejemplo:R2xR2}
    Consideramos ahora el espacio $\Omega := \R^2 \otimes R^2$. Por tanto, sabemos que los tensores puros de este espacio son de la forma:

    $$\begin{pmatrix} a \\ b \end{pmatrix} \otimes \begin{pmatrix} c \\ d \end{pmatrix}$$

    Tomando ahora la base usual de $\R^2$, cualquier tensor de $\gamma \in \Omega$ se puede expresar como:

    \begin{equation}
    \begin{split}
        \gamma &= \lambda_{11} \vectordd{1}{0} \otimes \vectordd{1}{0} + \lambda_{12} \vectordd{1}{0} \otimes \vectordd{0}{1} + \ldots \\
        \ldots &+ \lambda_{21} \vectordd{0}{1} \otimes \vectordd{1}{0} + \lambda_{22} \vectordd{0}{1} \otimes \vectordd{0}{1}
    \end{split}
    \end{equation}

    De nuevo, sabemos que $\R^2 \otimes \R^2 \cong \R^4$. La expresión anterior, sin embargo, nos invita a considerar el espacio $\Omega$ como $\R^2 \otimes \R^2 \cong \R^4 \cong \espaciomatrices{2}{2}$, con lo que con un adecuado isomorfismo, podemos expresar:

    $$\gamma = \begin{bmatrix}
        \lambda_{11} & \lambda_{12} \\
        \lambda_{21} & \lambda_{22}
    \end{bmatrix}$$

    Para ello basta con considerar el isomorfismo:

    \begin{equation}
        \phi: \R \otimes \mathbb{V} \to \mathbb{V}
    \end{equation}

    de forma que:

    $$\phi(\vectordd{1}{0} \otimes \vectordd{1}{0}) = \begin{bmatrix} 1 & 0 \\ 0 & 0 \end{bmatrix}$$
    $$\phi(\vectordd{1}{0} \otimes \vectordd{0}{1}) = \begin{bmatrix} 0 & 1 \\ 0 & 0 \end{bmatrix}$$
    $$\phi(\vectordd{0}{1} \otimes \vectordd{1}{0}) = \begin{bmatrix} 0 & 0 \\ 1 & 0 \end{bmatrix}$$
    $$\phi(\vectordd{0}{1} \otimes \vectordd{0}{1}) = \begin{bmatrix} 0 & 0 \\ 0 & 1 \end{bmatrix}$$

    A partir de este isomorfismo, podemos dar cierto significado al producto de tensores puros:

    \begin{equation}
    \begin{split}
        \gamma &:= \vectordd{a}{b} \otimes \vectordd{c}{d} = \left( a \cdot \vectordd{1}{0} + b \cdot \vectordd{0}{1} \right) \otimes \left( c \cdot \vectordd{1}{0} + d \cdot \vectordd{0}{1} \right) = \ldots \\
        \ldots &= ac \vectordd{1}{0} \otimes \vectordd{1}{0} + ad \vectordd{1}{0} \otimes \vectordd{0}{1} + bc \vectordd{0}{1} \otimes \vectordd{1}{0} + bd \vectordd{0}{1} \otimes \vectordd{0}{1} \underset{\phi}{\cong} \ldots \\
        \ldots & \underset{\phi}{\cong} \begin{bmatrix} ac & ad \\ bc & bd \end{bmatrix}
    \end{split}
    \end{equation}

    Es decir:

    \begin{equation}
    \begin{split}
        \phi\left( \vectordd{a}{b} \otimes \vectordd{c}{d} \right) & = \begin{bmatrix} ac & ad \\ bc & bd \end{bmatrix} = \\
        & = \vectordd{a}{d} \begin{pmatrix} c & d \end{pmatrix}
    \end{split}
    \end{equation}

    O lo que es lo mismo:

    \begin{equation}
    \begin{split}
        \phi(v \otimes w) = v w^T
    \end{split}
    \end{equation}

    Esto coincide con la forma de definir los tensores que se hace en \cite{matematicas:principal}, y que introducimos en \customref{sec:otra_forma_tensores}.
\end{ejemplo}

Veamos ahora un ejemplo que nos clarifique la naturaleza del espacio $\R^N \otimes \mathbb{V}$:

\begin{ejemplo}
    Sea ahora $\Omega := \R^N \otimes \mathbb{V}$, con $N$ un natural cualquiera y $\mathbb{V}$ un espacio vectorial real. Consideramos su base usual:

    $$\mathbb{B}_{\R^N} := \left\{\vectorn{1}{0}{0}, \vectorn{0}{1}{0}, \ldots, \vectorn{0}{0}{1} \right\} = \left\{\nv{e_1}, \nv{e_2}, \ldots, \nv{e_N} \right\}$$

    Ahora, veamos cómo podemos manipular un tensor cualquiera del espacio $\Omega$:

    \begin{equation}
    \begin{split}
        \nv{v} \otimes \nv{w} &= (\lambda_1 \nv{e_1} + \lambda_2 \nv{e_2} + \ldots \lambda_N \nv{e_N}) \otimes \nv{w} \eqtext{3.} \ldots \\
        \ldots &= \lambda_1 \nv{e_1} \otimes \nv{w} + \lambda_2 \nv{e_2} \otimes \nv{w} + \ldots \lambda_N \nv{e_N} \otimes \nv{w} \eqtext{1., 2.} \ldots \\
        \ldots &= \nv{e_1} \otimes \lambda_1 \nv{w} + \nv{e_2} \otimes \lambda_2 \nv{w} + \ldots \nv{e_N} \otimes \lambda_N \nv{w} = \ldots \\
        \ldots &= \nv{e_1} \otimes \nv{w_1} + \nv{e_2} \otimes \nv{w_2} + \ldots \nv{e_N} \otimes \nv{w_N} \qquad \text{con} \dspace \nv{w_i} = \lambda_i \nv{w} \in \mathbb{V}
    \end{split}
    \end{equation}

    Por lo tanto, tenemos que $\forall \nv{v} \in \R^N$, $\forall \nv{w} \in \mathbb{V}$, $\exists \nv{w_1}, \ldots \nv{w_N} \in \mathbb{V}$ tal que:

    $$\nv{v} \otimes \nv{w} = \nv{e_1} \otimes \nv{w_1} + \nv{e_2} \otimes \nv{w_2} + \ldots + \nv{e_N} \otimes \nv{w_N}$$

    Luego, como hemos hecho en un ejemplo anterior, podemos definir el siguiente isomorfimso:

    \begin{equation}
    \begin{split}
        \phi: \R^N \otimes \mathbb{V} &\to \mathbb{V}^N \\
        \nv{v} \otimes \nv{w} = \nv{e_1} \otimes \nv{w_1} + \nv{e_2} \otimes \nv{w_2} + \ldots + \nv{e_N} \otimes \nv{w_N} &\mapsto \vectorn{\nv{w_1}}{\nv{w_2}}{\nv{w_N}}
    \end{split}
    \end{equation}

    Es decir, $\R^N \otimes \mathbb{V} \cong \mathbb{V}^N$


\end{ejemplo}

\subsection{Otra forma de ver los tensores} \label{sec:otra_forma_tensores}
% TODO -- me quedo por aqui

Introducimos ahora una forma mucho más directa y concreta de definir los tensores y el producto tensorial. Podemos ver un tensor $\mathcal{A} \in \R^{M_1 \times \ldots \times M_N}$ como un \textit{array} multidimensional. Tenemos $N$ entradas sobre las que podemos indexar, y en cada entrada podemos usar un índice $idx \in \deltaset{M_i}$.

Con esta visión, podemos desarrollar los siguientes conceptos básicos sobre tensores:

\begin{itemize}
    \item Modos: cada una de las entradas $d_1, \ldots, d_N$ que podemos usar para indexar los elementos del tensor
    \item Orden: el número de modos del tensor. En el caso de nuestro tensor $\mathcal{A}$, tenemos $N$ modos, y por tanto ese es su orden
    \item Dimensión: el número de valores que puede tomar cada uno de los modos. Por lo tanto, si en el primer modo $d_i$ puede tomar valores en $\deltaset{M}$, diremos que el modo $i$-ésimo tiene dimensión $M$.
        \begin{itemize}
            \item Un tensor puede tener distintas dimensiones en cada uno de los modos, o tener la misma dimensión para todos los modos
            \item Por tanto, sería más correcto hablar de \textit{"dimensiones de los modos"} que de \textit{"dimensión de un tensor"}, pero en ocasiones se abusa del lenguaje
        \end{itemize}
\end{itemize}

Ahora, respecto al \textbf{producto tensorial}, por los isomorfismos que hemos introducido previamente, podemos ver el producto tensorial entre dos tensores reales de una forma más sencilla. Sean $\mathcal{A}, \mathcal{B}$ dos tensores de órdenes $P, Q$ respectivamente. Entonces el producto tensorial de estos dos, que ya sabemos que se denota como $\mathcal{A} \otimes \mathcal{B}$, es un tensor de orden $P + Q$ cuyos elementos se pueden expresar como:
\todo{Tenemos que desarrollar estos isomorfismos o dejarlos indicados!}


$$(A \otimes B)d_1, \ldots d_{P + Q} = A_{d_1, \ldots, d_P} \cdot B_{d_{P + 1}, \ldots, d_{P + Q}}$$

En el caso de que tengamos dos vectores $\nv{u} \in \R^{N_1}, \nv{v} \in \R^{N_2}$, tenemos que $\nv{u} \otimes \nv{v} = \nv{u} \nv{v}^T$. Esto coincide con lo que vimos en \customref{ejemplo:R2xR2}, a partir de un isomorfismo natural.

\subsection{Propiedad Universal del producto tensorial}

El siguiente teorema será de gran utilidad a la hora de entender la naturaleza del producto tensorial entre dos espacios vectoriales.

\begin{teorema}[Propiedad Universal del producto tensorial] Sean $\mathbb{V}, \mathbb{W}$ dos espacios vectoriales. Su producto tensorial $\mathbb{V} \otimes \mathbb{W}$ es un espacio vectorial con una aplicación bilineal:

\begin{equation}
\begin{split}
    \otimes : \mathbb{V} \times \mathbb{W} &\to \mathbb{V} \otimes \mathbb{V} \\
    \nv{v}, \nv{w} & \mapsto \nv{v} \otimes \nv{w}
\end{split}
\end{equation}

de forma que:

\begin{equation}
    \forall h: \mathbb{V} \times \mathbb{W} \to \mathbb{Z} \text{  bilineal  } \exists! \hat{h}: \mathbb{V} \otimes \mathbb{W} \to \mathbb{Z} \text{  lineal, verificando que: }
\end{equation}

\begin{equation}
    h = \hat{h} \circ \otimes
\end{equation}

es decir:

\begin{equation}
    h(\nv{u}, \nv{v}) = \hat{h}(\nv{u} \otimes \nv{v}); \dspace \forall \nv{u} \in \mathbb{V}, \forall \nv{v} \in \mathbb{W}
\end{equation}

Esto se resume en que el siguiente diagrama es conmutativo:

\begin{equation}
\begin{tikzcd}
    \mathbb{V} \times \mathbb{W} \ar{r}{h} \ar{d}[left]{\otimes} & \mathbb{Z} \\
    \mathbb{V} \otimes \mathbb{W} \ar[dashed]{ur}[right, below]{\hat{h}}
\end{tikzcd}
\end{equation}

\end{teorema}

Es decir, dada una aplicación bilineal en el producto cartesiano de dos espacios vectoriales, podemos asociar unívocamente una aplicación lineal en el producto tensorial de los dos espacios vectoriales. Esto sigue siendo cierto para aplicaciones multilineales en el producto cartesiano de un número arbitrario de espacios vectoriales.

Este teorema, que se puede probar a partir de todo lo que hemos visto hasta ahora, \textbf{sirve para dar una definición no constructiva del producto tensorial} de dos espacios vectoriales. A diferencia de lo que pasaba con la definición alternativa que se puede dar usando \customref{th:base_prod_tensorial}, esta definición no depende de ninguna base, y por lo tanto es igual de general que la definición por la que hemos optado.

\subsection{Descomposición CANDECOMP/PARAFAC}
\todo{Esta sección igual debería colocarla más adelante, porque no es del todo introductoria. Podría considerar que forma parte del paper, aunque realmente no sé cómo de conocida es la descomposición como para poder considerarse introducción}

Como ya se ha comentado en \customref{ch:introduccion}, las descomposiciones tensoriales van a ser fundamentales en este estudio. Las descomposiciones tensoriales buscan, en general, expresar un tensor cualquiera como función de otros tensores más sencillos, o incluso en función de vectores.

La primera descomposición con la que trabajamos (y la más sencilla de las dos que usamos) es la descomposición \textit{CANDECOMP/PARAFAC}. Para ello, comenzamos estudiando la siguiente propiedad:

\begin{proposicion}[Producto tensorial de dos vectores]
    Sean $\nv{v} \in R^{N}, \nv{w} \in \R^{M}$ dos vectores. Entonces su producto tensorial puede expresarse como:

    $$\nv{v} \otimes \nv{w} = \nv{v} \nv{w}^T \in \espaciomatrices{N}{M}$$

    Además, esta matriz es de rango uno o cero (este último caso cuando alguno de los dos vectores es $\vec{0}$)
\end{proposicion}

\begin{proof}

Sean $\nv{v} = (v_1, \ldots, v_N)$ y $\nv{w} = (w_1, \ldots, w_M)$, ambos no nulos. Entonces, usando la expresión del producto tensorial que introducimos en \customref{sec:otra_forma_tensores}, tenemos que:

\begin{equation}
    \nv{v} \otimes \nv{w} = (v_i w_j)_{i \in \deltaset{N},\ j \in \deltaset{M}}
\end{equation}

O lo que es lo mismo:

\begin{equation}
    \nv{v} \otimes \nv{w} = \begin{pmatrix}
        v_1 w_1 & v_1 w_2 & \ldots & v_1 w_M \\
        v_2 w_1 & v_2 w_2 & \ldots & v_1 w_M \\
        \vdots  & \vdots & & \vdots \\
        v_N w_1 & v_N w_2 & \ldots & v_N w_M \\
    \end{pmatrix}
\end{equation}

Que claramente se corresponde con la expresión de $\nv{v} \nv{w}^T$. Además, es claro que la fila $i$-ésima viene dada por el vector fila

$$(v_i w_1, v_i w_2, \ldots, v_i w_M) = v_i (w_1, \ldots, w_M) = v_i \nv{w}^T$$

Por tanto, todas las filas son combinación lineal de una de las filas.

En el caso de que alguno de los dos vectores sea $\vec{0}$, tenemos que la matriz $\nv{v} \otimes \nv{w}$ es la matriz de ceros, y por tanto su rango es cero.

Estudiado ese caso, ahora podemos suponer $\nv{v}, \nv{w} \neq \vec{0}$, y por tanto se debe cumplir que $\exists i_0 \in \deltaset{N}: v_{i_0} \neq 0$. Esa será la fila que escogemos para expresar el resto de filas como combinación lineal de esta, aprovechando de que es no nula. Dicha fila se puede expresar como $v_{i_0} \nv{w}^T$. La fila $i$-ésima se puede expresar como $v_i \nv{w}^T$. Y por tanto:

\begin{equation}
\begin{split}
    v_i \nv{w}^T = \lambda \cdot v_{i_0} \nv{w}^T \iif & \lambda = \frac{v_i}{v_{i_0}} \\
                                                               & \lambda \neq 0 \iif v_i \neq 0
\end{split}
\end{equation}

Con esto sabemos que $rank(\nv{v} \otimes \nv{w}) \leq 1$, pero podría darse el caso de que el rango fuese cero aún siendo $\nv{v}, \nv{w} \neq \vec{0}$. Sin embargo, esto no es posible. Como $\nv{v}, \nv{w} \neq \vec{0}$, entonces $\exists i_0 \in \deltaset{N}, \exists j_0 \in \deltaset{M}$ de modo que $v_{i_0}, w_{j_0} \neq 0$. Por tanto, el menor de orden 1 asociado a la fila $i_0$-ésima y columna $j_0$-ésima tiene el valor $v_{i_0} w_{j_0} \neq 0$, y con ello, $rank(\nv{v} \otimes \nv{w}) \geq 1$.

\end{proof}

\begin{observacion}
    Estamos escribiendo la igualdad $\nv{v} \otimes \nv{w} = \nv{v} \nv{w}^T$ sin definir lo que significa la igualdad entre un tensor de orden 2 y una matriz. Sin embargo, esta igualdad es totalmente natural. Decimos que un tensor de orden dos y una matriz son iguales cuando las dimensiones de la matriz y las dos dimensiones del tensor coinciden, y cuando las entradas de ambos coinciden.
\end{observacion}

Con esto, hemos caracterizado a aquellos tensores que se expresan como producto tensorial de dos vectores. Podemos generalizar esto al producto tensorial de $N$ vectores:

\begin{definicion}[Tensores puros]
    Un tensor $\mathcal{A}$ se dice que es \textbf{puro} (o también \textbf{elemental}) cuando es de la forma $\mathcal{A} = \otimes_{i = 1}^N \nv{v}^{(i)}$ con $\nv{v}^{(i)} \in \R^{N_i},\ \forall i \in \deltaset{N}$.
\end{definicion}
\todo{Esta definición está repetida de otra que ya teníamos}

\begin{observacion}
    A diferencia de lo que pasaba anteriormente, ahora no tenemos una forma tan directa de comparar estos tensores (de orden $N$) con matrices. Esto motiva en parte las siguientes herramientas que vamos a introducir.
\end{observacion}

Estamos ya en condiciones de introducir nuestra primera descomposición tensorial:

\begin{proposicion}[\textbf{Descomposición \textit{CP}}]
    Todo tensor $\mathcal{A}$ puede ser expresado como la suma de tensores puros, es decir:

    \begin{equation} \label{eq:cp_decomp}
        \mathcal{A} = \sum_{i = 1}^Z \nv{v_i^{(1)}} \otimes \ldots \nv{v_i^{(N)}}; \qquad
        \dspace \nv{v_i^{(k)}} \in,\R^{M_k}, \dspace \forall i \in \deltaset{Z}, \forall k \in \deltaset{N}
    \end{equation}
\end{proposicion}

A la descomposición anterior se le llama \textbf{descomposición \textit{CANDECOMP/PARAFAC}}, o abreviadamente, \textbf{descomposición \textit{CP}}

\begin{observacion}
    Esto descomposición es equivalente a:

    \begin{equation} \label{eq:cp_decomp}
        \mathcal{A} = \sum_{i = 1}^Z \lambda_i \cdot \nv{v_i^{(1)}} \otimes \ldots \nv{v_i^{(N)}}; \qquad
        \dspace \nv{v_i^{(k)}} \in \R^{M_k}, \lambda_i \in \R \dspace \forall i \in \deltaset{Z}, \forall k \in \deltaset{N}
    \end{equation}

    Puesto que podemos considerar:

    \begin{equation}
        \nv{w_i^{(1)}} := \lambda_i \cdot \nv{v_i^{(1)}} \in \R^{M_1}
    \end{equation}

    y, usando de nuevo las propiedades del producto tensorial:

    \begin{equation}
        \lambda_i \cdot \nv{v_i^{(1)}} \otimes \nv{v_i^{(2)}} \otimes \ldots \nv{v_i^{(N)}} = \nv{w_i^{(1)}} \otimes \nv{v_i^{(2)}} \otimes \ldots \nv{v_i^{(N)}}
    \end{equation}
\end{observacion}

\begin{proof}
    \todo{Hacer esta demostración. Más adelante hacemos otra en el que vemos que podemos hacerlo con $M^N$ sumandos. Pero ahí usamos $\espaciotensores{N}{M}$, aquí las dimensiones varían!}
\end{proof}

\begin{observacion}
    Notar que el número de vectores que multiplicamos tensorialmente es constante, no varía dicho número de vectores entre sumandos. Y dicho número coincide con el orden $N$ del tensor que estemos construyendo.
\end{observacion}

\begin{observacion}

En la demostración, hemos tomado $Z = TODO$ para asegurarnos de la existencia de una tal descomposición. Sin embargo, es razonable pensar que existirán combinaciones de vectores con las que podamos tomar un valor de $Z$ menor. Esto motivará la definición que haremos más adelante de \textit{rango CP}. De hecho, buscar una descomposición que minimice el valor de $Z$ es un problema de optimización complejo.
\todo{Escribir el valor de Z cuando hagamos la demostración}

\end{observacion}

\begin{definicion}[Rango \textit{CP}]
    Dado un tensor $\mathcal{A}$, se define su rango \textit{CP} como el mínimo valor de $Z$ para el cual la ecuación \eqref{eq:cp_decomp} se mantiene
\end{definicion}

Una propiedad interesante es la siguiente:

\begin{proposicion}[]
    Para un tensor de orden dos (que podemos ver como una matriz, por los isomorfismos previamente introducidos), su rango \textit{CP} coincide con el rango matricial usual
\end{proposicion}

\begin{proof}

\todo{TODO -- hay que demostrarlo}

\end{proof}
\section{Teoría de la medida}

\endinput
