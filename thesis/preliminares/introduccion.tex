% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción
%*******************************************************

% \manualmark

% \markboth{\textsc{Introducción}}{\textsc{Introducción}}

\chapter{Resumen}

\textbf{Palabras clave}: Análisis Tensorial, Descomposición Tensorial \textit{CP}, Descomposición Tensorial \textit{HT}, Aprendizaje Automático, Aprendizaje Profundo, Visión por Computador, Redes Convolucionales, Reconocimiento Facial Invariante a la Edad, \textit{Triplet Loss}.

El presente Trabajo de Fin de Grado tiene dos \textbf{objetivos principales}. El primero de ellos, asociado a la parte de matemáticas, realizar una modelización de las redes neuronales convolucionales, profundas y no profundas, y estudiar la propiedad conocida comúnmente como \textit{depth efficiency}. El segundo, asociado a la parte de informática, resolver un problema en el ámbito del reconocimiento facial invariante a cambios en la edad. Además, estudiaremos ciertas variantes sobre la función de pérdida \textit{Triplet Loss} buscando resolver algunos problemas asociados al uso de esta técnica. Dichas variantes suponen un gran esfuerzo de implementación, por lo tanto, buscamos aplicar patrones de diseño para que la arquitectura resultante sea fácil de modificar y extender.

El uso de técnicas de aprendizaje automático basado en redes neuronales profundas ha crecido enormemente en los últimos años. Su buen rendimiento en ciertas tareas específicas ha sido demostrado de forma contundente. Sin embargo, este buen rendimiento \textbf{se justifica principalmente sobre la evidencia experimental}. A pesar de que se han llevado a cabo estudios teóricos para comprender las causas detrás de este alto rendimiento en la práctica, generalmente estos estudios se quedan rezagados frente a la evidencia empírica. Los trabajos que realizan un estudio para justificar la línea experimental suelen trabajar con modelizaciones matemáticas alejadas de los modelos comunes del aprendizaje profundo y los resultados suelen tratar sobre casos concretos en los que las redes profundas son claramente superiores a las redes no profundas.

Nos apoyaremos principalmente sobre la publicación \cite{matematicas:principal}, que introduce las modelizaciones matemáticas y resultados que estudiaremos. Nuestro trabajo en esta línea ha consistido en entender y estudiar dicha publicación, corregir ciertos errores y presentar un estudio teórico más extenso y ordenado. Este estudio es novedoso por dos motivos principales: la \textbf{modelización matemática de las redes neuronales es muy cercana a la realidad}, y el resultado sobre \textbf{\textit{depth efficiency} da información concreta sobre la cantidad de escenarios} en los que ocurre este fenónmeno. Otros trabajos presentan modelizaciones que están en cierto modo alejadas de las arquitecturas empleadas en la práctica del aprendizaje automático. Por otro lado, presentan casos concretos en los que unos modelos son superiores a otros.

Expresaremos las funciones de puntuación que la red busca aprender en base a un tensor de coeficientes sobre un conjunto de funciones linealmente independiente y total. A partir de aquí, la modelización se basa en dos conocidas descomposiciones tensoriales, la descomposición \textit{CANDECOMP/PARAFAC} o \textit{CP} y la descomposición jerárquica \textit{Tucker} o \textit{HT}. Estas \textbf{modelizaciones tienen en cuenta las propiedades fundamentales de las redes convolucionales}: localidad de la convolución, coeficientes compartidos y operadores de \textit{pooling}.

Demostraremos \textbf{dos resultados centrales}. El primero nos muestra que, en el sentido de la medida de Lebesgue, casi todos los modelos no profundos necesitan un número exponencial de parámetros para implementar un modelo profundo. El segundo resultado añade que, de hecho, casi todos los modelos profundos no pueden ni siquiera ser aproximados por un modelo no profundo con menos de un número exponencial de parámetros.

En esta línea, \textbf{los objetivos de este estudio han sido alcanzados}. Hemos presentado y justificado una modelización matemática muy cercana a la realidad y hemos demostrado dos resultados que nos dan información exacta sobre en qué escenarios son más expresivas las redes profundas frente a las redes no profundas, y cuán frecuentemente sucede esto.

En la parte de informática hemos \textbf{trabajado un problema de reconocimiento facial invariante a la edad}, siendo de especial importancia el estudio de las técnicas introducidas en la publicación \cite{informatica:principal} aplicadas a la tarea de \textit{AIFR}, enfoque que según nuestro conocimiento nunca se había explorado previamente. Dichas técnicas consisten en dos variantes \textit{online} sobre la función de pérdida \textit{Triplet Loss}.  Además de los retos usuales de una solución basada en visión por computador debemos añadir todos los \textbf{retos asociados a cómo el envejecimiento afecta las características faciales} con las que trabajamos. Para resolver el problema planteado, realizamos un estudio del estado del arte, que nos indica las principales líneas de trabajo que más éxito han tenido. Estos trabajos utilizan arquitecturas y modelos que están fuera de nuestro alcance, tanto en conocimiento como en capacidad de cómputo. Principalmente redes generativas adversarias y modelos basados en mecanismos de atención.

Realizamos una \textbf{experimentación inicial y estudio de los resultados}. Dichos resultados son muy pobres, tanto que es imposible defenderlos. En este sentido, no cumplimos con el objetivo inicial del trabajo. Sin embargo, \textbf{planteamos tres nuevos objetivos}: identificar la raíz del problema, proponer una solución y validar su eficacia. Para comenzar, desarrollamos una amplia base de \textit{tests} para descartar que el motivo de los malos resultados sea un fallo por nuestra parte. Adicionalmente, exploramos distintas bases de código, en las que observamos los mismos resultados nefastos. Una vez descartado un problema por nuestra parte, buscamos el causante de los problemas. Identificamos un fallo en el diseño de las variantes de \textit{Triplet Loss} relacionado con un comportamiento degenerado en el que se queda atascado el proceso de entrenamiento. Justificamos teóricamente cómo ocurre este comportamiento degenerado y comprobamos experimentalmente que estamos en lo cierto. \textbf{Proponemos una solución en cierto modo original} al problema y validamos experimentalmente su eficacia. Observamos mejoras significativas en distintas métricas, incrementando su rendimiento entre dos y treinta veces, en dos conjuntos de datos distintos.

En resumen, \textbf{no cumplimos con el objetivo inicial del trabajo} al obtener resultados muy pobres. Pero no nos quedamos en este punto, proponemos la búsqueda de una solución eficaz al causante de estos resultados. En este sentido, \textbf{cumplimos contundente con los nuevos objetivos planteados}, al haber identificado el problema y haber propuesto una \textbf{solución cuya eficacia ha quedado más que demostrada}.

\endinput
