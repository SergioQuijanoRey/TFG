% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Introducción
%*******************************************************

% \manualmark

% \markboth{\textsc{Introducción}}{\textsc{Introducción}}

\chapter{Resumen}

\textbf{Palabras clave}: Análisis Tensorial, Descomposición Tensorial \textit{CP}, Descomposición Tensorial \textit{HT}, Aprendizaje Automático, Aprendizaje Profundo, Visión por Computador, Redes Convolucionales, Reconocimiento Facial Invariante a la Edad, \textit{Triplet Loss}

El presente Trabajo de Fin de Grado tiene dos \textbf{objetivos principales}. El primero de ellos,  realizar una modelización de las redes neuronales convolucionales, profundas y no profundas, y estudiar la propiedad conocida comúnmente como \textit{depth efficiency}. El segundo, resolver un problema en el ámbito del reconocimiento facial invariante a cambios en la edad. Además, estudiaremos ciertas variantes sobre la función de pérdida \textit{Triplet Loss} buscando resolver algunos problemas asociados al uso de esta técnica. Dichas variantes suponen un gran esfuerzo de implementación, por lo tanto, buscamos aplicar patrones de diseño para que la arquitectura resultante sea fácil de modificar y extender.

El uso de técnicas de aprendizaje automático basado en redes neuronales profundas ha crecido enormemente en los últimos años. Su buen rendimiento en ciertas tareas específicas ha sido demostrado de forma contundente. Sin embargo, este buen rendimiento \textbf{se justifica principalmente sobre la evidencia experimental}. A pesar de que se han llevado a cabo estudios teóricos para comprender las causas detrás de este alto rendimiento en la práctica, generalmente estos estudios se quedan rezagados frente a la evidencia empírica. Los trabajos que realizan un estudio para justificar la línea experimental suelen trabajar con modelizaciones matemáticas alejadas de los modelos comunes del \textit{deep learning}, y los resultados suelen tratar sobre casos concretos en los que las redes profundas son claramente superiores a las redes no profundas o \textit{shallow}.

El presente trabajo se fundamenta sobre la publicación \cite{matematicas:principal}, que introduce las modelizaciones matemáticas y resultados que estudiaremos. Nuestro trabajo en esta línea ha consistido en entender y estudiar dicha publicación, corregir ciertos errores y presentar un estudio teórico más extenso y ordenado. Este estudio es novedoso por dos motivos principales: la \textbf{modelización matemática de las redes neuronales es muy cercana a la realidad}, y el resultado sobre \textbf{\textit{depth efficiency} da información concreta sobre la cantidad de escenarios} en los que ocurre esta \textit{depth efficiency}. Otros trabajos presentan modelizaciones que están en cierto modo alejadas de las arquitecturas empleadas en soluciones prácticas de aprendizaje automático y presentan casos concretos en los que unos modelos son superiores a otros.

Expresaremos las funciones de puntuación que la red busca aprender en base a un tensor de coeficientes sobre una base de funciones linealmente independientes y totales. A partir de aquí, la modelización se basa en dos conocidas descomposiciones tensoriales, la descomposición \textit{CANDECOMP/PARAFAC} o \textit{CP} y la descomposición jerárquica \textit{Tucker} o \textit{HT}. Estas \textbf{modelizaciones tienen en cuenta las propiedades fundamentales de las redes convolucionales}: localidad de la convolución, coeficientes compartidos y operadores de \textit{pooling}.

Demostraremos \textbf{dos resultados centrales}. El primero nos muestra que, en el sentido de la medida de Lebesgue, casi todos los modelos no profundos necesitan un número exponencial de parámetros para implementar un modelo profundo. El segundo resultado añade que, de hecho, casi todos los modelos profundos no pueden ni siquiera ser aproximados por un modelo no profundo con menos de un número exponencial de parámetros.

En esta línea, \textbf{los objetivos de este estudio han sido alcanzados}. Hemos presentado una modelización matemática muy cercana a la realidad, y hemos demostrado dos resultados que nos dan información exacta sobre en qué escenarios son superiores las redes profundas frente a las redes no profundas, y cuán frecuentemente sucede esto.

Por otro lado, hemos \textbf{trabajado un problema en el ámbito del reconocimiento facial invariante a la edad}. Esta tarea resulta muy interesante en el ambiente de la \textbf{informática forense}, por todas las aplicaciones prácticas que tiene disponer de un modelo capaz de identificar a un individuo en distintas imágenes independientemente de la edad con la que aparezca. Buscamos aplicar las técnicas introducidas en la publicación \cite{informatica:principal} en la tarea de \textit{AIFR}, enfoque que según nuestro conocimiento nunca se había aplicado a esta tarea.

Dicha tarea presenta una complejidad elevada. Además de los retos usuales de una solución basada en visión por computador debemos añadir todos los \textbf{retos asociados a cómo el envejecimiento afecta las características faciales} con las que trabajamos. Para resolver el problema planteado, realizamos un estudio del estado del arte, que nos indica las principales líneas de trabajo que más éxito han tenido. Estos trabajos utilizan principalmente redes generativas adversarias para descomponer los datos de entrada en dos características incorreladas: edad e identidad. Algunos trabajos utilizan modelos basados en atención para aprender a resolver esta tarea. Este enfoque usa técnicas y modelos avanzados, que quedan fuera del alcance del presente trabajo.

Realizamos un estudio de los conjuntos de datos disponibles. Estos son escasos y en ocasiones de una calidad pobre. Estudiamos el tamaño de los conjuntos de datos, las distribuciones del número de imágenes por individuo (fundamental para aplicar nuestras técnicas), la distribución de edad de los individuos y la distribución del rango de edad de los individuos. En base a este estudio tomamos una decisión informada de qué conjuntos usar y qué protocolo experimental aplicar. Siguiendo la línea más común en el estado del arte, entrenamos sobre un conjunto de datos grande, \textit{CACD}, y validamos sobre el conjunto \textit{FG-Net}, que presenta un gran reto por su gran variedad en edades y rangos.

Desarrollamos una amplia base de código para poder aplicar las técnicas estudiadas. Realizamos una optimización del código de partida guiada por datos. Por otro lado, debido a los malos resultados obtenidos en la experimentación, aplicamos una amplia \textit{suite} de \textit{tests} para validar que los malos resultados no estén provocados por fallos en la implementación.

Realizamos una \textbf{experimentación y estudio de los resultados}. Dichos resultados están por debajo de las expectativas iniciales. Sin embargo, son en parte justificados. Son muy pocos los trabajos realizados sobre la re-identificación de personas invariante a cambios en la edad. Los trabajos que han obtenido buenos resultados se han basado en arquitecturas que no nos hemos planteado estudiar (tanto por una falta de conocimiento técnico por nuestra parte como por una falta de medios computacionales). Por ejemplo, estos trabajos se basan en modelos generativos adversarios y especialmente en arquitecturas basadas en mecanismos de atención (\textit{transformers}), los cuales requieren de requisitos computacionales muy elevados. Además, la solución técnica que proponemos nunca ha sido empleada para resolver esta tarea. Por lo tanto, a la hora de entender el motivo por el cuál obtenemos estos resultados, no disponemos de una literatura extensa sobre la que basarnos.

Por lo tanto, \textbf{se ha cumplido el objetivo de desarrollar una base de código bien diseñada}, con la que sea cómodo trabajar y experimentar. Sin embargo, los \textbf{resultados experimentales no son los esperados y quedan lejos de las expectativas iniciales}. Teniendo esto en cuenta, \textbf{hemos estudiado por qué nuestra solución no resuelve satisfactoriamente la tarea} de re-identificación de personas invariante a cambios en la edad.

\todo{Volver a leer antes de enviar el borrador, tiene que estar perfecto}

\endinput
