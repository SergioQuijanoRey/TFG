% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Abstract}


\textbf{Keywords:} Tensor Analysis, \textit{CP} Tensor Decomposition, \textit{HT} Tensor Decomposition, Machine Learning, Deep Learning, Computer Vision, Convolutional Networks, Age-Invariant Facial Recognition, \textit{Triplet Loss}.

The present Bachelor's Thesis has two \textbf{main objectives}. The first one, related to the mathematics part, involves modeling convolutional neural networks, both deep and shallow, and studying the property commonly known as \textit{depth efficiency}. The second objective, related to the computer science part, is to address an age-invariant facial recognition task, with special focus on studying certain variations of the \textit{Triplet Loss} loss function, aiming to resolve some issues associated with its use.

The use of deep neural network-based machine learning techniques has grown enormously in recent years. Their excellent performance in certain specific tasks has been convincingly demonstrated. However, this good performance \textbf{is solely justified based on experimental evidence}. A comprehensive and fruitful study on the theoretical reasons supporting the experimental results has not been conducted. Works that undertake a study to support the experimental line of evidence often deal with mathematical modelizations that are distant from common deep learning models, and the results typically focus on specific cases where deep networks are clearly superior to shallow or non-deep networks. The main publication we primarily refer to is \cite{matematicas:principal}.

Our work in this direction is innovative for two main reasons: the \textbf{mathematical modeling of neural networks is very close to reality}, and the result on \textbf{\textit{depth efficiency} provides specific information about the number of scenarios} in which this \textit{depth efficiency} occurs.

We will express the scoring functions that the network aims to learn based on a tensor of coefficients over a basis of linearly independent and total functions. From here, the modeling is based on two well-known tensor decompositions, the \textit{CANDECOMP/PARAFAC} or \textit{CP} decomposition and the hierarchical \textit{Tucker} or \textit{HT} decomposition. These \textbf{modelings take into account the fundamental properties of convolutional networks}: convolutional locality, coefficient sharing, and pooling operators.

We will prove \textbf{two main results}. The first one shows that, in terms of the Lebesgue measure, almost all shallow models require an exponential number of parameters to implement a deep model. The second result adds that, in fact, almost all deep models cannot even be approximated by a shallow model with fewer than an exponential number of parameters.

In this vein, \textbf{the objectives of this study have been achieved}. We have presented a mathematical modeling very close to reality, and we have demonstrated two results that provide precise information about in which scenarios deep networks outperform shallow networks and how often this occurs.

In the computer science part we have worked on an age-invariant face recognition problem. In addition, of particular importance has been the study of techniques introduced in the \cite{informatica:principal} publication applied to this task, an approach that to our knowledge had never been previously explored. These techniques consist of two \textit{online} variants on the \textit{Triplet Loss} function \cite{informatica:principal}. In addition to the usual challenges of a computer vision-based solution, we must add all the challenges associated with how aging affects facial features.

Concerning the study of the \textit{online} variants of \textit{Triplet Loss}, thanks to experimentation, we identified a problem in their design. We propose an original solution and experimentally validate its high efficiency. Once we have a method that generates good results thanks to this solution, we obtain a model that obtains a value of 0.6 in \textit{Rank@5 Accuracy} for the \textit{CACD} dataset. Moreover, our solution allows to improve different metrics between 2 and 10 times in \textit{MNIST} and between 10 and 30 times in \textit{CACD}.

We have described step by step the process that allowed us to find this original solution. Preliminary experimentation showed unusual results. We observed these same results in third party experiments. By studying such preliminary experimentation we located a problem in the design of the loss function. Based on this, we proposed an original solution that we validated by repeating the experiments and obtaining the significant improvements we have already discussed.

Therefore, we consider that we have satisfactorily fulfilled our objectives. First, we have studied in depth the online variants of Triplet Loss. We have detected a problem in their design, proposed an original solution and experimentally validated its high efficiency. Without our solution, these variants \textit{online} do not produce good results and, therefore, we believe that \textbf{our proposal has great value}. Second, we have obtained a model that exhibits good results in the age-invariant face recognition task. Central to this has been our proposed solution.

\selectlanguage{spanish}
\endinput
