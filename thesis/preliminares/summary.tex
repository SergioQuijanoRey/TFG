% !TeX root = ../libro.tex
% !TeX encoding = utf8
%
%*******************************************************
% Summary
%*******************************************************

\selectlanguage{english}
\chapter{Abstract}


\textbf{Keywords:} Tensor Analysis, \textit{CP} Tensor Decomposition, \textit{HT} Tensor Decomposition, Machine Learning, Deep Learning, Computer Vision, Convolutional Networks, Age-Invariant Facial Recognition, \textit{Triplet Loss}.

The present Bachelor's Thesis has two \textbf{main objectives}. The first one, related to the mathematics part, involves modeling convolutional neural networks, both deep and shallow, and studying the property commonly known as \textit{depth efficiency}. The second objective, related to the computer science part, is to address an age-invariant facial recognition task, with special focus on studying certain variations of the \textit{Triplet Loss} loss function, aiming to resolve some issues associated with its use.

The use of deep neural network-based machine learning techniques has grown enormously in recent years. Their excellent performance in certain specific tasks has been convincingly demonstrated. However, this good performance \textbf{is solely justified based on experimental evidence}. A comprehensive and fruitful study on the theoretical reasons supporting the experimental results has not been conducted. Works that undertake a study to support the experimental line of evidence often deal with mathematical modelizations that are distant from common deep learning models, and the results typically focus on specific cases where deep networks are clearly superior to shallow or non-deep networks. The main publication we primarily refer to is \cite{matematicas:principal}.

Our work in this direction is innovative for two main reasons: the \textbf{mathematical modeling of neural networks is very close to reality}, and the result on \textbf{\textit{depth efficiency} provides specific information about the number of scenarios} in which this \textit{depth efficiency} occurs.

We will express the scoring functions that the network aims to learn based on a tensor of coefficients over a basis of linearly independent and total functions. From here, the modeling is based on two well-known tensor decompositions, the \textit{CANDECOMP/PARAFAC} or \textit{CP} decomposition and the hierarchical \textit{Tucker} or \textit{HT} decomposition. These \textbf{modelings take into account the fundamental properties of convolutional networks}: convolutional locality, coefficient sharing, and pooling operators.

We will prove \textbf{two main results}. The first one shows that, in terms of the Lebesgue measure, almost all shallow models require an exponential number of parameters to implement a deep model. The second result adds that, in fact, almost all deep models cannot even be approximated by a shallow model with fewer than an exponential number of parameters.

In this vein, \textbf{the objectives of this study have been achieved}. We have presented a mathematical modeling very close to reality, and we have demonstrated two results that provide precise information about in which scenarios deep networks outperform shallow networks and how often this occurs.

In the computer science part we have \textbf{worked an age-invariant face recognition problem}, being of particular importance the study of techniques introduced in the publication \cite{informatica:principal} applied to the \textit{AIFR} task, an approach that to our knowledge had never been previously explored. These techniques consist of two \textit{online} variants on the \textit{Triplet Loss} function. In addition to the usual challenges of a computer vision-based solution, we must add all the challenges associated with how aging affects facial features that we work with. This study is novel, because when investigating the state of the art we find that they do not use this approach to solve the task. Furthermore, they use architectures and models that are out of our reach, both in knowledge and computational capacity. Mainly generative adversarial networks and models based on attention mechanisms.

We performed an initial experimentation and study of the results. These results are very poor, so much so that it is impossible to defend them. In this sense, we did not fulfill the initial objective of the work. However, we set three new objectives: to identify the root of the problem, to propose a solution and to validate its effectiveness. To begin with, we developed an extensive database of tests to rule out that the reason for the poor results is a failure on our part. Additionally, we explored different code bases, in which we observed the same dismal results. Once we ruled out a problem on our part, we looked for the cause of the problems. We identify a flaw in the design of the variants of \textit{Triplet Loss} related to a degenerate behavior in which the training process gets stuck. We justify theoretically how this degenerate behavior occurs and verify experimentally that we are correct. We propose a somewhat original solution to the problem and experimentally validate its effectiveness. We observe significant improvements in different metrics, increasing their performance between two and thirty times, on two different data sets.

In short, we did not fulfill the initial objective of the work by obtaining very poor results. But we do not stop at this point, we propose the search for an effective solution to the cause of these results. In this sense, \textbf{we have fulfilled the new objectives set}, having identified the problem and proposed a solution whose effectiveness has been more than demonstrated. This is of great value, not only because of its proven effectiveness, but also because it is applicable to other areas and tasks in which an attempt has been made to apply \textit{triplet loss} or any of its variants and in which the problem we have found and solved has appeared.

\selectlanguage{spanish}
\endinput
