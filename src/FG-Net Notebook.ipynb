{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231cd5e5-28e2-4d34-87a3-d921d2d89e38",
   "metadata": {},
   "source": [
    "# FG-Net\n",
    "\n",
    "- The purpose of this notebook is to provide the EDA done before trying to solve this dataset\n",
    "- Found this dataset in [this webpage](https://yanweifu.github.io/FG_NET_data/)\n",
    "- We have a [papers with code entry](https://paperswithcode.com/dataset/fg-net) where we can find how some people have used this dataset\n",
    "- From the *papers with code* entry, I find the **[following paper](https://arxiv.org/abs/1602.06149)**\n",
    "    - They propose a new dataset, Large Age-Gap dataset (LAG dataset)\n",
    "    - They talk about **LFW dataset**: is the most famous dataset where there are almost no constraints (lighting, pose...)\n",
    "    - But they constraint age, which is in what we are interested in!\n",
    "    - They talk about **FG-NET dataset** as one of the most famous datasets with aging gaps\n",
    "    - So I think it is **valuable to talk about this paper in my thesis**\n",
    "- *Papers with code* says that [this github repo](https://github.com/Hzzone/MTLFace) has the best model for the age-invariant recognition problem\n",
    "    - They have a related [paper](https://arxiv.org/abs/2103.01520)\n",
    "    - They use attention mechanisms\n",
    "    - They talk about age-invariant face recognition or _**AIFR**_\n",
    "    - They have a table with the results of different papers in this dataset, so **it can be interesting to talk about this paper in my thesis**\n",
    "    - They say that *FG-NET* is the most challenging dataset for *AIFR*\n",
    "    - They **describe precisely how testing is done**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77db779-ffab-4cef-9b0a-0963f969c035",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da856e-6736-46be-a867-76909b9590e8",
   "metadata": {
    "id": "41da856e-6736-46be-a867-76909b9590e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests, zipfile, io\n",
    "import itertools\n",
    "from typing import Union, Tuple, List\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "import lib.visualizations as visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LPwVGyBZIN3q",
   "metadata": {
    "id": "LPwVGyBZIN3q"
   },
   "source": [
    "# Global parameters of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3J1WEPR9IQDB",
   "metadata": {
    "id": "3J1WEPR9IQDB"
   },
   "outputs": [],
   "source": [
    "# Lib to define paths\n",
    "import os\n",
    "\n",
    "# - For ease of use, we are going to store all global parameters into a dict\n",
    "# - This way, we can pass this dict directly to wandb init, so we can keep track\n",
    "# of which parameters produced which output\n",
    "\n",
    "from typing import Dict, Union\n",
    "GLOBALS: Dict[str, Union[str, int, float, bool]] = dict()\n",
    "\n",
    "# Define if we are running the notebook in our computer (\"local\")\n",
    "# or in Google Colab (\"remote\")\n",
    "GLOBALS['RUNNING_ENV'] = \"local\"\n",
    "\n",
    "# Base path for the rest of paths defined in the notebook\n",
    "GLOBALS['BASE_PATH'] = \"./\" if GLOBALS['RUNNING_ENV'] == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "# Path to our lib dir\n",
    "GLOBALS['LIB_PATH'] = os.path.join(GLOBALS['BASE_PATH'], \"lib\")\n",
    "\n",
    "# Path where we store training / test data\n",
    "GLOBALS['DATA_PATH'] = os.path.join(GLOBALS['BASE_PATH'], \"data/FG_NET\")\n",
    "\n",
    "# URL of the zipfile with the dataset\n",
    "GLOBALS['DATASET_URL'] = \"http://yanweifu.github.io/FG_NET_data/FGNET.zip\"\n",
    "\n",
    "# Dataset has images and metadata. Here we store the path to the img dir \n",
    "GLOBALS['IMAGE_DIR_PATH'] = os.path.join(GLOBALS['DATA_PATH'], \"FGNET/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btdP3TCgJMDx",
   "metadata": {
    "id": "btdP3TCgJMDx"
   },
   "source": [
    "# Auth for Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXPJXSQ0JNc2",
   "metadata": {
    "id": "rXPJXSQ0JNc2"
   },
   "outputs": [],
   "source": [
    "if GLOBALS['RUNNING_ENV'] == \"remote\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VK-X452lIQ46",
   "metadata": {
    "id": "VK-X452lIQ46"
   },
   "source": [
    "# Dataset downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcbcb0-8f63-43b6-b3cc-e71de37feb3d",
   "metadata": {
    "id": "14dcbcb0-8f63-43b6-b3cc-e71de37feb3d"
   },
   "outputs": [],
   "source": [
    "def get_size(start_path = '.'):\n",
    "    \"\"\"\n",
    "    Got from:\n",
    "        https://stackoverflow.com/questions/1392413/calculating-a-directorys-size-using-python\n",
    "    \"\"\"\n",
    "    total_size = 0\n",
    "    for dirpath, dirnames, filenames in os.walk(start_path):\n",
    "        for f in filenames:\n",
    "            fp = os.path.join(dirpath, f)\n",
    "            # skip if it is symbolic link\n",
    "            if not os.path.islink(fp):\n",
    "                total_size += os.path.getsize(fp)\n",
    "\n",
    "    return total_size\n",
    "\n",
    "\n",
    "def download_dataset(path: str, url: str, can_skip_download: bool = False):\n",
    "    \"\"\"\"Downloads and extracts the dataset from a given `url` into a given `path`\"\"\"\n",
    "    \n",
    "    # Create the dir if it does not exist\n",
    "    if os.path.exists(path) is False:\n",
    "        print(f\"Dir {path} does not exist, creating that dir\")\n",
    "        os.mkdir(path)\n",
    "        \n",
    "\n",
    "    # If the dir has a filesize bigger than 42.2MB, then it should be already \n",
    "    # downloaded, and we can skip this step. However, the user can tell this \n",
    "    # function to do not skip, so they assure there has not been any data \n",
    "    # corruption\n",
    "    file_B = get_size(path)\n",
    "    file_MB = file_B / (1024 * 1024)\n",
    "    \n",
    "    if file_MB > 44.2 and can_skip_download is True:\n",
    "        print(\"Skipping the download, files are already downloaded\")\n",
    "        return\n",
    "\n",
    "    # Download the dataset\n",
    "    try:\n",
    "        print(\"Downloading the dataset\")\n",
    "        req = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: could not download data from url\")\n",
    "        print(f\"ERROR: error is:\\n{e}\")\n",
    "        return\n",
    "        \n",
    "    # Extract the dataset contents\n",
    "    print(\"Extracting the dataset contents\")\n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(req.content))\n",
    "    zip_file.extractall(path)\n",
    "\n",
    "    print(\"Succesful download\")\n",
    "\n",
    "    \n",
    "download_dataset(\n",
    "    GLOBALS['DATA_PATH'],\n",
    "    GLOBALS['DATASET_URL'],\n",
    "    can_skip_download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978ddb50-5fb3-4ec5-8acd-0046a679ba57",
   "metadata": {},
   "source": [
    "# Putting the data into a pytorch `Dataset`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c2b77-54a9-45d4-9121-88d340f7aba3",
   "metadata": {
    "id": "176c2b77-54a9-45d4-9121-88d340f7aba3"
   },
   "outputs": [],
   "source": [
    "# TODO -- properly document this class\n",
    "# TODO -- say that we are not using all the metadata that the dataset has\n",
    "class FGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: str, transform = None):\n",
    "        \n",
    "        # Path of the dir where all the images are stored\n",
    "        # NOTE: This is the path of the image dir, and not the dataset dir, where\n",
    "        # some metadata is also stored\n",
    "        self.path = path\n",
    "        \n",
    "        # Transformation to apply to the images of the dataset\n",
    "        # Items of this dataset are made up of: image, id and age\n",
    "        # But the transformation is done only to the image\n",
    "        self.transform = transform\n",
    "        \n",
    "        # Dict containing the following data:\n",
    "        # - Keys are the ids of the individuals\n",
    "        # - Values are lists with all the ages associated with that individual\n",
    "        self.individuals: Optional[Dict[int, List[int]]] = None\n",
    "        \n",
    "        # Number of images stored in this class \n",
    "        self.__number_images: Union[int, None] = None\n",
    "        \n",
    "        # All the filenames of the images stored in `self.path` dir \n",
    "        self.file_names: Union[List, None] = None\n",
    "\n",
    "        # Get the data from the dir and instanciate all the attributes of this class\n",
    "        # Before that, all the attrs are None\n",
    "        self.__generate_dataset()\n",
    "                \n",
    "        # Check that the dataset is properly created\n",
    "        self.__check_integrity()\n",
    "\n",
    "        super(FGDataset, self).__init__()\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        # Check that we have the number of images of the dataset\n",
    "        if self.__number_images is None:\n",
    "            raise Exception(\"Dataset is not initialized, thus, number of images is unknown\")\n",
    "        \n",
    "        return self.__number_images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "\n",
    "        # Get the image from the index\n",
    "        img_name = os.path.join(self.path, self.file_names[idx])\n",
    "        image = torchvision.io.read_image(img_name)\n",
    "\n",
    "        # Get the id and age from the file_name\n",
    "        id, age = self.__id_and_age_from_file_name(self.file_names[idx])\n",
    "\n",
    "        # Put together all the info\n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"id\": id,\n",
    "            \"age\": age,\n",
    "        }\n",
    "\n",
    "        # Items are made up of: image, id and age; as the prev code shows\n",
    "        # But the transform is only made to the image, that is the only \n",
    "        # part of the dict where it makes sense\n",
    "        if self.transform:\n",
    "            sample[\"image\"] = self.transform(sample[\"image\"])\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __generate_dataset(self):\n",
    "\n",
    "        # Get all the names of the files\n",
    "        self.file_names = os.listdir(self.path)\n",
    "\n",
    "        # Use that for computing the size\n",
    "        self.__number_images = len(self.file_names)\n",
    "            \n",
    "        self.individuals = dict()\n",
    "\n",
    "        # Use the names to get the persons IDs and their ages\n",
    "        for file_name in self.file_names:\n",
    "\n",
    "            # Split into id and age\n",
    "            id, age = self.__id_and_age_from_file_name(file_name)\n",
    "\n",
    "            \n",
    "            # If there is not already a instance for this id, create it \n",
    "            # and set the initial value for the list \n",
    "            if self.individuals.get(id) is None:\n",
    "                self.individuals[id] = [age]\n",
    "                continue\n",
    "                \n",
    "            # This individual already has a list (we have checked before)\n",
    "            # so append to that list\n",
    "            self.individuals[id].append(age)\n",
    "\n",
    "    def __id_and_age_from_file_name(self, file_name: str) -> Tuple[str, str]:\n",
    "        # Remove file extension\n",
    "        file_name_no_extension = file_name.split(\".JPG\")[0]\n",
    "\n",
    "        # Split into id and age\n",
    "        id, age = file_name_no_extension.split(\"A\")\n",
    "\n",
    "        # Age can contain trailing letters, for example, the entries corresponding to:\n",
    "        # `068A10a.JPG` and `068A10b.JPG`\n",
    "        age = ''.join(filter(str.isdigit, age))\n",
    "\n",
    "        # Now it is safe to cast both id and age to an int\n",
    "        id, age = int(id), int(age)\n",
    "        \n",
    "        return id, age\n",
    "\n",
    "    \n",
    "    def __check_integrity(self):\n",
    "                \n",
    "        # Check that we have the proper number of images \n",
    "        assert self.__len__() == 1002\n",
    "    \n",
    "    \n",
    "transform = T.transforms.Compose([\n",
    "    T.ToPILImage(),\n",
    "])\n",
    "dataset = FGDataset(path = GLOBALS['IMAGE_DIR_PATH'], transform = transform)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db5e34f1-f448-4882-afda-df8d9725d2a4",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a0d7de9-3900-4679-8492-d9aa525dbe0a",
   "metadata": {},
   "source": [
    "## Show some examples of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQ6sifNvNhZE",
   "metadata": {
    "id": "MQ6sifNvNhZE"
   },
   "outputs": [],
   "source": [
    "# Get a single element of the dataset\n",
    "\n",
    "for index in range(3):\n",
    "    \n",
    "    sample = dataset[index]\n",
    "    img = sample[\"image\"]\n",
    "    age = sample[\"age\"]\n",
    "    id = sample[\"id\"]\n",
    "    \n",
    "    print(f\"Id {id} at age {age}\")\n",
    "\n",
    "    plt.imshow(img)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0374a9bb-f68b-4fb8-ab7a-77354e416a6e",
   "metadata": {},
   "source": [
    "## Show all the images of a given individual, identified by its ID, sorted by their age"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16a03957-e166-438b-8107-5c3b669e3899",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Set the id of the individual we want to identify\n",
    "id = 14\n",
    "\n",
    "# Select all the indixes corresponding to that individual\n",
    "id_indixes = [idx for idx, element in enumerate(dataset) if element[\"id\"] == id]\n",
    "\n",
    "# Sort the list of indixes by age\n",
    "id_indixes = sorted(\n",
    "    id_indixes, \n",
    "    key = lambda id: dataset[id][\"age\"],\n",
    "    reverse = False\n",
    ")\n",
    "\n",
    "# With the sorted list of indixes, now we can get the images \n",
    "# and also use the ages as the title for the subplots\n",
    "\n",
    "images = [\n",
    "    dataset[idx][\"image\"]\n",
    "    for idx in id_indixes\n",
    "]\n",
    "\n",
    "ages = [dataset[idx][\"age\"] for idx in id_indixes]\n",
    "titles = [f\"Age: {age}\" for age in ages]\n",
    "\n",
    "# Plot the images\n",
    "visualizations.PIL_show_images_with_titles_same_window(images, titles = ages, figsize = (20, 40))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73232964-8972-4688-8f5f-e3c867db45b7",
   "metadata": {},
   "source": [
    "Checking different ID's shows us that the dataset generation seems to be properly implemented."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d8215f1-d31f-4ce6-8300-d74091183f1e",
   "metadata": {},
   "source": [
    "## Exploring the *images-per-person* distribution\n",
    "\n",
    "- One key aspect of the problem we are solving is the number of images per person\n",
    "- For example, when doing `P-K` sampling, if there are persons with less than `K` images, there might be a problem (we have some mechanisms to deal with that problem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd910a2a-acec-486c-9b4b-1a4f9d838ab9",
   "metadata": {},
   "source": [
    "First, show the histogram of how many images per person there are:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R2qDSe8cVQQE",
   "metadata": {
    "id": "R2qDSe8cVQQE"
   },
   "outputs": [],
   "source": [
    "# Remember that `dataset.individuals` is a dict with keys the indixes of persons and with values\n",
    "# lists of ages (each age correspond to a stored image, thus there might be repeated ages if there\n",
    "# are more of one image for one concrete age)\n",
    "imgs_per_user = [len(individual_imgs) for individual_imgs in dataset.individuals.values()]\n",
    "\n",
    "# Now, plot the distribution of the ages\n",
    "visualizations.plot_histogram(\n",
    "    values = imgs_per_user,\n",
    "    num_bins = 20,\n",
    "    title = \"Images per user\",\n",
    "    xlabel = \"Images per user\",\n",
    "    ylabel = \"Number of instances\",\n",
    "    figsize = (10, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf2f0baf-5505-456b-824b-b9dd120d1b5f",
   "metadata": {
    "tags": []
   },
   "source": [
    "There seems to be at least 6 images per person, and at most 18. The distribution seems to follow a normal distribution, but we are not interested in checking that assumption. Check the numbers putting the data into a pandas dataframe:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "928ee4f8-3da9-4ec9-8601-b1d2e7c4dc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "imgs_per_user_df = pd.DataFrame({\n",
    "    \"IDs\": dataset.individuals.keys(),\n",
    "    \"NÂº images\": imgs_per_user\n",
    "})\n",
    "imgs_per_user_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12234830-29c7-4f91-9d49-a7befc923d56",
   "metadata": {
    "tags": []
   },
   "source": [
    "So, in fact, we have at least 6 images per class, and 18 images per class at most! This is a huge improvement from the *LFW dataset*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2870ff-2903-43b3-a588-572cfe15346f",
   "metadata": {},
   "source": [
    "## Exploring the age distribution\n",
    "\n",
    "- We have worked with the *LFW dataset*, but there was no variance in the age distribution (which is a key component in our problem)\n",
    "- So now study that age distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5wpu8ZSe3W",
   "metadata": {
    "id": "6a5wpu8ZSe3W"
   },
   "outputs": [],
   "source": [
    "# Get a flat list with all the ages in the dataset\n",
    "ages = dataset.individuals.values()\n",
    "ages = list(itertools.chain(*ages))\n",
    "\n",
    "# Now, plot the histogram of the ages distribution\n",
    "visualizations.plot_histogram(\n",
    "    values = ages,\n",
    "    num_bins = 70,\n",
    "    title = \"Age distribution\",\n",
    "    xlabel = \"Age\",\n",
    "    ylabel = \"Number of instances\",\n",
    "    figsize = (10, 8)\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f81c8e-1005-44ce-a8f4-1ca19f48b33c",
   "metadata": {},
   "source": [
    "This histogram shows us a Skewed distribution. That is to say, we have more samples of young people than older people. This bias can be a problem if we want to use the trained model in real world enviroments, where the distribution can vary a lot! \n",
    "\n",
    "We can observe a great spike around the 20's. Lets explore further this distribution with a few basic metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5dc18926-78ee-4796-9ab9-78263719aa74",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Mean age: {np.mean(ages)}\")\n",
    "print(f\"Min age: {min(ages)}\")\n",
    "print(f\"Max age: {max(ages)}\")\n",
    "print(f\"Most frequent age = {max(set(ages), key = ages.count)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916e9cb9-b6fe-4419-a8a1-e03a6286d3ec",
   "metadata": {
    "tags": []
   },
   "source": [
    "The most frequent age, that we saw in the histogram, is in fact eighteen years. Ages range from 0 to 69 years. Lets get more detailed information using pandas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bab2035-aa26-4fa3-af4e-776c5edb687a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "min_ages = [min(age_list) for age_list in dataset.individuals.values()]\n",
    "max_ages = [max(age_list) for age_list in dataset.individuals.values()]\n",
    "\n",
    "ages_df = pd.DataFrame({\n",
    "    \"IDs\": dataset.individuals.keys(),\n",
    "    \"Min age\": min_ages,\n",
    "    \"Max age\": max_ages,\n",
    "})\n",
    "ages_df[\"Age range\"] = ages_df[\"Max age\"] - ages_df[\"Min age\"]\n",
    "\n",
    "ages_df.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd373546-25c9-4b85-a42e-a56b8bb3b8de",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ages_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb296d76-8087-4d32-bc36-e474e57a84f6",
   "metadata": {},
   "source": [
    "We can see that, at least, we have 11 years of difference among images of the same person. The mean age range is 27.80 years, which can make solving this task hard. But in the other hand, shows that this dataset is relevant for the problem that we are trying to solve. The biggest age range is 54 years.\n",
    "\n",
    "Let's see an histogram for the age range:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae140b34-1552-4915-b456-b739846fbac1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "visualizations.plot_histogram(\n",
    "    values = ages_df[\"Age range\"],\n",
    "    num_bins = 82,\n",
    "    title = \"Distribution of the age range\",\n",
    "    xlabel = \"Difference in years for the same person\",\n",
    "    ylabel = \"Frequency\",\n",
    "    figsize = (15, 10)\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
