{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "231cd5e5-28e2-4d34-87a3-d921d2d89e38",
   "metadata": {},
   "source": [
    "# FG-Net\n",
    "\n",
    "- The purpose of this notebook is to provide the EDA done before trying to solve this dataset\n",
    "- Found this dataset in [this webpage](https://yanweifu.github.io/FG_NET_data/)\n",
    "- We have a [papers with code entry](https://paperswithcode.com/dataset/fg-net) where we can find how some people have used this dataset\n",
    "- From the *papers with code* entry, I find the **[following paper](https://arxiv.org/abs/1602.06149)**\n",
    "    - They propose a new dataset, Large Age-Gap dataset (LAG dataset)\n",
    "    - They talk about **LFW dataset**: is the most famous dataset where there are almost no constraints (lighting, pose...)\n",
    "    - But they constraint age, which is in what we are interested in!\n",
    "    - They talk about **FG-NET dataset** as one of the most famous datasets with aging gaps\n",
    "    - So I think it is **valuable to talk about this paper in my thesis**\n",
    "- *Papers with code* says that [this github repo](https://github.com/Hzzone/MTLFace) has the best model for the age-invariant recognition problem\n",
    "    - They have a related [paper](https://arxiv.org/abs/2103.01520)\n",
    "    - They use attention mechanisms\n",
    "    - They talk about age-invariant face recognition or _**AIFR**_\n",
    "    - They have a table with the results of different papers in this dataset, so **it can be interesting to talk about this paper in my thesis**\n",
    "    - They say that *FG-NET* is the most challenging dataset for *AIFR*\n",
    "    - They **describe precisely how testing is done**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f77db779-ffab-4cef-9b0a-0963f969c035",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41da856e-6736-46be-a867-76909b9590e8",
   "metadata": {
    "id": "41da856e-6736-46be-a867-76909b9590e8"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import requests, zipfile, io\n",
    "import itertools\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as T\n",
    "\n",
    "from typing import Union, Tuple, List"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "LPwVGyBZIN3q",
   "metadata": {
    "id": "LPwVGyBZIN3q"
   },
   "source": [
    "# Global parameters of the notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3J1WEPR9IQDB",
   "metadata": {
    "id": "3J1WEPR9IQDB"
   },
   "outputs": [],
   "source": [
    "# Lib to define paths\n",
    "import os\n",
    "\n",
    "# - For ease of use, we are going to store all global parameters into a dict\n",
    "# - This way, we can pass this dict directly to wandb init, so we can keep track\n",
    "# of which parameters produced which output\n",
    "\n",
    "from typing import Dict, Union\n",
    "GLOBALS: Dict[str, Union[str, int, float, bool]] = dict()\n",
    "\n",
    "# Define if we are running the notebook in our computer (\"local\")\n",
    "# or in Google Colab (\"remote\")\n",
    "GLOBALS['RUNNING_ENV'] = \"local\"\n",
    "\n",
    "# Base path for the rest of paths defined in the notebook\n",
    "GLOBALS['BASE_PATH'] = \"./\" if GLOBALS['RUNNING_ENV'] == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "# Path to our lib dir\n",
    "GLOBALS['LIB_PATH'] = os.path.join(GLOBALS['BASE_PATH'], \"lib\")\n",
    "\n",
    "# Path where we store training / test data\n",
    "GLOBALS['DATA_PATH'] = os.path.join(GLOBALS['BASE_PATH'], \"data/FG_NET\")\n",
    "\n",
    "# URL of the zipfile with the dataset\n",
    "GLOBALS['DATASET_URL'] = \"http://yanweifu.github.io/FG_NET_data/FGNET.zip\"\n",
    "\n",
    "# Dataset has images and metadata. Here we store the path to the img dir \n",
    "GLOBALS['IMAGE_DIR_PATH'] = os.path.join(GLOBALS['DATA_PATH'], \"FGNET/images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "btdP3TCgJMDx",
   "metadata": {
    "id": "btdP3TCgJMDx"
   },
   "source": [
    "# Auth for Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rXPJXSQ0JNc2",
   "metadata": {
    "id": "rXPJXSQ0JNc2"
   },
   "outputs": [],
   "source": [
    "if GLOBALS['RUNNING_ENV'] == \"remote\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "VK-X452lIQ46",
   "metadata": {
    "id": "VK-X452lIQ46"
   },
   "source": [
    "# Dataset downloading "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14dcbcb0-8f63-43b6-b3cc-e71de37feb3d",
   "metadata": {
    "id": "14dcbcb0-8f63-43b6-b3cc-e71de37feb3d"
   },
   "outputs": [],
   "source": [
    "def download_dataset(path: str, url: str):\n",
    "    \n",
    "    # Create the dir if it does not exist\n",
    "    if os.path.exists(path) is False:\n",
    "        print(f\"Dir {path} does not exist, creating that dir\")\n",
    "        os.mkdir(path)\n",
    "        \n",
    "    # Download the dataset and extract it at that path\n",
    "    try:\n",
    "        req = requests.get(url)\n",
    "    except Exception as e:\n",
    "        print(f\"ERROR: could not download data from url\")\n",
    "        print(f\"ERROR: error is:\\n{e}\")\n",
    "        return\n",
    "        \n",
    "    \n",
    "    zip_file = zipfile.ZipFile(io.BytesIO(req.content))\n",
    "    zip_file.extractall(path)\n",
    "\n",
    "    print(\"Succesful download\")\n",
    "\n",
    "    \n",
    "download_dataset(\n",
    "    GLOBALS['DATA_PATH'],\n",
    "    GLOBALS['DATASET_URL'],\n",
    "    can_skip_download = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "176c2b77-54a9-45d4-9121-88d340f7aba3",
   "metadata": {
    "id": "176c2b77-54a9-45d4-9121-88d340f7aba3"
   },
   "outputs": [],
   "source": [
    "class FGDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self, path: str, transform = None):\n",
    "        self.path = path\n",
    "        self.transform = transform\n",
    "        self.labels = None\n",
    "        self.imgs = None\n",
    "        self.individuals = None\n",
    "        self.number_images: Union[int, None] = None\n",
    "        self.file_names: Union[List, None] = None\n",
    "\n",
    "        # Get the data from the dir\n",
    "        self.__generate_dataset()\n",
    "\n",
    "        super(FGDataset, self).__init__()\n",
    "\n",
    "    \n",
    "    def __len__(self) -> int:\n",
    "\n",
    "        # Check that we have the number of images of the dataset\n",
    "        if self.number_images is None:\n",
    "            raise Exception(\"Dataset is not initialized, thus, number of images is unknown\")\n",
    "        \n",
    "        return self.number_images\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist() \n",
    "\n",
    "        # Get the image from the index\n",
    "        img_name = os.path.join(self.path, self.file_names[idx])\n",
    "        image = torchvision.io.read_image(img_name)\n",
    "\n",
    "        # Get the id and age from the file_name\n",
    "        id, age = self.__id_and_age_from_file_name(self.file_names[idx])\n",
    "\n",
    "        # Put together all the info\n",
    "        sample = {\n",
    "            \"image\": image,\n",
    "            \"id\": id,\n",
    "            \"age\": age,\n",
    "        }\n",
    "\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "\n",
    "        return sample\n",
    "\n",
    "    def __generate_dataset(self):\n",
    "\n",
    "        # Get all the names of the files\n",
    "        self.file_names = os.listdir(self.path)\n",
    "\n",
    "        # Use that for computing the size\n",
    "        self.number_images = len(self.file_names)\n",
    "            \n",
    "        self.individuals = dict()\n",
    "\n",
    "        # Use the names to get the persons IDs and their ages\n",
    "        for file_name in self.file_names:\n",
    "\n",
    "            # Split into id and age\n",
    "            id, age = self.__id_and_age_from_file_name(file_name)\n",
    "\n",
    "            # Put the data into a dict\n",
    "            # If the individual has already an entry, append to their list\n",
    "            # Otherwise create an entry \n",
    "            if self.individuals.get(id) is not None:\n",
    "                self.individuals[id].append(age)\n",
    "                continue\n",
    "            \n",
    "            # Otherwise, create an entry for that individual\n",
    "            self.individuals[id] = [age]\n",
    "\n",
    "    def __id_and_age_from_file_name(self, file_name: str) -> Tuple[str, str]:\n",
    "        # Remove file extension\n",
    "        file_name_no_extension = file_name.split(\".JPG\")[0]\n",
    "\n",
    "        # Split into id and age\n",
    "        id, age = file_name_no_extension.split(\"A\")\n",
    "\n",
    "        return id, age\n",
    "\n",
    "dataset = FGDataset(path = GLOBALS['IMAGE_DIR_PATH'], transform = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "MQ6sifNvNhZE",
   "metadata": {
    "id": "MQ6sifNvNhZE"
   },
   "outputs": [],
   "source": [
    "sample = dataset[20]\n",
    "img = sample[\"image\"]\n",
    "age = sample[\"age\"]\n",
    "id = sample[\"id\"]\n",
    "\n",
    "print(f\"Id {id} at age {age}\")\n",
    "\n",
    "transform = T.ToPILImage()\n",
    "img = transform(img)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "n4irkIQWVvEl",
   "metadata": {
    "id": "n4irkIQWVvEl"
   },
   "outputs": [],
   "source": [
    "def plot_histogram(values: List[float], num_bins: int):\n",
    "\n",
    "    # Make plots bigger\n",
    "    plt.figure(figsize=(8, 6), dpi=80)\n",
    "\n",
    "    # Get the data needed for the plot \n",
    "    counts, bins = np.histogram(values, bins = num_bins)\n",
    "\n",
    "    # Plot the data\n",
    "    plt.hist(bins[:-1], bins, weights=counts)\n",
    "    plt.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5wpu8ZSe3W",
   "metadata": {
    "id": "6a5wpu8ZSe3W"
   },
   "outputs": [],
   "source": [
    "# Get a flat list with all the ages in the dataset\n",
    "ages = dataset.individuals.values()\n",
    "ages = list(itertools.chain(*ages))\n",
    "\n",
    "# Some persons have a few photos at the same age\n",
    "# In that case, ages are labeled with letters\n",
    "# For example, 13a, 13b, 13c\n",
    "# Get rid of that extra car\n",
    "def remove_label(age_str: str) -> str:\n",
    "    if len(age_str) > 2:\n",
    "        return age_str[0:2]\n",
    "    \n",
    "    return age_str\n",
    "\n",
    "ages = [int(remove_label(age)) for age in ages]\n",
    "\n",
    "# Now, plot the histogram of the ages distribution\n",
    "plot_histogram(ages, num_bins = 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "R2qDSe8cVQQE",
   "metadata": {
    "id": "R2qDSe8cVQQE"
   },
   "outputs": [],
   "source": [
    "imgs_per_user = [len(user_imgs) for user_imgs in dataset.individuals.values()]\n",
    "\n",
    "# Now, plot the distribution of the ages\n",
    "plot_histogram(imgs_per_user, num_bins = 15)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
