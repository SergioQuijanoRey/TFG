{"cells":[{"cell_type":"markdown","id":"SCUXXp1GrTvw","metadata":{"id":"SCUXXp1GrTvw"},"source":["# Benchmarking notebook\n","\n","- The purpose of this notebook is to run all the benchmarks in a notebook format\n","- The benchmarks can be runned in a terminal with `just benchmarks`\n","- But that method don't work well in Google Colab Notebooks\n","- So we do this adaptation to use Google Colab power"]},{"cell_type":"markdown","id":"58b07228","metadata":{"id":"58b07228"},"source":["\n","# Global Parameters of the Notebook"]},{"cell_type":"markdown","id":"f955a4a9-088e-43ea-820e-05846a99463f","metadata":{"id":"f955a4a9-088e-43ea-820e-05846a99463f"},"source":["## Paths\n","\n","- Parameters related to data / model / lib paths"]},{"cell_type":"code","execution_count":null,"id":"51aad6c9-1243-4de4-b799-e0a9ab8e7149","metadata":{"id":"51aad6c9-1243-4de4-b799-e0a9ab8e7149"},"outputs":[],"source":["# Lib to define paths\n","import os\n","\n","# Define if we are running the notebook in our computer (\"local\")\n","# or in Google Colab (\"remote\")\n","RUNNING_ENV = \"remote\"\n","\n","# Base path for the rest of paths defined in the notebook\n","BASE_PATH = \"./src\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n","\n","# Path to our lib dir\n","LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n","\n","# Path to the benchmarks dir\n","BENCHMARK_PATH = os.path.join(BASE_PATH, \"benchmarks\")\n","\n","# Path where we store training / test data\n","DATA_PATH = os.path.join(BASE_PATH, \"data\")\n","\n","# Dir with all cached models \n","# This cached models can be loaded from disk when training is skipped\n","MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")\n","\n","# Cache for the augmented dataset\n","AUGMENTED_DATASET_CACHE_FILE = os.path.join(BASE_PATH, \"cached_augmented_dataset.pt\")\n","\n","# File where the logs are written\n","LOGGING_FILE = os.path.join(BASE_PATH, \"training.log\")\n","\n","# Binary file where the stats of the profiling are saved\n","PROFILE_SAVE_FILE = os.path.join(BASE_PATH, \"training_profile.stat\")"]},{"cell_type":"markdown","id":"8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28","metadata":{"id":"8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28"},"source":["## ML parameters\n","\n","- Parameters related to machine learning\n","- For example, batch sizes, learning rates, ..."]},{"cell_type":"code","execution_count":null,"id":"021a8a47-2dbb-4ef7-8045-496ff228a4ab","metadata":{"id":"021a8a47-2dbb-4ef7-8045-496ff228a4ab"},"outputs":[],"source":["# Parameters of P-K sampling\n","P = 100   # Number of classes used in each minibatch\n","K = 2     # Number of images sampled for each selected class\n","\n","# Batch size for online training \n","# We can use `P * K` as batch size. Thus, minibatches will be\n","# as we expect in P-K sampling. \n","# \n","# But we can use `n * P * K`. Thus, minibatches will be n P-K sampling\n","# minibatche concatenated together\n","# Be careful when doing this because it can be really slow, and there is no\n","# clear reason to do this\n","ONLINE_BATCH_SIZE = P * K\n","\n","# Epochs for hard triplets, online training \n","TRAINING_EPOCHS = 1\n","\n","# Learning rate for hard triplets, online training\n","ONLINE_LEARNING_RATE = 0.01\n","\n","# How many single elements we want to see before logging \n","# It has to be a multiple of P * K, otherwise `should_log` would return always \n","# false as `it % LOGGING_ITERATIONS != 0` always\n","#\n","# `LOGGING_ITERATIONS = P * K * n` means we log after seeing `n` P-K sampled\n","# minibatches\n","LOGGING_ITERATIONS = P * K * 20\n","\n","# Which percentage of the training and validation set we want to use for the logging\n","ONLINE_LOGGER_TRAIN_PERCENTAGE = 1 / 5\n","ONLINE_LOGGER_VALIDATION_PERCENTAGE = 1 / 3\n","\n","# Choose which model we're going to use\n","# Can be \"ResNet18\", \"LightModel\" or \"LFWResNet18\"\n","NET_MODEL = \"LFWResNet18\"\n","\n","# Epochs used in k-Fold Cross validation \n","# k-Fold Cross validation used for parameter exploration\n","HYPERPARAMETER_TUNING_EPOCHS = 7\n","\n","# Number of folds used in k-fold Cross Validation\n","NUMBER_OF_FOLDS = 4\n","\n","# Margin used in the loss function\n","MARGIN = 1.0\n","\n","# Dim of the embedding calculated by the network\n","EMBEDDING_DIMENSION = 5\n","\n","# Number of neighbours considered in K-NN\n","# K-NN used for transforming embedding task to classification task \n","NUMBER_NEIGHBOURS = 3\n","\n","# Batch Triplet Loss Function\n","# This way we can choose among \"hard\", \"all\"\n","BATCH_TRIPLET_LOSS_FUNCTION = \"hard\"\n","\n","# Wether or not use softplus loss function instead of vanilla triplet loss\n","USE_SOFTPLUS_LOSS = False\n","\n","# Count all sumamnds in the mean loss or only those summands greater than zero\n","USE_GT_ZERO_MEAN_LOSS = True\n","\n","# Wether or not use lazy computations in the data augmentation\n","LAZY_DATA_AUGMENTATION = True"]},{"cell_type":"markdown","id":"_O0TCQMuc7-w","metadata":{"id":"_O0TCQMuc7-w"},"source":["## Section parameters"]},{"cell_type":"markdown","id":"066f9a63-cc85-4203-bd4e-29d49eb64339","metadata":{"id":"066f9a63-cc85-4203-bd4e-29d49eb64339"},"source":["- Flags to choose if some sections will run or not\n","- This way we can skip some heavy computations when not needed "]},{"cell_type":"code","execution_count":null,"id":"40ef107c-a812-4abb-97d2-8d375cd0e685","metadata":{"id":"40ef107c-a812-4abb-97d2-8d375cd0e685"},"outputs":[],"source":["# Skip hyper parameter tuning for online training\n","SKIP_HYPERPARAMTER_TUNING = True\n","\n","# Skip training and use a cached model\n","# Useful for testing the embedding -> classifier transformation\n","# Thus, when False training is not computed and a cached model\n","# is loaded from disk\n","# Cached models are stored in `MODEL_CACHE_FOLDER`\n","USE_CACHED_MODEL = False\n","\n","# Skip data augmentation and use the cached augmented dataset\n","USE_CACHED_AUGMENTED_DATASET = False\n","\n","# Most of the time we're not exploring the data, but doing\n","# either hyperparameter settings or training of the model\n","# So if we skip this step we can start the process faster\n","SKIP_EXPLORATORY_DATA_ANALYSYS = True\n","\n","# Wether or not profile the training \n","# This should be False most of the times\n","# Note that profiling adds a significant overhead to the training\n","PROFILE_TRAINING = False"]},{"cell_type":"markdown","id":"Dfnhm9Pt1OCf","metadata":{"id":"Dfnhm9Pt1OCf"},"source":["## WANDB Parameters"]},{"cell_type":"code","execution_count":null,"id":"nl1eCSLX1P3p","metadata":{"id":"nl1eCSLX1P3p"},"outputs":[],"source":["from datetime import datetime\n","\n","# Name for the project\n","# One project groups different runs\n","WANDB_PROJECT_NAME = \"Benchmarking\"\n","\n","# Name for this concrete run \n","# I don't care too much about it, because wandb tracks the parameters we use \n","# in this run (see \"Configuration for Weights and Biases\" section)\n","WANDB_RUN_NAME = str(datetime.now())"]},{"cell_type":"markdown","id":"f109c94c-8595-49d2-a328-9ef9233adf27","metadata":{"id":"f109c94c-8595-49d2-a328-9ef9233adf27"},"source":["## Others"]},{"cell_type":"code","execution_count":null,"id":"f735abd1","metadata":{"id":"f735abd1"},"outputs":[],"source":["# Number of workers we want to use \n","# We can have less, equal or greater num of workers than CPUs\n","# In the following forum:\n","#   https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4\n","# they recomend to explore this parameter, growing it until system RAM saturates\n","# Using a value greater than 2 makes pytorch tell us that this value is not optimal\n","# So sticking with what pytorch tells uss\n","NUM_WORKERS = 2\n","\n","# Fix random seed to make reproducible results\n","RANDOM_SEED = 123456789"]},{"cell_type":"markdown","id":"01a1a0d0","metadata":{"id":"01a1a0d0"},"source":["# Auth forGoogle Drive"]},{"cell_type":"code","execution_count":null,"id":"b1264a2a","metadata":{"id":"b1264a2a"},"outputs":[],"source":["if RUNNING_ENV == \"remote\":\n","    from google.colab import drive\n","    drive.mount('/content/drive')"]},{"cell_type":"markdown","id":"bblM9skNz8_J","metadata":{"id":"bblM9skNz8_J"},"source":["# Pre-installations\n","\n","- Some packages are not installed in the Colab Enviroment\n","- So install them if we're running in Colab"]},{"cell_type":"code","execution_count":null,"id":"69yRzLZaz8t7","metadata":{"id":"69yRzLZaz8t7"},"outputs":[],"source":["if RUNNING_ENV == \"remote\":\n","    !pip install wandb"]},{"cell_type":"markdown","id":"d1327dec","metadata":{"id":"d1327dec"},"source":["# Importing the modules we are going to use"]},{"cell_type":"code","execution_count":null,"id":"fc75086e","metadata":{"id":"fc75086e"},"outputs":[],"source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","\n","import torchvision\n","import torchvision.datasets as datasets\n","\n","# For using pre-trained ResNets\n","import torchvision.models as models \n","import torchvision.transforms as transforms\n","\n","from torch.utils.data import Dataset, DataLoader\n","\n","import matplotlib.pyplot as plt\n","import numpy as np\n","import os\n","import logging\n","from datetime import datetime\n","from pprint import pprint\n","import gc\n","import functools\n","import math\n","import seaborn as sns\n","from collections import Counter\n","import time\n","import copy\n","import cProfile\n","\n","import wandb\n","\n","# All concrete pieces we're using form sklearn\n","from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n","from sklearn.model_selection import ShuffleSplit\n","\n","\n","from tqdm import tqdm\n","from typing import List\n","\n","# Load in the notebook all .py files that make our personal lib\n","# This way we keep notebook code as small as possible, and only pertinent \n","# to the concrete task that this notebook solves (generic and reusable code\n","# goes to personal lib)\n","# Also, \n","!mkdir -p ./src/lib/\n","!mkdir -p ./src/benchmarks/\n","!cp -r \"$LIB_PATH\"/* ./src/lib/\n","!cp -r \"$BENCHMARK_PATH\"/* ./src/benchmarks\n","\n","# Now that files are loaded, we can import pieces of code\n","import src.lib.core as core\n","import src.lib.trainers as trainers\n","import src.lib.board as board \n","import src.lib.filesystem as filesystem\n","import src.lib.metrics as metrics\n","import src.lib.loss_functions as loss_functions\n","import src.lib.embedding_to_classifier as embedding_to_classifier\n","import src.lib.sampler as sampler\n","import src.lib.utils as utils\n","import src.lib.data_augmentation as data_augmentation\n","\n","import src.benchmarks.benchmark_metrics as bb_metrics\n","import src.benchmarks.benchmark_loss_functions as bb_loss_functions\n","\n","from src.lib.trainers import train_model_offline, train_model_online\n","from src.lib.train_loggers import ClassificationLogger, SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger, CompoundLogger, IntraClusterLogger, InterClusterLogger\n","from src.lib.models import *\n","from src.lib.visualizations import *\n","from src.lib.models import ResNet18, LFWResNet18\n","from src.lib.loss_functions import MeanTripletBatchTripletLoss, BatchHardTripletLoss, BatchAllTripletLoss\n","from src.lib.embedding_to_classifier import EmbeddingToClassifier\n","from src.lib.sampler import CustomSampler\n","from src.lib.data_augmentation import AugmentatedDataset, LazyAugmentatedDataset"]},{"cell_type":"markdown","id":"KyePreRCLI2x","metadata":{"id":"KyePreRCLI2x"},"source":["# Configuration of the logger\n","\n","- Here we set the configuration for all logging done \n","- In lib, `logging.getLogger(\"MAIN_LOGGER\")` is used everywhere, so we get it, configure it once, and use that config everywhere"]},{"cell_type":"code","execution_count":null,"id":"tiO4YfykLIdi","metadata":{"id":"tiO4YfykLIdi"},"outputs":[],"source":["# Get the logger that is used everywhere\n","file_logger = logging.getLogger(\"MAIN_LOGGER\")\n","\n","# Configure it\n","file_logger.propagate = False # Avoid propagatint to upper logger, which logs to \n","                         # the console\n","file_logger.setLevel(logging.DEBUG)\n","formatter = logging.Formatter(\"%(asctime)s::%(levelname)s::%(funcName)s::> %(message)s\")\n","file_handler = logging.FileHandler(LOGGING_FILE)\n","file_handler.setFormatter(formatter)\n","file_logger.addHandler(file_handler)\n","\n","# 'application' code\n","file_logger.debug('debug message')\n"]},{"cell_type":"markdown","id":"ePE7z9rVzCza","metadata":{"id":"ePE7z9rVzCza"},"source":["# Configuration for Weigths and Biases\n","\n","- We're going to use `wandb` for tracking the training of the models\n","- In this section, we configure `wandb`, mainly selecting which parameters of the notebook are we going to track"]},{"cell_type":"code","execution_count":null,"id":"kUP3capL7BhL","metadata":{"id":"kUP3capL7BhL"},"outputs":[],"source":["# Select which parameters of the notebook we're going to track in wand\n","# This has to be done before `wandb.init()` in order to pass this dict to \n","# `wandb.init`\n","# \n","# I could create a config dict in \"Global Parameters of the Notebook\" and pass it\n","# rightaway. Or use directly wandb.config.SOMETHING everywhere. We don't do this \n","# because of the following reasons:\n","# \n","# 1. We don't want to track all parameters (ie. section parameters, dir paths...)\n","# 2. At this moment, we're not 100% sure that wandb is the right tool, so we are\n","#    looking for loose coupling\n","\n","wandb_config_dict = {}\n","\n","\n","wandb_config_dict[\"P\"] = P \n","wandb_config_dict[\"K\"] = K\n","wandb_config_dict[\"ONLINE_BATCH_SIZE\"] = ONLINE_BATCH_SIZE\n","wandb_config_dict[\"TRAINING_EPOCHS\"] = TRAINING_EPOCHS\n","wandb_config_dict[\"ONLINE_LEARNING_RATE\"] = ONLINE_LEARNING_RATE\n","wandb_config_dict[\"LOGGING_ITERATIONS\"] = LOGGING_ITERATIONS\n","wandb_config_dict[\"ONLINE_LOGGER_TRAIN_PERCENTAGE\"] = ONLINE_LOGGER_TRAIN_PERCENTAGE\n","wandb_config_dict[\"ONLINE_LOGGER_VALIDATION_PERCENTAGE\"] = ONLINE_LOGGER_VALIDATION_PERCENTAGE\n","wandb_config_dict[\"NET_MODEL\"] = NET_MODEL\n","wandb_config_dict[\"HYPERPARAMETER_TUNING_EPOCHS\"] = HYPERPARAMETER_TUNING_EPOCHS\n","wandb_config_dict[\"NUMBER_OF_FOLDS\"] = NUMBER_OF_FOLDS\n","wandb_config_dict[\"MARGIN\"] = MARGIN\n","wandb_config_dict[\"EMBEDDING_DIMENSION\"] = EMBEDDING_DIMENSION\n","wandb_config_dict[\"NUMBER_NEIGHBOURS\"] = NUMBER_NEIGHBOURS\n","wandb_config_dict[\"BATCH_TRIPLET_LOSS_FUNCTION\"] = BATCH_TRIPLET_LOSS_FUNCTION\n","wandb_config_dict[\"USE_SOFTPLUS_LOSS\"] = USE_SOFTPLUS_LOSS\n","wandb_config_dict[\"USE_GT_ZERO_MEAN_LOSS\"] = USE_GT_ZERO_MEAN_LOSS\n","wandb_config_dict[\"PROFILE_TRAINING\"] = PROFILE_TRAINING"]},{"cell_type":"code","execution_count":null,"id":"HjQOqYORzK28","metadata":{"id":"HjQOqYORzK28"},"outputs":[],"source":["# Init the wandb tracker\n","# We need to do this before \n","wandb.init(\n","    project = WANDB_PROJECT_NAME, \n","    name = WANDB_RUN_NAME, \n","    config = wandb_config_dict\n",")"]},{"cell_type":"code","execution_count":null,"id":"yb9CtFO17BaT","metadata":{"id":"yb9CtFO17BaT"},"outputs":[],"source":["# Set env variable to allow wandb to save the code of the notebook\n","%env WANDB_NOTEBOOK_NAME=WANDB_RUN_NAME"]},{"cell_type":"markdown","id":"a1e330fb","metadata":{"id":"a1e330fb"},"source":["# Functions that we are going to use"]},{"cell_type":"code","execution_count":null,"id":"8eb846cd","metadata":{"id":"8eb846cd"},"outputs":[],"source":["def show_learning_curve(training_history: dict):\n","    # Take two learning curves\n","    loss = training_history['loss']\n","    val_loss = training_history['val_loss']\n","\n","    # Move the lists to cpu, because that's what matplotlib needs\n","    loss = [loss_el.cpu() for loss_el in loss]\n","    val_loss = [val_loss_el.cpu() for val_loss_el in val_loss]\n","    \n","    # Show graphics\n","    plt.plot(loss)\n","    plt.plot(val_loss)\n","    plt.legend(['Training loss', 'Validation loss'])\n","    plt.show()\n","    \n","def try_to_clean_memory(): \n","    torch.cuda.empty_cache() \n","    gc.collect()"]},{"cell_type":"markdown","id":"0f2b68f5-753f-4677-8127-3b241b3a654d","metadata":{"id":"0f2b68f5-753f-4677-8127-3b241b3a654d"},"source":["# Running the benchmarks"]},{"cell_type":"code","execution_count":null,"id":"4e75f7db-a6f1-4293-8eaa-eda75858bb04","metadata":{"id":"4e75f7db-a6f1-4293-8eaa-eda75858bb04"},"outputs":[],"source":["bb_metrics.main()"]},{"cell_type":"code","source":["bb_loss_functions.main()"],"metadata":{"id":"PI3xkj_pSyGV"},"id":"PI3xkj_pSyGV","execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":[],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.7"}},"nbformat":4,"nbformat_minor":5}