{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b07228",
   "metadata": {
    "id": "58b07228"
   },
   "source": [
    "\n",
    "# Global Parameters of the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955a4a9-088e-43ea-820e-05846a99463f",
   "metadata": {},
   "source": [
    "## Paths\n",
    "\n",
    "- Parameters related to data / model / lib paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aad6c9-1243-4de4-b799-e0a9ab8e7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib to define paths\n",
    "import os\n",
    "\n",
    "# Define if we are running the notebook in our computer (\"local\")\n",
    "# or in Google Colab (\"remote\")\n",
    "RUNNING_ENV = \"local\"\n",
    "\n",
    "# Base path for the rest of paths defined in the notebook\n",
    "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "# Path to our lib dir\n",
    "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
    "\n",
    "# Path where we store training / test data\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
    "\n",
    "# Dir with all cached models \n",
    "# This cached models can be loaded from disk when training is skipped\n",
    "MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28",
   "metadata": {},
   "source": [
    "## ML parameters\n",
    "\n",
    "- Parameters related to machine learning\n",
    "- For example, batch sizes, learning rates, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be30e-79ea-46e7-9562-561a38c5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch size for offline training\n",
    "DATALOADER_BATCH_SIZE = 32\n",
    "\n",
    "# Batch size for online training\n",
    "ONLINE_BATCH_SIZE = 2 ** 9\n",
    "\n",
    "# Size of random triplets sets\n",
    "# It has to be a multiple of `DATALOADER_BATCH_SIZE`, so the network\n",
    "# has batches of proper size\n",
    "RANDOM_TRIPLETS_DATA_SIZE = DATALOADER_BATCH_SIZE * 15\n",
    "\n",
    "# Training epochs for the random triplets model\n",
    "TRAINING_EPOCHS_RANDOM = 10\n",
    "\n",
    "# Learning rate for random triplets model\n",
    "RANDOM_TRIPLET_LEARNING_RATE = 0.001\n",
    " \n",
    "# Epochs for hard triplets, online training \n",
    "TRAINING_EPOCHS = 3 \n",
    "\n",
    "# Epochs used in k-Fold Cross validation \n",
    "# k-Fold Cross validation used for parameter exploration\n",
    "HYPERPARAMETER_TUNING_EPOCHS = 7\n",
    "\n",
    "# Number of folds used in k-fold Cross Validation\n",
    "NUMBER_OF_FOLDS = 4\n",
    "\n",
    "# Margin used in the loss function\n",
    "MARGIN = 0.001\n",
    "\n",
    "# Dim of the embedding calculated by the network\n",
    "EMBEDDING_DIMENSION = 2\n",
    "\n",
    "# Learning rate para el entrenamiento con triples dificiles online\n",
    "# Learning rate for hard triplets, online training\n",
    "ONLINE_LEARNING_RATE = 0.01\n",
    "\n",
    "# Number of neighbours considered in K-NN\n",
    "# K-NN used for transforming embedding task to classification task \n",
    "NUMBER_NEIGHBOURS = 3\n",
    "\n",
    "# Batch Triplet Loss Function\n",
    "# This way we can choose among \"hard\", \"all\"\n",
    "BATCH_TRIPLET_LOSS_FUNCTION = \"hard\"\n",
    "\n",
    "# Wether or not use softplus loss function instead of vanilla triplet loss\n",
    "USE_SOFTPLUS_LOSS = False\n",
    "\n",
    "# Count all sumamnds in the mean loss or only those summands greater than zero\n",
    "USE_GT_ZERO_MEAN_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f9a63-cc85-4203-bd4e-29d49eb64339",
   "metadata": {},
   "source": [
    "## Section parameters\n",
    "\n",
    "- Flags to choose if some sections will run or not\n",
    "- This way we can skip some heavy computations when not needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef107c-a812-4abb-97d2-8d375cd0e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the training using random triplets \n",
    "SKIP_RANDOM_TRIPLETS_TRAINING = True\n",
    "\n",
    "# Skip hyper parameter tuning for online training\n",
    "SKIP_HYPERPARAMTER_TUNING = True\n",
    "\n",
    "# Skip training and use a cached model\n",
    "# Useful for testing the embedding -> classifier transformation\n",
    "# Thus, when False training is not computed and a cached model\n",
    "# is loaded from disk\n",
    "# Cached models are stored in `MODEL_CACHE_FOLDER`\n",
    "USE_CACHED_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109c94c-8595-49d2-a328-9ef9233adf27",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735abd1",
   "metadata": {
    "id": "f735abd1"
   },
   "outputs": [],
   "source": [
    "# Number of processes we want to use \n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Fix random seed to make reproducible results\n",
    "RANDOM_SEED = 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1a0d0",
   "metadata": {
    "id": "01a1a0d0"
   },
   "source": [
    "# Auth forGoogle Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1264a2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1264a2a",
    "outputId": "3329b00b-4e45-4d14-9599-66124e4ca45c"
   },
   "outputs": [],
   "source": [
    "if RUNNING_ENV == \"remote\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1327dec",
   "metadata": {
    "id": "d1327dec"
   },
   "source": [
    "# Importing the modules we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75086e",
   "metadata": {
    "id": "fc75086e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# For using pre-trained ResNets\n",
    "import torchvision.models as models \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import functools\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# All concrete pieces we're using form sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# Load in the notebook all .py files that make our personal lib\n",
    "# This way we keep notebook code as small as possible, and only pertinent \n",
    "# to the concrete task that this notebook solves (generic and reusable code\n",
    "# goes to personal lib)\n",
    "!cp -r \"$LIB_PATH\"/* .\n",
    "\n",
    "# Now that files are loaded, we can import pieces of code\n",
    "import core\n",
    "import time\n",
    "import copy\n",
    "import board\n",
    "import filesystem\n",
    "import metrics\n",
    "import loss_functions\n",
    "import embedding_to_classifier\n",
    "import sampler\n",
    "from train_loggers import ClassificationLogger, SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger\n",
    "from models import *\n",
    "from visualizations import *\n",
    "from tqdm.notebook import tqdm\n",
    "from core import train_model_offline, train_model_online\n",
    "from models import ResNet18\n",
    "from loss_functions import MeanTripletBatchTripletLoss, BatchHardTripletLoss, BatchAllTripletLoss\n",
    "from embedding_to_classifier import EmbeddingToClassifier\n",
    "from sampler import CustomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e330fb",
   "metadata": {
    "id": "a1e330fb"
   },
   "source": [
    "# Functions that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb846cd",
   "metadata": {
    "id": "8eb846cd"
   },
   "outputs": [],
   "source": [
    "def show_learning_curve(training_history: dict):\n",
    "    # Take two learning curves\n",
    "    loss = training_history['loss']\n",
    "    val_loss = training_history['val_loss']\n",
    "\n",
    "    # Move the lists to cpu, because that's what matplotlib needs\n",
    "    loss = [loss_el.cpu() for loss_el in loss]\n",
    "    val_loss = [val_loss_el.cpu() for val_loss_el in val_loss]\n",
    "    \n",
    "    # Show graphics\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.show()\n",
    "    \n",
    "def try_to_clean_memory(): \n",
    "    torch.cuda.empty_cache() \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDEKdrG0oNZ_",
   "metadata": {
    "id": "wDEKdrG0oNZ_"
   },
   "source": [
    "# Random Triplets \n",
    "\n",
    "- In this section, we are going to work with random triplets\n",
    "- If minibatch size is $3\\times n$, then:\n",
    "    - Select a random element. For that element, select a random anchor and a random negative\n",
    "    - Add this triple to the minibatch\n",
    "    - Repeat the process $n$ times\n",
    "- We use this model (determined heavily by the sampling method) as a baseline for more sophisticated models (determined heavily by more sophisticated sampling methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1847bb",
   "metadata": {
    "id": "2c1847bb"
   },
   "source": [
    "## Dataset loading\n",
    "\n",
    "- Load training / test data\n",
    "- Split training in train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a184c",
   "metadata": {
    "id": "140a184c"
   },
   "outputs": [],
   "source": [
    "# Transformations that we want to apply when loading the data\n",
    "# Now we are only transforming images to tensors (pythorch only works with tensors)\n",
    "# But we can apply here some normalizations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # TODO -- apply some normalizations here\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# torchvision has a method to download and load the dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Train -> train / validation split\n",
    "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
    "\n",
    "# Data loaders to access the datasets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = DATALOADER_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = DATALOADER_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = DATALOADER_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44717f5",
   "metadata": {
    "id": "e44717f5"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88baf997",
   "metadata": {
    "id": "88baf997"
   },
   "source": [
    "Show some images with their classes, to verify that data loading was properly done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc478f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ebc478f5",
    "outputId": "b053e456-7b2e-4e59-f121-79b1ef4cfd2b"
   },
   "outputs": [],
   "source": [
    "imgs_to_show = 5\n",
    "\n",
    "for _ in range(imgs_to_show):\n",
    "\n",
    "    # Cargamos un batch de imagenes\n",
    "    images, images_classes = next(iter(train_loader))\n",
    "\n",
    "    # Nos quedamos con la primera imagen del batch\n",
    "    img, img_class = images[0], images_classes[0]\n",
    "\n",
    "    # Mostramos alguna informacion de la imagen\n",
    "    print(f\"Img class is: {img_class}\")\n",
    "\n",
    "    # Re-escalamos y mostramos la imagen\n",
    "    img = img.reshape((28, 28))\n",
    "    show_img(img, color_format_range = (-1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc5d89",
   "metadata": {
    "id": "dfcc5d89"
   },
   "source": [
    "Show the sizes of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a35e9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6a35e9b",
    "outputId": "61769ff2-927b-45c0-acab-dc21dabb329a"
   },
   "outputs": [],
   "source": [
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset: {len(validation_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5029063",
   "metadata": {
    "id": "c5029063"
   },
   "source": [
    "## Random triplets generation\n",
    "\n",
    "- For training the siamese network, we need triplets for computing *triplet loss*\n",
    "- One simple way of generating triplets is randomly creating triplets (as explained before) in an offline fashion\n",
    "    - Created a dataset of random triplets beforehand\n",
    "    - With fixed elements \n",
    "- Later, we are going to generate triplets in an online fashion with more sophisticated methods\n",
    "- In this case, we create a *pytorch* `Dataset` class, following the [official documentation]](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f54f66",
   "metadata": {
    "id": "59f54f66"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "class RandomTriplets(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset en el que los elementos son triples obtenidos de forma aleatoria\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_data: Dataset, custom_len: int, transform = None):\n",
    "        self.base_data = base_data\n",
    "        self.custom_len = custom_len\n",
    "        self.transform = transform\n",
    "        self.random_sampler = RandomSampler(self.base_data, replacement=True, num_samples=1, generator=None)\n",
    "        \n",
    "        # Por motivos de eficiencia, pre-computamos una lista de listas, de forma\n",
    "        # que tengamos disponibles las listas con las posiciones de cada clase por\n",
    "        # separado.\n",
    "        self.posiciones_clases = self.__precompute_list_of_classes()\n",
    "\n",
    "    def __len__(self):\n",
    "        \"\"\"\n",
    "        Devolvemos el tamaño del dataset\n",
    "        Como estamos generando triples aleatorios, devolvemos el tamaño definido\n",
    "        por parametro\n",
    "        \"\"\"\n",
    "        return self.custom_len\n",
    "\n",
    "    def __getitem__(self, idx) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Funcion que es llamada cuando se hace dataset[idx]\n",
    "        En vez de devolver una imagen (que es lo comun en esta clase dataset), \n",
    "        devolvemos un triple (anchor, positive, negative) aleatorio\n",
    "        \"\"\"\n",
    "\n",
    "        # Hacemos esto por temas de eficiencia\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Tomamos una imagen aleatoria que sera el ancla\n",
    "        anchor, anchor_class = self.base_data[next(iter(self.random_sampler))]\n",
    "\n",
    "        # Tomamos una imagen de la misma clase, que sera la positiva, de forma aleatoria\n",
    "        random_index = np.random.choice(self.posiciones_clases[anchor_class])\n",
    "        positive, positive_class = self.base_data[random_index]\n",
    "\n",
    "        # Tomamos una imagen de otra clase, que sera la negativa\n",
    "        # Empiezo tomando una clase que no sea la del anchor\n",
    "        posible_classes = list(range(10))\n",
    "        posible_classes.remove(anchor_class)\n",
    "        negative_class = np.random.choice(posible_classes)\n",
    "\n",
    "        # Ahora tomamos un indice aleatorio de esta clase negativa\n",
    "        random_index = np.random.choice(self.posiciones_clases[negative_class])\n",
    "        negative, negative_class = self.base_data[random_index]\n",
    "        \n",
    "        # Generamos ahora el triple\n",
    "        triplet = [anchor, positive, negative]\n",
    "\n",
    "        # Aplicamos la transformacion dada al dataset al ejemplo que devolvemos\n",
    "        if self.transform:\n",
    "            triplet = [self.transform(np.array(img)) for img in triplet]\n",
    "\n",
    "        return triplet\n",
    "\n",
    "    def __precompute_list_of_classes(self) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Calcula la lista con las listas de posiciones de cada clase por separado\n",
    "        \"\"\"\n",
    "        # Inicializamos la lista de listas\n",
    "        posiciones_clases = [[] for _ in range(10)]\n",
    "\n",
    "        # Recorremos el dataset y colocamos los indices donde corresponde\n",
    "        for idx, element in enumerate(self.base_data):\n",
    "            _, img_class = element\n",
    "            posiciones_clases[img_class].append(idx)\n",
    "\n",
    "        return posiciones_clases\n",
    "\n",
    "    \n",
    "class CustomReshape(object):\n",
    "    \"\"\"Pasamos la imagen de (28, 1, 28) a (28, 28)\"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = image.reshape(28, 28)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cbf68",
   "metadata": {
    "id": "2b4cbf68"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "   \n",
    "    # Antes de modificar la base de datos para convertirla a triples\n",
    "    # la guardamos, porque mas adelante nos hara falta\n",
    "    old_train_dataset = train_dataset\n",
    "    old_test_dataset = test_dataset\n",
    "\n",
    "    # Necesitamos hacer reshape de las imagenes para que\n",
    "    # sean (28, 28) y no (28, 1, 28)\n",
    "    transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "\n",
    "        # Hacemos reshape de las imagenes para\n",
    "        # que sean tensores (28, 28)\n",
    "        CustomReshape(),\n",
    "    ])\n",
    "\n",
    "    # Generamos los triples aleatorios para training\n",
    "    random_triplets_train = RandomTriplets(\n",
    "        base_data = train_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_train,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Generamos los triples aleatorios para validacion\n",
    "    random_triplets_validation = RandomTriplets(\n",
    "        base_data = validation_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_validation,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Generamos los triples aleatorios para test\n",
    "    random_triplets_test = RandomTriplets(\n",
    "        base_data = test_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_test,\n",
    "        batch_size = DATALOADER_BACH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Visualizamos algunos triples aleatorios para comprobar el funcionamiento\n",
    "    custom_triplet = random_triplets_train[2]\n",
    "    for i in custom_triplet :\n",
    "        show_img(i, color_format_range = (-1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0c7d0",
   "metadata": {
    "id": "58b0c7d0"
   },
   "source": [
    "## Entrenamiento del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ed64d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "569ed64d",
    "outputId": "367dc6bd-29ea-496a-9baf-7d878423c030"
   },
   "outputs": [],
   "source": [
    "net = LightModel(EMBEDDING_DIMENSION)\n",
    "\n",
    "# TODO -- fijar bien los parametros\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS_RANDOM\n",
    "parameters[\"lr\"] = RANDOM_TRIPLET_LEARNING_RATE\n",
    "parameters[\"criterion\"] = MeanTripletBatchTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS)\n",
    "\n",
    "# Definimos el logger que queremos para el entrenamiento\n",
    "logger = TripletLoggerOffline(\n",
    "    net = net,\n",
    "    iterations = 10 * DATALOADER_BATCH_SIZE,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc94a2d",
   "metadata": {
    "id": "5bc94a2d"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    training_history = train_model_offline(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetwork\",\n",
    "        logger = logger,\n",
    "        snapshot_iterations = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312e365",
   "metadata": {
    "id": "f312e365"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedfac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cedfac4",
    "outputId": "1ed77730-683b-40fd-f627-d034abbb2145"
   },
   "outputs": [],
   "source": [
    "# A partir de este punto ya no volvemos a entrenar el modelo\n",
    "# Asi que lo ponemos en modo evaluacion para que no lleve \n",
    "# la cuenta de los gradientes\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2783",
   "metadata": {
    "id": "967f2783"
   },
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "- Mostramos algunas métricas fundamentales sobre el conjunto de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60be0c4",
   "metadata": {
    "id": "c60be0c4"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
    "    with torch.no_grad(): core.test_model(net, test_loader, parameters[\"criterion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oOPlWZWYu7cR",
   "metadata": {
    "id": "oOPlWZWYu7cR"
   },
   "source": [
    "We use the adapter to classification task to have more metrics about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f14b37",
   "metadata": {
    "id": "29f14b37",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
    "        # preparado para descargar\n",
    "        train_dataset = old_train_dataset\n",
    "        test_dataset = old_test_dataset\n",
    "    \n",
    "        # Data loaders para acceder a los datos\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = DATALOADER_BACH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = DATALOADER_BACH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "    \n",
    "    \n",
    "        classifier = EmbeddingToClassifier(\n",
    "            net, \n",
    "            k = NUMBER_NEIGHBOURS, \n",
    "            data_loader = train_loader,\n",
    "            embedding_dimension = EMBEDDING_DIMENSION\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61cab8",
   "metadata": {
    "id": "ae61cab8"
   },
   "source": [
    "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b77a7a",
   "metadata": {
    "id": "f2b77a7a"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Hacemos esto y no `in test_dataset[:max_iterations]`\n",
    "        # para no tener que tomar todo el dataset y quedarnos con\n",
    "        # una parte de el, que es un proceso mucho mas lento que usar\n",
    "        # el iterador que da `in test_dataset` y parar con el contador\n",
    "        counter = 0\n",
    "        max_iterations = 20\n",
    "    \n",
    "        for img, img_class in test_dataset:\n",
    "            predicted_class = classifier.predict(img)\n",
    "            print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
    "    \n",
    "            # Actualizamos el contador\n",
    "            counter += 1\n",
    "            if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4def6fb",
   "metadata": {
    "id": "d4def6fb"
   },
   "source": [
    "## Plot del embedding\n",
    "\n",
    "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
    "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c93584",
   "metadata": {
    "id": "47c93584"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
    "    with torch.no_grad(): classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcedc5e",
   "metadata": {
    "id": "bfcedc5e"
   },
   "source": [
    "## Evaluación del clasificador obtenido\n",
    "\n",
    "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bcfa1",
   "metadata": {
    "id": "ef6bcfa1"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_roc_auc(true_labels_prob: np.array, predicted_labels_prob: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el area bajo la curva ROC, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \n",
    "    Las listas de etiquetas deben ser etiquetas probabilisticas\n",
    "    \"\"\"\n",
    "    return roc_auc_score(true_labels_prob, predicted_labels_prob, multi_class = \"ovo\")\n",
    "    \n",
    "def calculate_accuracy(true_labels: np.array, predicted_labels: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el accuracy, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \"\"\"\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "def calculate_silhouette(x, y):\n",
    "    \"\"\"Calcula el indice de silhouette para el embedding calculado por el modelo\"\"\"\n",
    "    return silhouette_score(x, y)\n",
    "\n",
    "def evaluate_model(model, train_loader, test_loader) -> dict:\n",
    "    \"\"\"\n",
    "    Evalua, usando distintas metricas, el modelo que hemos entrenado\n",
    "    Tambien evaluamos el embedding obtenido, no solo el clasificador\n",
    "    \n",
    "    Devuelve un diccionario con todas las metricas calculadas con el modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario en el que vamos a almacenar todas las metricas\n",
    "    metrics = dict()\n",
    "    \n",
    "    # Tomamos los arrays en formato adecuado para calcular ciertas metricas\n",
    "    x_train, y_train = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
    "    x_test, y_test = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = test_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
    "    \n",
    "    # Empezamos usando el modelo para realizar las predicciones+\n",
    "    # Usamos predicciones probabilisticas pues estas son fundamentales para\n",
    "    # la metrica roc auc\n",
    "    train_predicted_labels_prob = model.predict_proba_using_embedding(x_train)\n",
    "    test_predicted_labels_prob = model.predict_proba_using_embedding(x_test)\n",
    "\n",
    "    # Tomamos ahora las etiqeutas sin probabilidad\n",
    "    train_predicted_labels= model.predict_using_embedding(x_train)\n",
    "    test_predicted_labels = model.predict_using_embedding(x_test)\n",
    "    \n",
    "    # Tomamos las metricas de accuracy\n",
    "    metrics[\"train_acc\"] = calculate_accuracy(y_train, train_predicted_labels)\n",
    "    metrics[\"test_acc\"] = calculate_accuracy(y_test, test_predicted_labels)\n",
    "    \n",
    "    # Tomamos las areas bajo la curva ROC\n",
    "    metrics[\"train_roc_auc\"] = calculate_roc_auc(y_train, train_predicted_labels_prob)\n",
    "    metrics[\"test_roc_auc\"] = calculate_roc_auc(y_test, test_predicted_labels_prob)\n",
    "    \n",
    "    # Tomamos el indice de silhouette\n",
    "    metrics[\"train_silhouette\"] = calculate_silhouette(x_train, y_train)\n",
    "    metrics[\"test_silhouette\"] = calculate_silhouette(x_test, y_test)\n",
    "    \n",
    "    #Calculamos Matriz de confusion\n",
    "    metrics[\"train_confusion_matrix\"]=confusion_matrix(y_train, train_predicted_labels)\n",
    "    metrics[\"test_confusion_matrix\"]=confusion_matrix(y_test, test_predicted_labels)\n",
    "    \n",
    "    # Devolvemos las metricas en formato diccionario, que nos va a ser comodo para\n",
    "    # pasarlas a tablas y para mostrar muchas metricas simultaneamente\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(train_confusion_matrix,test_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Mostramos graficamente (mapa de calor) la matriz de confusion\n",
    "    \n",
    "    Podriamos mostrar directamente los valores de la matriz, pero al estar\n",
    "    trabajando con 25 clases es mas dificil de interpretar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mostramos graficamente las dos matrices de confusion\n",
    "    color_palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    print(\"Matriz de confusion en TRAIN\")\n",
    "    sns.heatmap(train_confusion_matrix, annot = False, cmap = color_palette, fmt=\"d\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Matriz de confusion en TEST\")\n",
    "    sns.heatmap(test_confusion_matrix, annot = False, cmap = color_palette)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc2fcb",
   "metadata": {
    "id": "f7dc2fcb"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos ejecutar esta seccion o no \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        classifier.embedder.set_permute(False)\n",
    "    \n",
    "        metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "        pprint(metrics)\n",
    "    \n",
    "        classifier.embedder.set_permute(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXxDVsblof-O",
   "metadata": {
    "id": "OXxDVsblof-O"
   },
   "source": [
    "# Online Batch Triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IDZBu1P-o4TM",
   "metadata": {
    "id": "IDZBu1P-o4TM"
   },
   "source": [
    "## Choose the loss function to use \n",
    "\n",
    "- We have so many combinations for loss functions that is not feasible to use one Colab section for each\n",
    "- Combinations depend on:\n",
    "    1. Batch hard vs Batch all \n",
    "    2. Classical triplet loss vs Softplus loss\n",
    "    3. All summands mean vs Only > 0 summands mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lU4wWC-GpHdI",
   "metadata": {
    "id": "lU4wWC-GpHdI"
   },
   "outputs": [],
   "source": [
    "batch_loss_function = None\n",
    "if BATCH_TRIPLET_LOSS_FUNCTION == \"hard\":\n",
    "    batch_loss_function = BatchHardTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean = USE_GT_ZERO_MEAN_LOSS) \n",
    "if BATCH_TRIPLET_LOSS_FUNCTION == \"all\":\n",
    "    batch_loss_function = BatchAllTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean =  USE_GT_ZERO_MEAN_LOSS) \n",
    "\n",
    "# Sanity check\n",
    "if batch_loss_function is None:\n",
    "    raise Exception(f\"BATCH_TRIPLET_LOSS global parameter got unexpected value: {BATCH_TRIPLET_LOSS_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378947c",
   "metadata": {
    "id": "7378947c"
   },
   "source": [
    "## Load again the data\n",
    "\n",
    "- With this online loss function we no longer need to compute triplets in an offline way\n",
    "- So we load the data again, so we have the original data without precomputed triplets \n",
    "- Also, use custom sampler to choose, for each minibatch, P classes and K elements for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425cd40",
   "metadata": {
    "id": "8425cd40"
   },
   "outputs": [],
   "source": [
    "# Transformaciones que queremos aplicar al cargar los datos\n",
    "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
    "])\n",
    "\n",
    "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
    "# preparado para descargar\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Separamos train en train y validacion\n",
    "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
    "\n",
    "# Dont work with subsets\n",
    "train_dataset, validation_dataset = train_dataset.dataset, validation_dataset.dataset\n",
    "\n",
    "# Data loaders para acceder a los datos\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    #shuffle = True,\n",
    "    num_workers = NUM_WORKERS, \n",
    "    pin_memory = True,\n",
    "    # TODO -- using magic numbers\n",
    "    sampler = CustomSampler(3, 16, train_dataset)\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    #shuffle = True,\n",
    "    num_workers = NUM_WORKERS, \n",
    "    pin_memory = True,\n",
    "    # TODO -- using magic numbers\n",
    "    # Maybe here I can use default sampler?\n",
    "    sampler = CustomSampler(3, 16, validation_dataset)\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size = ONLINE_BATCH_SIZE,\n",
    "  shuffle = True,\n",
    "  num_workers = NUM_WORKERS,\n",
    "  pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cPlf4HWo15go",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "cPlf4HWo15go",
    "outputId": "1c9e02ca-f330-4b81-ba28-3a786f7aa78b"
   },
   "outputs": [],
   "source": [
    "# TODO -- remove this check\n",
    "for element in train_loader:\n",
    "    print(f\"Element loaded: {element}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593607e",
   "metadata": {
    "id": "5593607e"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bd047",
   "metadata": {
    "id": "867bd047"
   },
   "outputs": [],
   "source": [
    "def custom_cross_validation(net, parameters, train_dataset, k):\n",
    "    \"\"\"Funcion propia para hacer k-fold cross validation de una red convolucional\"\"\"\n",
    "\n",
    "    # Definimos la forma en la que vamos a hacer el split de los folds\n",
    "    ss = ShuffleSplit(n_splits=k, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Lista en la que guardamos las perdidas encontradas en cada fold\n",
    "    losses = []\n",
    "\n",
    "    # Iteramos usando el split que nos da sklearn\n",
    "    for train_index, validation_index in ss.split(train_dataset):\n",
    "        \n",
    "        # Tenemos los indices de los elementos, asi que tomamos los dos datasets\n",
    "        # usando dichos indices\n",
    "        train_fold = [train_dataset[idx] for idx in train_index]\n",
    "        validation_fold = [train_dataset[idx] for idx in validation_index]\n",
    "\n",
    "        # Transformamos los datasets en dataloaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_fold,\n",
    "            batch_size = ONLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "        validation_loader = torch.utils.data.DataLoader(\n",
    "            validation_fold,\n",
    "            batch_size = ONLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "\n",
    "        # Entrenamos la red\n",
    "        _ = train_model_online(\n",
    "            net = net,\n",
    "            path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "            parameters = parameters,\n",
    "            train_loader = train_loader,\n",
    "            validation_loader = validation_loader,\n",
    "            name = \"SiameseNetworkOnline\",\n",
    "            logger = SilentLogger(),\n",
    "            snapshot_iterations = None\n",
    "        )\n",
    "\n",
    "        # Evaluamos la red en el fold de validacion\n",
    "        net.eval()\n",
    "        loss = metrics.calculate_mean_triplet_loss_online(net, validation_loader, parameters[\"criterion\"], 1.0)\n",
    "        loss = float(loss) # Pasamos el tensor de un unico elemento a un float simple\n",
    "\n",
    "        # Añadimos el loss a nuestra lista\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Devolvemos el array en formato numpy para que sea mas comodo trabajar con ella\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f5ea4",
   "metadata": {
    "id": "292f5ea4"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos realizar el hyperparameater tuning o no\n",
    "if SKIP_HYPERPARAMTER_TUNING is False:\n",
    "\n",
    "    # Para controlar que parametros ya hemos explorado y queremos saltar\n",
    "    already_explored_parameters = [\n",
    "        # Embedding dimension, learning rate, margin\n",
    "        (2, 0.0001, 0.01),\n",
    "        (2, 0.0001, 1),\n",
    "        (3, 0.0001),\n",
    "    ]\n",
    "\n",
    "    # Parametros que queremos mover\n",
    "    #margin_values = [0.01, 0.1, 1.0]\n",
    "    # TODO -- volver a poner todos los valores\n",
    "    margin_values = [1.0]\n",
    "    learning_rate_values = [0.0001, 0.001, 0.01]\n",
    "    embedding_dimension_values = [2, 3, 4]\n",
    "    \n",
    "    # Parametros que fijamos de antemano\n",
    "    epochs = HYPERPARAMETER_TUNING_EPOCHS\n",
    "    \n",
    "    # Llevamos la cuenta de los mejores parametros y el mejor error encontrados hasta\n",
    "    # el momento\n",
    "    best_loss = None\n",
    "    best_parameters = {\n",
    "        \"embedding_dimension\": None,\n",
    "        \"lr\": None,\n",
    "        \"margin\": None\n",
    "    }\n",
    "    \n",
    "    # Exploramos las combinaciones de parametros\n",
    "    for margin in margin_values:\n",
    "        for learning_rate in learning_rate_values:\n",
    "            for embedding_dimension in embedding_dimension_values:\n",
    "        \n",
    "                print(f\"Optimizando para margin: {margin}, lr: {learning_rate}, embedding_dim: {embedding_dimension}\")\n",
    "\n",
    "                # Comprobamos si tenemos que saltarnos el calculo de algun valor\n",
    "                # porque ya se haya hecho\n",
    "                if (embedding_dimension, learning_rate, margin) in already_explored_parameters:\n",
    "                    print(\"\\tSaltando este calculo porque ya se realizo\")\n",
    "                    continue\n",
    "                \n",
    "                # Definimos el modelo que queremos optimizar\n",
    "                net = ResNet18(embedding_dimension)\n",
    "                \n",
    "                # En este caso, al no estar trabajando con los minibatches\n",
    "                # (los usamos directamente como nos los da pytorch), no tenemos\n",
    "                # que manipular los tensores\n",
    "                net.set_permute(False)\n",
    "                \n",
    "                parameters = dict()\n",
    "                parameters[\"epochs\"] = epochs\n",
    "                parameters[\"lr\"] = learning_rate\n",
    "                parameters[\"criterion\"] = BatchHardTripletLoss(margin)\n",
    "                logger = SilentLogger()\n",
    "    \n",
    "                # Usamos nuestra propia funcion de cross validation para validar el modelo\n",
    "                losses = custom_cross_validation(net, parameters, train_dataset, k = NUMBER_OF_FOLDS) \n",
    "                print(f\"El loss conseguido es {losses.mean()}\")\n",
    "                print(\"\")\n",
    "            \n",
    "                # Comprobamos si hemos mejorado la funcion de perdida\n",
    "                # En cuyo caso, actualizamos nuestra estructura de datos y, sobre todo, mostramos\n",
    "                # por pantalla los nuevos mejores parametros\n",
    "                basic_condition = math.isnan(losses.mean()) is False             # Si es NaN no entramos al if\n",
    "                enter_condition = best_loss is None or losses.mean() < best_loss # Entramos al if si mejoramos la perdida\n",
    "                compound_condition = basic_condition and enter_condition\n",
    "                if compound_condition:\n",
    "                \n",
    "                    # Actualizamos nuestra estructura de datos\n",
    "                    best_loss = losses.mean()\n",
    "                    best_parameters = {\n",
    "                        \"embedding_dimension\": embedding_dimension,\n",
    "                        \"lr\": learning_rate,\n",
    "                        \"margin\": margin,\n",
    "                    }\n",
    "            \n",
    "                    # Mostramos el cambio encontrado\n",
    "                    print(\"==> ENCONTRADOS NUEVOS MEJORES PARAMETROS\")\n",
    "                    print(f\"Mejores parametros: {best_parameters}\")\n",
    "                    print(f\"Mejor loss: {best_loss}\")\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ee8ba",
   "metadata": {
    "id": "3e1ee8ba"
   },
   "source": [
    "## Entrenamiento online"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2afd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a2afd9",
    "outputId": "4a53bc0c-c978-4ea4-8fef-587b38158cd7"
   },
   "outputs": [],
   "source": [
    "net = LightModel(EMBEDDING_DIMENSION)\n",
    "\n",
    "# En este caso, al no estar trabajando con los minibatches\n",
    "# (los usamos directamente como nos los da pytorch), no tenemos\n",
    "# que manipular los tensores\n",
    "net.set_permute(False)\n",
    "\n",
    "# Parameters of the training\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
    "parameters[\"lr\"] = ONLINE_LEARNING_RATE\n",
    "\n",
    "# We use the loss function that depends on the global parameter BATCH_TRIPLET_LOSS_FUNCTION\n",
    "parameters[\"criterion\"] = batch_loss_function\n",
    "\n",
    "# Definimos el logger que queremos para el entrenamiento\n",
    "logger = TripletLoggerOnline(\n",
    "    net = net,\n",
    "    iterations = -1,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    "    train_percentage = 0.01,\n",
    "    validation_percentage = 1.0,\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa722a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "aa722a06",
    "outputId": "b1ad36c9-d052-4355-9c40-9b5f2642ebc3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Entrenamos solo si lo especifica\n",
    "# el parametro que controla el uso de cache\n",
    "if USE_CACHED_MODEL is False:\n",
    "\n",
    "    # Para saber cuanto tarda\n",
    "    ts = time.time()\n",
    "\n",
    "    torch.jit.script(\n",
    "    training_history = train_model_online(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetworkOnline\",\n",
    "        logger = logger,\n",
    "        snapshot_iterations = None\n",
    "    ))\n",
    "\n",
    "    # Calculamos cuanto ha tardado\n",
    "    te = time.time()\n",
    "    print(f\"Ha tardado {te - ts}\")\n",
    "    \n",
    "    # Actualizamos la cache del modelo\n",
    "    filesystem.save_model(net, MODEL_CACHE_FOLDER, \"online_model_cached\")\n",
    "    \n",
    "# Nos saltamos el entrenamiento y cargamos el modelo desde la cache\n",
    "else:\n",
    "       \n",
    "    net = filesystem.load_model(\n",
    "        os.path.join(MODEL_CACHE_FOLDER, \"online_model_cached\"), \n",
    "        lambda: ResNet18(EMBEDDING_DIMENSION)\n",
    "    )\n",
    "    \n",
    "    # Tenemos que cargar la red en la memoria correspondiente\n",
    "    device = core.get_device()\n",
    "    net.to(device)\n",
    "    \n",
    "# A partir de este punto no hacemos entrenamiento\n",
    "# asi que ponemos la red en modo evaluacion para que \n",
    "# no vaya almacenando los gradientes\n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae35995",
   "metadata": {
    "id": "3ae35995"
   },
   "outputs": [],
   "source": [
    "# Entrenamos solo si lo especifica\n",
    "# el parametro que controla el uso de cache\n",
    "if USE_CACHED_MODEL is False:\n",
    "        \n",
    "    # Al principio tenemos un pico de error que hace \n",
    "    # que la grafica no sea legible. Ignoramos las primeras metricas\n",
    "    # de error para que la grafica sea legible\n",
    "    cut = 5\n",
    "    training_history[\"loss\"] = training_history[\"loss\"][cut:]\n",
    "    training_history[\"val_loss\"] = training_history[\"val_loss\"][cut:]\n",
    "\n",
    "    show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b8fef",
   "metadata": {
    "id": "d68b8fef"
   },
   "source": [
    "## Evaluación del modelo\n",
    "\n",
    "- Mostramos algunas métricas fundamentales sobre el conjunto de"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210628c",
   "metadata": {
    "id": "5210628c"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.set_permute(False)\n",
    "    \n",
    "    core.test_model_online(net, test_loader, parameters[\"criterion\"], online = True)\n",
    "    \n",
    "    net.set_permute(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e58d6f",
   "metadata": {
    "id": "f2e58d6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try_to_clean_memory()\n",
    "    classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8ad35f7",
   "metadata": {
    "id": "e8ad35f7"
   },
   "source": [
    "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acde43",
   "metadata": {
    "id": "46acde43"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    # Hacemos esto y no `in test_dataset[:max_iterations]`\n",
    "    # para no tener que tomar todo el dataset y quedarnos con\n",
    "    # una parte de el, que es un proceso mucho mas lento que usar\n",
    "    # el iterador que da `in test_dataset` y parar con el contador\n",
    "    counter = 0\n",
    "    max_iterations = 20\n",
    "    \n",
    "    for img, img_class in test_dataset:\n",
    "        predicted_class = classifier.predict(img)\n",
    "        print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
    "    \n",
    "        # Actualizamos el contador\n",
    "        counter += 1\n",
    "        if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa44a0",
   "metadata": {
    "id": "58fa44a0"
   },
   "source": [
    "## Plot del embedding\n",
    "\n",
    "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
    "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab56fa9",
   "metadata": {
    "id": "8ab56fa9"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cea66d",
   "metadata": {
    "id": "83cea66d"
   },
   "source": [
    "## Evaluación del clasificador obtenido\n",
    "\n",
    "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4ae22",
   "metadata": {
    "id": "27b4ae22"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try_to_clean_memory()\n",
    "    classifier.embedder.set_permute(False)\n",
    "    \n",
    "    metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "    pprint(metrics)\n",
    "    \n",
    "    classifier.embedder.set_permute(True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wDEKdrG0oNZ_",
    "2c1847bb",
    "e44717f5",
    "c5029063",
    "a772b2e2",
    "58b0c7d0",
    "967f2783",
    "d4def6fb",
    "bfcedc5e",
    "d68b8fef",
    "58fa44a0",
    "83cea66d"
   ],
   "name": "Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
