{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "58b07228",
   "metadata": {
    "id": "58b07228"
   },
   "source": [
    "\n",
    "# Global Parameters of the Notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f955a4a9-088e-43ea-820e-05846a99463f",
   "metadata": {},
   "source": [
    "## Paths\n",
    "\n",
    "- Parameters related to data / model / lib paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51aad6c9-1243-4de4-b799-e0a9ab8e7149",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lib to define paths\n",
    "import os\n",
    "\n",
    "# Define if we are running the notebook in our computer (\"local\")\n",
    "# or in Google Colab (\"remote\")\n",
    "RUNNING_ENV = \"local\"\n",
    "\n",
    "# Base path for the rest of paths defined in the notebook\n",
    "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
    "\n",
    "# Path to our lib dir\n",
    "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
    "\n",
    "# Path where we store training / test data\n",
    "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
    "\n",
    "# Dir with all cached models \n",
    "# This cached models can be loaded from disk when training is skipped\n",
    "MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28",
   "metadata": {},
   "source": [
    "## ML parameters\n",
    "\n",
    "- Parameters related to machine learning\n",
    "- For example, batch sizes, learning rates, ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "191be30e-79ea-46e7-9562-561a38c5cac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO -- split this parameters in offline and online subsections\n",
    "\n",
    "# Batch size for offline training\n",
    "OFFLINE_BATCH_SIZE = 32\n",
    "\n",
    "# Parameters of P-K sampling\n",
    "P = 3   # Number of classes used in each minibatch\n",
    "K = 16  # Number of images sampled for each selected class\n",
    "\n",
    "# Batch size for online training \n",
    "# We can use `P * K` as batch size. Thus, minibatches will be\n",
    "# as we expect in P-K sampling. \n",
    "# \n",
    "# But we can use `n * P * K`. Thus, minibatches will be n P-K sampling\n",
    "# minibatche concatenated together\n",
    "# Be careful when doing this because it can be really slow, and there is no\n",
    "# clear reason to do this\n",
    "ONLINE_BATCH_SIZE = P * K\n",
    "\n",
    "# Size of random triplets sets\n",
    "# It has to be a multiple of `OFFLINE_BATCH_SIZE`, so the network\n",
    "# has batches of proper size\n",
    "RANDOM_TRIPLETS_DATA_SIZE = OFFLINE_BATCH_SIZE * 15\n",
    "\n",
    "# Training epochs for the random triplets model\n",
    "TRAINING_EPOCHS_RANDOM = 10\n",
    "\n",
    "# Learning rate for random triplets model\n",
    "RANDOM_TRIPLET_LEARNING_RATE = 0.001\n",
    " \n",
    "# Epochs for hard triplets, online training \n",
    "TRAINING_EPOCHS = 1\n",
    "\n",
    "# Epochs used in k-Fold Cross validation \n",
    "# k-Fold Cross validation used for parameter exploration\n",
    "HYPERPARAMETER_TUNING_EPOCHS = 7\n",
    "\n",
    "# Number of folds used in k-fold Cross Validation\n",
    "NUMBER_OF_FOLDS = 4\n",
    "\n",
    "# Margin used in the loss function\n",
    "MARGIN = 0.001\n",
    "\n",
    "# Dim of the embedding calculated by the network\n",
    "EMBEDDING_DIMENSION = 3\n",
    "\n",
    "# Learning rate for hard triplets, online training\n",
    "ONLINE_LEARNING_RATE = 0.01\n",
    "\n",
    "# Number of neighbours considered in K-NN\n",
    "# K-NN used for transforming embedding task to classification task \n",
    "NUMBER_NEIGHBOURS = 3\n",
    "\n",
    "# Batch Triplet Loss Function\n",
    "# This way we can choose among \"hard\", \"all\"\n",
    "BATCH_TRIPLET_LOSS_FUNCTION = \"all\"\n",
    "\n",
    "# Wether or not use softplus loss function instead of vanilla triplet loss\n",
    "USE_SOFTPLUS_LOSS = True\n",
    "\n",
    "# Count all sumamnds in the mean loss or only those summands greater than zero\n",
    "USE_GT_ZERO_MEAN_LOSS = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "066f9a63-cc85-4203-bd4e-29d49eb64339",
   "metadata": {},
   "source": [
    "## Section parameters\n",
    "\n",
    "- Flags to choose if some sections will run or not\n",
    "- This way we can skip some heavy computations when not needed "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40ef107c-a812-4abb-97d2-8d375cd0e685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Skip the training using random triplets \n",
    "SKIP_RANDOM_TRIPLETS_TRAINING = True\n",
    "\n",
    "# Skip hyper parameter tuning for online training\n",
    "SKIP_HYPERPARAMTER_TUNING = True\n",
    "\n",
    "# Skip training and use a cached model\n",
    "# Useful for testing the embedding -> classifier transformation\n",
    "# Thus, when False training is not computed and a cached model\n",
    "# is loaded from disk\n",
    "# Cached models are stored in `MODEL_CACHE_FOLDER`\n",
    "USE_CACHED_MODEL = False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f109c94c-8595-49d2-a328-9ef9233adf27",
   "metadata": {},
   "source": [
    "## Others"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f735abd1",
   "metadata": {
    "id": "f735abd1"
   },
   "outputs": [],
   "source": [
    "# Number of processes we want to use \n",
    "NUM_WORKERS = 2\n",
    "\n",
    "# Fix random seed to make reproducible results\n",
    "RANDOM_SEED = 123456789"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01a1a0d0",
   "metadata": {
    "id": "01a1a0d0"
   },
   "source": [
    "# Auth forGoogle Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1264a2a",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b1264a2a",
    "outputId": "3329b00b-4e45-4d14-9599-66124e4ca45c"
   },
   "outputs": [],
   "source": [
    "if RUNNING_ENV == \"remote\":\n",
    "    from google.colab import drive\n",
    "    drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1327dec",
   "metadata": {
    "id": "d1327dec"
   },
   "source": [
    "# Importing the modules we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc75086e",
   "metadata": {
    "id": "fc75086e"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "import torchvision\n",
    "import torchvision.datasets as datasets\n",
    "\n",
    "# For using pre-trained ResNets\n",
    "import torchvision.models as models \n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "from datetime import datetime\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import functools\n",
    "import math\n",
    "import seaborn as sns\n",
    "\n",
    "# All concrete pieces we're using form sklearn\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n",
    "from sklearn.model_selection import ShuffleSplit\n",
    "\n",
    "from typing import List\n",
    "\n",
    "# Load in the notebook all .py files that make our personal lib\n",
    "# This way we keep notebook code as small as possible, and only pertinent \n",
    "# to the concrete task that this notebook solves (generic and reusable code\n",
    "# goes to personal lib)\n",
    "!cp -r \"$LIB_PATH\"/* .\n",
    "\n",
    "# Now that files are loaded, we can import pieces of code\n",
    "import core\n",
    "import time\n",
    "import copy\n",
    "import board\n",
    "import filesystem\n",
    "import metrics\n",
    "import loss_functions\n",
    "import embedding_to_classifier\n",
    "import sampler\n",
    "from train_loggers import ClassificationLogger, SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger\n",
    "from models import *\n",
    "from visualizations import *\n",
    "from tqdm.notebook import tqdm\n",
    "from core import train_model_offline, train_model_online\n",
    "from models import ResNet18\n",
    "from loss_functions import MeanTripletBatchTripletLoss, BatchHardTripletLoss, BatchAllTripletLoss\n",
    "from embedding_to_classifier import EmbeddingToClassifier\n",
    "from sampler import CustomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e330fb",
   "metadata": {
    "id": "a1e330fb"
   },
   "source": [
    "# Functions that we are going to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8eb846cd",
   "metadata": {
    "id": "8eb846cd"
   },
   "outputs": [],
   "source": [
    "def show_learning_curve(training_history: dict):\n",
    "    # Take two learning curves\n",
    "    loss = training_history['loss']\n",
    "    val_loss = training_history['val_loss']\n",
    "\n",
    "    # Move the lists to cpu, because that's what matplotlib needs\n",
    "    loss = [loss_el.cpu() for loss_el in loss]\n",
    "    val_loss = [val_loss_el.cpu() for val_loss_el in val_loss]\n",
    "    \n",
    "    # Show graphics\n",
    "    plt.plot(loss)\n",
    "    plt.plot(val_loss)\n",
    "    plt.legend(['Training loss', 'Validation loss'])\n",
    "    plt.show()\n",
    "    \n",
    "def try_to_clean_memory(): \n",
    "    torch.cuda.empty_cache() \n",
    "    gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wDEKdrG0oNZ_",
   "metadata": {
    "id": "wDEKdrG0oNZ_"
   },
   "source": [
    "# Random Triplets \n",
    "\n",
    "- In this section, we are going to work with random triplets\n",
    "- If minibatch size is $3\\times n$, then:\n",
    "    - Select a random element. For that element, select a random anchor and a random negative\n",
    "    - Add this triple to the minibatch\n",
    "    - Repeat the process $n$ times\n",
    "- We use this model (determined heavily by the sampling method) as a baseline for more sophisticated models (determined heavily by more sophisticated sampling methods)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1847bb",
   "metadata": {
    "id": "2c1847bb"
   },
   "source": [
    "## Dataset loading\n",
    "\n",
    "- Load training / test data\n",
    "- Split training in train / validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "140a184c",
   "metadata": {
    "id": "140a184c"
   },
   "outputs": [],
   "source": [
    "# Transformations that we want to apply when loading the data\n",
    "# Now we are only transforming images to tensors (pythorch only works with tensors)\n",
    "# But we can apply here some normalizations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor()\n",
    "    # TODO -- apply some normalizations here\n",
    "])\n",
    "\n",
    "# Load the dataset\n",
    "# torchvision has a method to download and load the dataset\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = True,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root = DATA_PATH,\n",
    "    train = False,\n",
    "    download = True,\n",
    "    transform = transform,\n",
    ")\n",
    "\n",
    "# Train -> train / validation split\n",
    "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
    "\n",
    "# Data loaders to access the datasets\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = OFFLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = OFFLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size = OFFLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS,\n",
    "    pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44717f5",
   "metadata": {
    "id": "e44717f5"
   },
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88baf997",
   "metadata": {
    "id": "88baf997"
   },
   "source": [
    "Show some images with their classes, to verify that data loading was properly done:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc478f5",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 0
    },
    "id": "ebc478f5",
    "outputId": "b053e456-7b2e-4e59-f121-79b1ef4cfd2b"
   },
   "outputs": [],
   "source": [
    "imgs_to_show = 5\n",
    "\n",
    "for _ in range(imgs_to_show):\n",
    "\n",
    "    # Cargamos un batch de imagenes\n",
    "    images, images_classes = next(iter(train_loader))\n",
    "\n",
    "    # Nos quedamos con la primera imagen del batch\n",
    "    img, img_class = images[0], images_classes[0]\n",
    "\n",
    "    # Mostramos alguna informacion de la imagen\n",
    "    print(f\"Img class is: {img_class}\")\n",
    "\n",
    "    # Re-escalamos y mostramos la imagen\n",
    "    img = img.reshape((28, 28))\n",
    "    show_img(img, color_format_range = (-1.0, 1.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfcc5d89",
   "metadata": {
    "id": "dfcc5d89"
   },
   "source": [
    "Show the sizes of the datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6a35e9b",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "b6a35e9b",
    "outputId": "61769ff2-927b-45c0-acab-dc21dabb329a"
   },
   "outputs": [],
   "source": [
    "print(f\"Train dataset: {len(train_dataset)}\")\n",
    "print(f\"Validation dataset: {len(validation_dataset)}\")\n",
    "print(f\"Test dataset: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5029063",
   "metadata": {
    "id": "c5029063"
   },
   "source": [
    "## Random triplets generation\n",
    "\n",
    "- For training the siamese network, we need triplets for computing *triplet loss*\n",
    "- One simple way of generating triplets is randomly creating triplets (as explained before) in an offline fashion\n",
    "    - Created a dataset of random triplets beforehand\n",
    "    - With fixed elements \n",
    "- Later, we are going to generate triplets in an online fashion with more sophisticated methods\n",
    "- In this case, we create a *pytorch* `Dataset` class, following the [official documentation]](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59f54f66",
   "metadata": {
    "id": "59f54f66"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import RandomSampler\n",
    "\n",
    "# TODO -- change attr names to english ones\n",
    "class RandomTriplets(Dataset):\n",
    "    \"\"\"\n",
    "    Dataset where elements are random triplets\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, base_data: Dataset, custom_len: int, transform = None):\n",
    "        self.base_data = base_data\n",
    "        self.custom_len = custom_len\n",
    "        self.transform = transform\n",
    "        self.random_sampler = RandomSampler(self.base_data, replacement=True, num_samples=1, generator=None)\n",
    "        \n",
    "        # For speeding things up, we can pre-compute a list of lists, such that\n",
    "        # self.posiciones_clases[i] has all the indixes of elements corresponding \n",
    "        # to i-th class\n",
    "        self.posiciones_clases = self.__precompute_list_of_classes()\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        We are generating random triplets. WAe have an attr that defines\n",
    "        how many triplets we want, so we return that value\n",
    "        \"\"\"\n",
    "        return self.custom_len\n",
    "\n",
    "    def __getitem__(self, idx: int) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
    "        \"\"\"       \n",
    "        Function which is called when using `dataset[idx]`\n",
    "        \n",
    "        Instead of returning an image, which is the common behavior, we return \n",
    "        a triplet (anchor, positive, negative)\n",
    "        \"\"\"\n",
    "\n",
    "        # Speed up computations\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        # Take a random class, which will act as anchor\n",
    "        anchor, anchor_class = self.base_data[next(iter(self.random_sampler))]\n",
    "\n",
    "        # Take a random image of the same class\n",
    "        # Thus, this image will be the anchor\n",
    "        random_index = np.random.choice(self.posiciones_clases[anchor_class])\n",
    "        positive, positive_class = self.base_data[random_index]\n",
    "\n",
    "        # Take a random image of other class\n",
    "        # Start taking a class that is not the same as the anchor class\n",
    "        posible_classes = list(range(10))\n",
    "        posible_classes.remove(anchor_class)\n",
    "        negative_class = np.random.choice(posible_classes)\n",
    "\n",
    "        # Take a random image of that negative class selected before\n",
    "        random_index = np.random.choice(self.posiciones_clases[negative_class])\n",
    "        negative, negative_class = self.base_data[random_index]\n",
    "        \n",
    "        # Pack up the triplet\n",
    "        triplet = [anchor, positive, negative]\n",
    "\n",
    "        # Apply the dataset transformation to the three images \n",
    "        if self.transform:\n",
    "            triplet = [self.transform(np.array(img)) for img in triplet]\n",
    "\n",
    "        return triplet\n",
    "\n",
    "    def __precompute_list_of_classes(self) -> List[List[int]]:\n",
    "        \"\"\"\n",
    "        Computes a list containing list. Each list contains the positions of elements of given class\n",
    "        ie. class_positions[i] contains all positions of elements of i-th class\n",
    "        \"\"\"\n",
    "\n",
    "        # Init the list of lists\n",
    "        posiciones_clases = [[] for _ in range(10)]\n",
    "\n",
    "        # Walk the dataset and assign each element to its corresponding class\n",
    "        for idx, element in enumerate(self.base_data):\n",
    "            _, img_class = element\n",
    "            posiciones_clases[img_class].append(idx)\n",
    "\n",
    "        return posiciones_clases\n",
    "\n",
    "    \n",
    "class CustomReshape(object):\n",
    "    \"\"\"\n",
    "    Transform an (28, 1, 28) img to an (28, 28) img\n",
    "    \"\"\"\n",
    "\n",
    "    def __call__(self, image):\n",
    "        image = image.reshape(28, 28)\n",
    "        return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cbf68",
   "metadata": {
    "id": "2b4cbf68"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section \n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "   \n",
    "    # Reshape images from (28, 1, 28) to (28, 28)\n",
    "    transform = transforms.Compose([\n",
    "        # Move images to torch tensors that are needed for pytorch\n",
    "        transforms.ToTensor(),\n",
    "            \n",
    "        # Use our custom reshape\n",
    "        CustomReshape(),\n",
    "    ])\n",
    "\n",
    "    # Generate a dataset with the random triplets and use it \n",
    "    # on a dataloader, for train, validation and test original \n",
    "    # datasets \n",
    "    random_triplets_train = RandomTriplets(\n",
    "        base_data = train_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "    train_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_train,\n",
    "        batch_size = OFFLINE_BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    random_triplets_validation = RandomTriplets(\n",
    "        base_data = validation_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "    validation_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_validation,\n",
    "        batch_size = OFFLINE_BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    random_triplets_test = RandomTriplets(\n",
    "        base_data = test_dataset,\n",
    "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
    "        transform = transform,\n",
    "    )\n",
    "    test_loader = torch.utils.data.DataLoader(\n",
    "        random_triplets_test,\n",
    "        batch_size = OFFLINE_BATCH_SIZE,\n",
    "        shuffle = True,\n",
    "        num_workers = NUM_WORKERS,\n",
    "        pin_memory = True,\n",
    "    )\n",
    "\n",
    "    # Show some triplets to check behaviour\n",
    "    custom_triplet = random_triplets_train[2]\n",
    "    for i in custom_triplet :\n",
    "        show_img(i, color_format_range = (-1.0, 1.0))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b0c7d0",
   "metadata": {
    "id": "58b0c7d0"
   },
   "source": [
    "## Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "569ed64d",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "569ed64d",
    "outputId": "367dc6bd-29ea-496a-9baf-7d878423c030"
   },
   "outputs": [],
   "source": [
    "net = LightModel(EMBEDDING_DIMENSION)\n",
    "\n",
    "# The training function takes a dict of parameters \n",
    "# Build this dict with Global Parameters values\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS_RANDOM\n",
    "parameters[\"lr\"] = RANDOM_TRIPLET_LEARNING_RATE\n",
    "parameters[\"criterion\"] = MeanTripletBatchTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS)\n",
    "\n",
    "# Define the logger we want for the training\n",
    "logger = TripletLoggerOffline(\n",
    "    net = net,\n",
    "    iterations = 10 * OFFLINE_BATCH_SIZE,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc94a2d",
   "metadata": {
    "id": "5bc94a2d"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this training\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "\n",
    "    training_history = train_model_offline(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetwork\",\n",
    "        logger = logger,\n",
    "        snapshot_iterations = None\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f312e365",
   "metadata": {
    "id": "f312e365"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cedfac4",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cedfac4",
    "outputId": "1ed77730-683b-40fd-f627-d034abbb2145"
   },
   "outputs": [],
   "source": [
    "# From this point, we won't perform training on the model\n",
    "# So eval mode is set for better performance \n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "967f2783",
   "metadata": {
    "id": "967f2783"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c60be0c4",
   "metadata": {
    "id": "c60be0c4"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
    "    \n",
    "    # Get the \"criterion\" metric over test loader\n",
    "    # Remember this test loader is made of random triplets from the test dataset\n",
    "    with torch.no_grad(): core.test_model(net, test_loader, parameters[\"criterion\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "oOPlWZWYu7cR",
   "metadata": {
    "id": "oOPlWZWYu7cR"
   },
   "source": [
    "We use the adapter to classification task to have more metrics about the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29f14b37",
   "metadata": {
    "id": "29f14b37",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        # Get back the two original dataloaders\n",
    "        # That's to say, dataloaders that return minibatches of images\n",
    "        # No triplets involved with this dataloaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = OFFLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "        test_loader = torch.utils.data.DataLoader(\n",
    "            test_dataset,\n",
    "            batch_size = OFFLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "    \n",
    "        # Generate a classifier using the embedding (represented by the network)\n",
    "        classifier = EmbeddingToClassifier(\n",
    "            net, \n",
    "            k = NUMBER_NEIGHBOURS, \n",
    "            data_loader = train_loader,\n",
    "            embedding_dimension = EMBEDDING_DIMENSION\n",
    "        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae61cab8",
   "metadata": {
    "id": "ae61cab8"
   },
   "source": [
    "We evaluate this classifier by watching how it works over a small test set. Later we take some metrics from this classifier to evaluate it more precisely.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2b77a7a",
   "metadata": {
    "id": "f2b77a7a"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "      \n",
    "        # Shoow only `max_iterations` classifications\n",
    "        counter = 0\n",
    "        max_iterations = 20\n",
    "    \n",
    "        for img, img_class in test_dataset:\n",
    "            predicted_class = classifier.predict(img)\n",
    "            print(f\"True label: {img_class}, predicted label: {predicted_class[0]}, correct: {img_class == predicted_class[0]}\")\n",
    "    \n",
    "            counter += 1\n",
    "            if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4def6fb",
   "metadata": {
    "id": "d4def6fb"
   },
   "source": [
    "## Plot of the embedding\n",
    "\n",
    "- If the dimension of the embedding is 2, then we can plot how the transformation to a classificator works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47c93584",
   "metadata": {
    "id": "47c93584"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
    "    with torch.no_grad(): classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bfcedc5e",
   "metadata": {
    "id": "bfcedc5e"
   },
   "source": [
    "## Evaluating the obtained classifier \n",
    "\n",
    "- Now that we adapted our network to a classification task, we can compute some classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6bcfa1",
   "metadata": {
    "id": "ef6bcfa1"
   },
   "outputs": [],
   "source": [
    "# TODO -- translate to english\n",
    "# TODO -- move this to lib \n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def calculate_roc_auc(true_labels_prob: np.array, predicted_labels_prob: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el area bajo la curva ROC, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \n",
    "    Las listas de etiquetas deben ser etiquetas probabilisticas\n",
    "    \"\"\"\n",
    "    return roc_auc_score(true_labels_prob, predicted_labels_prob, multi_class = \"ovo\")\n",
    "    \n",
    "def calculate_accuracy(true_labels: np.array, predicted_labels: np.array) -> float:\n",
    "    \"\"\"\n",
    "    Calcula el accuracy, dadas las etiquetas verdaderas y las\n",
    "    etiqeutas predichas por un modelo\n",
    "    \"\"\"\n",
    "    return accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "def calculate_silhouette(x, y):\n",
    "    \"\"\"Calcula el indice de silhouette para el embedding calculado por el modelo\"\"\"\n",
    "    return silhouette_score(x, y)\n",
    "\n",
    "def evaluate_model(model, train_loader, test_loader) -> dict:\n",
    "    \"\"\"\n",
    "    Evalua, usando distintas metricas, el modelo que hemos entrenado\n",
    "    Tambien evaluamos el embedding obtenido, no solo el clasificador\n",
    "    \n",
    "    Devuelve un diccionario con todas las metricas calculadas con el modelo\n",
    "    \"\"\"\n",
    "    \n",
    "    # Diccionario en el que vamos a almacenar todas las metricas\n",
    "    metrics = dict()\n",
    "    \n",
    "    # Tomamos los arrays en formato adecuado para calcular ciertas metricas\n",
    "    x_train, y_train = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
    "    x_test, y_test = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = test_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
    "    \n",
    "    # Empezamos usando el modelo para realizar las predicciones+\n",
    "    # Usamos predicciones probabilisticas pues estas son fundamentales para\n",
    "    # la metrica roc auc\n",
    "    train_predicted_labels_prob = model.predict_proba_using_embedding(x_train)\n",
    "    test_predicted_labels_prob = model.predict_proba_using_embedding(x_test)\n",
    "\n",
    "    # Tomamos ahora las etiqeutas sin probabilidad\n",
    "    train_predicted_labels= model.predict_using_embedding(x_train)\n",
    "    test_predicted_labels = model.predict_using_embedding(x_test)\n",
    "    \n",
    "    # Tomamos las metricas de accuracy\n",
    "    metrics[\"train_acc\"] = calculate_accuracy(y_train, train_predicted_labels)\n",
    "    metrics[\"test_acc\"] = calculate_accuracy(y_test, test_predicted_labels)\n",
    "    \n",
    "    # Tomamos las areas bajo la curva ROC\n",
    "    metrics[\"train_roc_auc\"] = calculate_roc_auc(y_train, train_predicted_labels_prob)\n",
    "    metrics[\"test_roc_auc\"] = calculate_roc_auc(y_test, test_predicted_labels_prob)\n",
    "    \n",
    "    # Tomamos el indice de silhouette\n",
    "    metrics[\"train_silhouette\"] = calculate_silhouette(x_train, y_train)\n",
    "    metrics[\"test_silhouette\"] = calculate_silhouette(x_test, y_test)\n",
    "    \n",
    "    #Calculamos Matriz de confusion\n",
    "    metrics[\"train_confusion_matrix\"]=confusion_matrix(y_train, train_predicted_labels)\n",
    "    metrics[\"test_confusion_matrix\"]=confusion_matrix(y_test, test_predicted_labels)\n",
    "    \n",
    "    # Devolvemos las metricas en formato diccionario, que nos va a ser comodo para\n",
    "    # pasarlas a tablas y para mostrar muchas metricas simultaneamente\n",
    "    return metrics\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(train_confusion_matrix,test_confusion_matrix):\n",
    "    \"\"\"\n",
    "    Mostramos graficamente (mapa de calor) la matriz de confusion\n",
    "    \n",
    "    Podriamos mostrar directamente los valores de la matriz, pero al estar\n",
    "    trabajando con 25 clases es mas dificil de interpretar\n",
    "    \"\"\"\n",
    "    \n",
    "    # Mostramos graficamente las dos matrices de confusion\n",
    "    color_palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
    "    print(\"Matriz de confusion en TRAIN\")\n",
    "    sns.heatmap(train_confusion_matrix, annot = False, cmap = color_palette, fmt=\"d\")\n",
    "    plt.show()\n",
    "    \n",
    "    print(\"Matriz de confusion en TEST\")\n",
    "    sns.heatmap(test_confusion_matrix, annot = False, cmap = color_palette)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7dc2fcb",
   "metadata": {
    "id": "f7dc2fcb"
   },
   "outputs": [],
   "source": [
    "# Check if we want to skip this section\n",
    "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
    "    with torch.no_grad():\n",
    "    \n",
    "        classifier.embedder.set_permute(False)\n",
    "    \n",
    "        metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "        pprint(metrics)\n",
    "    \n",
    "        classifier.embedder.set_permute(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "OXxDVsblof-O",
   "metadata": {
    "id": "OXxDVsblof-O"
   },
   "source": [
    "# Online Batch Triplets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "IDZBu1P-o4TM",
   "metadata": {
    "id": "IDZBu1P-o4TM"
   },
   "source": [
    "## Choose the loss function to use \n",
    "\n",
    "- We have so many combinations for loss functions that is not feasible to use one Colab section for each\n",
    "- Combinations depend on:\n",
    "    1. Batch hard vs Batch all \n",
    "    2. Classical triplet loss vs Softplus loss\n",
    "    3. All summands mean vs Only > 0 summands mean\n",
    "- This election is done in *Global Parameters of the Notebook* section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "lU4wWC-GpHdI",
   "metadata": {
    "id": "lU4wWC-GpHdI"
   },
   "outputs": [],
   "source": [
    "batch_loss_function = None\n",
    "if BATCH_TRIPLET_LOSS_FUNCTION == \"hard\":\n",
    "    batch_loss_function = BatchHardTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean = USE_GT_ZERO_MEAN_LOSS) \n",
    "if BATCH_TRIPLET_LOSS_FUNCTION == \"all\":\n",
    "    batch_loss_function = BatchAllTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean =  USE_GT_ZERO_MEAN_LOSS) \n",
    "\n",
    "# Sanity check\n",
    "if batch_loss_function is None:\n",
    "    raise Exception(f\"BATCH_TRIPLET_LOSS global parameter got unexpected value: {BATCH_TRIPLET_LOSS_FUNCTION}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7378947c",
   "metadata": {
    "id": "7378947c"
   },
   "source": [
    "## Use same data but use custom samplers\n",
    "\n",
    "- In the previous section we didn't changed the datasets\n",
    "- But dataloaders have changed\n",
    "- Also, we want to use custom `Sampler` to do $P-K$ sampling\n",
    "    - For each minibatch, select $P$ random classes. For each selected class, select $K$ random images\n",
    "- Thus, each minibatch has a size of $P \\times K$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8425cd40",
   "metadata": {
    "id": "8425cd40"
   },
   "outputs": [],
   "source": [
    "# Train dataset and validation dataset were splitted before, resulting in two `Subset` objects\n",
    "# We want to work only with `Dataset` objects, so get only that part\n",
    "train_dataset = train_dataset.dataset\n",
    "validation_dataset = validation_dataset.dataset\n",
    "\n",
    "# New dataloaders that use our custom sampler\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    num_workers = NUM_WORKERS, \n",
    "    pin_memory = True,\n",
    "    sampler = CustomSampler(P, K, train_dataset)\n",
    ")\n",
    "\n",
    "# TODO -- here I don't know if use default sampler or custom sampler\n",
    "validation_loader = torch.utils.data.DataLoader(\n",
    "    validation_dataset,\n",
    "    batch_size = ONLINE_BATCH_SIZE,\n",
    "    shuffle = True,\n",
    "    num_workers = NUM_WORKERS, \n",
    "    pin_memory = True,\n",
    ")\n",
    "\n",
    "# TODO -- here I don't know if use default sampler or custom sampler\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "  test_dataset,\n",
    "  batch_size = ONLINE_BATCH_SIZE,\n",
    "  shuffle = True,\n",
    "  num_workers = NUM_WORKERS,\n",
    "  pin_memory = True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5593607e",
   "metadata": {
    "id": "5593607e"
   },
   "source": [
    "## Hyperparameter tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "867bd047",
   "metadata": {
    "id": "867bd047"
   },
   "outputs": [],
   "source": [
    "# TODO -- translate to english\n",
    "# TODO -- move to lib\n",
    "def custom_cross_validation(net, parameters, train_dataset, k):\n",
    "    \"\"\"Funcion propia para hacer k-fold cross validation de una red convolucional\"\"\"\n",
    "\n",
    "    # Definimos la forma en la que vamos a hacer el split de los folds\n",
    "    ss = ShuffleSplit(n_splits=k, test_size=0.25, random_state=RANDOM_SEED)\n",
    "    \n",
    "    # Lista en la que guardamos las perdidas encontradas en cada fold\n",
    "    losses = []\n",
    "\n",
    "    # Iteramos usando el split que nos da sklearn\n",
    "    for train_index, validation_index in ss.split(train_dataset):\n",
    "        \n",
    "        # Tenemos los indices de los elementos, asi que tomamos los dos datasets\n",
    "        # usando dichos indices\n",
    "        train_fold = [train_dataset[idx] for idx in train_index]\n",
    "        validation_fold = [train_dataset[idx] for idx in validation_index]\n",
    "\n",
    "        # Transformamos los datasets en dataloaders\n",
    "        train_loader = torch.utils.data.DataLoader(\n",
    "            train_fold,\n",
    "            batch_size = ONLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "        validation_loader = torch.utils.data.DataLoader(\n",
    "            validation_fold,\n",
    "            batch_size = ONLINE_BATCH_SIZE,\n",
    "            shuffle = True,\n",
    "            num_workers = NUM_WORKERS,\n",
    "            pin_memory = True,\n",
    "        )\n",
    "\n",
    "        # Entrenamos la red\n",
    "        _ = train_model_online(\n",
    "            net = net,\n",
    "            path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "            parameters = parameters,\n",
    "            train_loader = train_loader,\n",
    "            validation_loader = validation_loader,\n",
    "            name = \"SiameseNetworkOnline\",\n",
    "            logger = SilentLogger(),\n",
    "            snapshot_iterations = None\n",
    "        )\n",
    "\n",
    "        # Evaluamos la red en el fold de validacion\n",
    "        net.eval()\n",
    "        loss = metrics.calculate_mean_triplet_loss_online(net, validation_loader, parameters[\"criterion\"], 1.0)\n",
    "        loss = float(loss) # Pasamos el tensor de un unico elemento a un float simple\n",
    "\n",
    "        # Añadimos el loss a nuestra lista\n",
    "        losses.append(loss)\n",
    "    \n",
    "    # Devolvemos el array en formato numpy para que sea mas comodo trabajar con ella\n",
    "    return np.array(losses)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "292f5ea4",
   "metadata": {
    "id": "292f5ea4"
   },
   "outputs": [],
   "source": [
    "# Controlamos si queremos realizar el hyperparameater tuning o no\n",
    "if SKIP_HYPERPARAMTER_TUNING is False:\n",
    "\n",
    "    # Para controlar que parametros ya hemos explorado y queremos saltar\n",
    "    already_explored_parameters = [\n",
    "        # Embedding dimension, learning rate, margin\n",
    "        (2, 0.0001, 0.01),\n",
    "        (2, 0.0001, 1),\n",
    "        (3, 0.0001),\n",
    "    ]\n",
    "\n",
    "    # Parametros que queremos mover\n",
    "    #margin_values = [0.01, 0.1, 1.0]\n",
    "    # TODO -- volver a poner todos los valores\n",
    "    margin_values = [1.0]\n",
    "    learning_rate_values = [0.0001, 0.001, 0.01]\n",
    "    embedding_dimension_values = [2, 3, 4]\n",
    "    \n",
    "    # Parametros que fijamos de antemano\n",
    "    epochs = HYPERPARAMETER_TUNING_EPOCHS\n",
    "    \n",
    "    # Llevamos la cuenta de los mejores parametros y el mejor error encontrados hasta\n",
    "    # el momento\n",
    "    best_loss = None\n",
    "    best_parameters = {\n",
    "        \"embedding_dimension\": None,\n",
    "        \"lr\": None,\n",
    "        \"margin\": None\n",
    "    }\n",
    "    \n",
    "    # Exploramos las combinaciones de parametros\n",
    "    for margin in margin_values:\n",
    "        for learning_rate in learning_rate_values:\n",
    "            for embedding_dimension in embedding_dimension_values:\n",
    "        \n",
    "                print(f\"Optimizando para margin: {margin}, lr: {learning_rate}, embedding_dim: {embedding_dimension}\")\n",
    "\n",
    "                # Comprobamos si tenemos que saltarnos el calculo de algun valor\n",
    "                # porque ya se haya hecho\n",
    "                if (embedding_dimension, learning_rate, margin) in already_explored_parameters:\n",
    "                    print(\"\\tSaltando este calculo porque ya se realizo\")\n",
    "                    continue\n",
    "                \n",
    "                # Definimos el modelo que queremos optimizar\n",
    "                net = ResNet18(embedding_dimension)\n",
    "                \n",
    "                # En este caso, al no estar trabajando con los minibatches\n",
    "                # (los usamos directamente como nos los da pytorch), no tenemos\n",
    "                # que manipular los tensores\n",
    "                net.set_permute(False)\n",
    "                \n",
    "                parameters = dict()\n",
    "                parameters[\"epochs\"] = epochs\n",
    "                parameters[\"lr\"] = learning_rate\n",
    "                parameters[\"criterion\"] = BatchHardTripletLoss(margin)\n",
    "                logger = SilentLogger()\n",
    "    \n",
    "                # Usamos nuestra propia funcion de cross validation para validar el modelo\n",
    "                losses = custom_cross_validation(net, parameters, train_dataset, k = NUMBER_OF_FOLDS) \n",
    "                print(f\"El loss conseguido es {losses.mean()}\")\n",
    "                print(\"\")\n",
    "            \n",
    "                # Comprobamos si hemos mejorado la funcion de perdida\n",
    "                # En cuyo caso, actualizamos nuestra estructura de datos y, sobre todo, mostramos\n",
    "                # por pantalla los nuevos mejores parametros\n",
    "                basic_condition = math.isnan(losses.mean()) is False             # Si es NaN no entramos al if\n",
    "                enter_condition = best_loss is None or losses.mean() < best_loss # Entramos al if si mejoramos la perdida\n",
    "                compound_condition = basic_condition and enter_condition\n",
    "                if compound_condition:\n",
    "                \n",
    "                    # Actualizamos nuestra estructura de datos\n",
    "                    best_loss = losses.mean()\n",
    "                    best_parameters = {\n",
    "                        \"embedding_dimension\": embedding_dimension,\n",
    "                        \"lr\": learning_rate,\n",
    "                        \"margin\": margin,\n",
    "                    }\n",
    "            \n",
    "                    # Mostramos el cambio encontrado\n",
    "                    print(\"==> ENCONTRADOS NUEVOS MEJORES PARAMETROS\")\n",
    "                    print(f\"Mejores parametros: {best_parameters}\")\n",
    "                    print(f\"Mejor loss: {best_loss}\")\n",
    "            \n",
    "                \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1ee8ba",
   "metadata": {
    "id": "3e1ee8ba"
   },
   "source": [
    "## Online training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7a2afd9",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7a2afd9",
    "outputId": "4a53bc0c-c978-4ea4-8fef-587b38158cd7"
   },
   "outputs": [],
   "source": [
    "net = LightModel(EMBEDDING_DIMENSION)\n",
    "\n",
    "# The custom sampler takes care of minibatch management\n",
    "# Thus, we don't have to make manipulations on them\n",
    "net.set_permute(False)\n",
    "\n",
    "# Training parameters\n",
    "parameters = dict()\n",
    "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
    "parameters[\"lr\"] = ONLINE_LEARNING_RATE\n",
    "\n",
    "# We use the loss function that depends on the global parameter BATCH_TRIPLET_LOSS_FUNCTION\n",
    "# We selected this loss func in *Choose the loss function to use* section\n",
    "parameters[\"criterion\"] = batch_loss_function\n",
    "\n",
    "# Define the logger we want to use \n",
    "logger = TripletLoggerOnline(\n",
    "    net = net,\n",
    "    iterations = -1,\n",
    "    loss_func = parameters[\"criterion\"],\n",
    "    train_percentage = 0.01,\n",
    "    validation_percentage = 1.0,\n",
    ")\n",
    "\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa722a06",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 536
    },
    "id": "aa722a06",
    "outputId": "b1ad36c9-d052-4355-9c40-9b5f2642ebc3"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "# Check if we want to skip training\n",
    "if USE_CACHED_MODEL is False:\n",
    "\n",
    "    # To measure the time it takes to train\n",
    "    ts = time.time()\n",
    "    \n",
    "    # torch.jit.script to speed up the training\n",
    "    torch.jit.script(\n",
    "    training_history = train_model_online(\n",
    "        net = net,\n",
    "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
    "        parameters = parameters,\n",
    "        train_loader = train_loader,\n",
    "        validation_loader = validation_loader,\n",
    "        name = \"SiameseNetworkOnline\",\n",
    "        logger = logger,\n",
    "        snapshot_iterations = None\n",
    "    ))\n",
    "\n",
    "    # Compute how long it took\n",
    "    te = time.time()\n",
    "    print(f\"Ha tardado {te - ts}\")\n",
    "    \n",
    "    # Update the model cache\n",
    "    filesystem.save_model(net, MODEL_CACHE_FOLDER, \"online_model_cached\")\n",
    "    \n",
    "# In case we skipped training, load the model from cache\n",
    "else:\n",
    "    \n",
    "    # Load the model from cache\n",
    "    net = filesystem.load_model(\n",
    "        os.path.join(MODEL_CACHE_FOLDER, \"online_model_cached\"), \n",
    "        lambda: ResNet18(EMBEDDING_DIMENSION)\n",
    "    )\n",
    "    \n",
    "    # Load the network in corresponding mem device (cpu -> ram, gpu -> gpu mem\n",
    "    device = core.get_device()\n",
    "    net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0473016a-7a1b-4cd6-9c69-d9326f9f3180",
   "metadata": {},
   "outputs": [],
   "source": [
    "# From this point, we won't perform training on the model\n",
    "# So eval mode is set for better performance \n",
    "net.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ae35995",
   "metadata": {
    "id": "3ae35995"
   },
   "outputs": [],
   "source": [
    "# Check if we have to skip this section\n",
    "if USE_CACHED_MODEL is False:\n",
    "        \n",
    "    # In the start, we have too much error that goes down very fast\n",
    "    # So we ignore first 5 values for the metric, improving legibility\n",
    "    # cut = 5\n",
    "    # training_history[\"loss\"] = training_history[\"loss\"][cut:]\n",
    "    # training_history[\"val_loss\"] = training_history[\"val_loss\"][cut:]\n",
    "    \n",
    "    show_learning_curve(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d68b8fef",
   "metadata": {
    "id": "d68b8fef"
   },
   "source": [
    "## Model evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6aaa8b0-37ba-4bbe-9de2-169badb66db6",
   "metadata": {},
   "source": [
    "Show the \"criterion\" metric on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5210628c",
   "metadata": {
    "id": "5210628c"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    net.set_permute(False)\n",
    "    \n",
    "    core.test_model_online(net, test_loader, parameters[\"criterion\"], online = True)\n",
    "    \n",
    "    net.set_permute(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "583be3ea-7684-4312-b6af-4462ed77cdda",
   "metadata": {},
   "source": [
    "Now take the classifier from the embedding and use it to compute some classification metrics:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2e58d6f",
   "metadata": {
    "id": "f2e58d6f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    \n",
    "    # Try to clean memory, because we can easily run out of memory\n",
    "    # This provoke the notebook to crash, and all in-memory objects to be lost\n",
    "    try_to_clean_memory()\n",
    "    \n",
    "    # With hopefully enough memory, try to convert the embedding to a classificator\n",
    "    classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4363deb-186f-4834-8207-e54099c58abd",
   "metadata": {},
   "source": [
    "We evaluate this classifier by watching how it works over a small test set. Later we take some metrics from this classifier to evaluate it more precisely.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46acde43",
   "metadata": {
    "id": "46acde43"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "    # Shoow only `max_iterations` classifications\n",
    "    counter = 0\n",
    "    max_iterations = 20\n",
    "\n",
    "    for img, img_class in test_dataset:\n",
    "        predicted_class = classifier.predict(img)\n",
    "        print(f\"True label: {img_class}, predicted label: {predicted_class[0]}, correct: {img_class == predicted_class[0]}\")\n",
    "\n",
    "        counter += 1\n",
    "        if counter == max_iterations: break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58fa44a0",
   "metadata": {
    "id": "58fa44a0"
   },
   "source": [
    "## Plot of the embedding\n",
    "\n",
    "- If the dimension of the embedding is 2, then we can plot how the transformation to a classificator works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab56fa9",
   "metadata": {
    "id": "8ab56fa9"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    classifier.scatter_plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83cea66d",
   "metadata": {
    "id": "83cea66d"
   },
   "source": [
    "## Evaluating the obtained classifier \n",
    "\n",
    "- Now that we adapted our network to a classification task, we can compute some classification metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27b4ae22",
   "metadata": {
    "id": "27b4ae22"
   },
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    try_to_clean_memory()\n",
    "    classifier.embedder.set_permute(False)\n",
    "    \n",
    "    metrics = evaluate_model(classifier, train_loader, test_loader)\n",
    "    pprint(metrics)\n",
    "    \n",
    "    classifier.embedder.set_permute(True)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [
    "wDEKdrG0oNZ_",
    "2c1847bb",
    "e44717f5",
    "c5029063",
    "a772b2e2",
    "58b0c7d0",
    "967f2783",
    "d4def6fb",
    "bfcedc5e",
    "d68b8fef",
    "58fa44a0",
    "83cea66d"
   ],
   "name": "Notebook.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
