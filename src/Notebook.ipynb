{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "58b07228",
      "metadata": {
        "id": "58b07228"
      },
      "source": [
        "\n",
        "# Global Parameters of the Notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f735abd1",
      "metadata": {
        "id": "f735abd1"
      },
      "outputs": [],
      "source": [
        "# Para definir los path\n",
        "import os\n",
        "\n",
        "# Define si estamos ejecutando el Notebook en nuestro \n",
        "# ordenador (\"local\") o en Google Colab (\"remote\")\n",
        "RUNNING_ENV = \"remote\"\n",
        "\n",
        "# Path que vamos a usar como base para el resto de paths\n",
        "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# Directorio en el que guardamos los scripts de python que usamos \n",
        "# como libreria propia\n",
        "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
        "\n",
        "# Directorio en el que guardamos los datos de entrenamiento y test\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
        "\n",
        "# Numero de procesos que queremos usar\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Batch size que queremos usar para entrenamiento offline\n",
        "DATALOADER_BACH_SIZE = 32\n",
        "\n",
        "# Bath size que queremos usar para entrenamiento online\n",
        "ONLINE_BATCH_SIZE = 2 ** 9\n",
        "\n",
        "# Tamaño del conjunto de triples aleatorios\n",
        "# Tiene que ser un multiplo de DATALOADER_BACH_SIZE para que\n",
        "# la red tome correctamente los batches que espera\n",
        "RANDOM_TRIPLETS_DATA_SIZE = DATALOADER_BACH_SIZE * 15\n",
        "\n",
        "# Numero de epocas para las que queremos entrenar el modelo de triples aleatorios\n",
        "TRAINING_EPOCHS_RANDOM = 10\n",
        "\n",
        "# Learning rate para los triples aleatorios\n",
        "RANDOM_TRIPLET_LEARNING_RATE = 0.001\n",
        " \n",
        "# Numero de epocas por las que queremos entrenar el modelo de triples dificiles online\n",
        "TRAINING_EPOCHS = 3 \n",
        "\n",
        "# Numero de epocas que usamos en k-fold Cross Validation para \n",
        "# hacer la exploracion de los parametros\n",
        "HYPERPARAMETER_TUNING_EPOCHS = 7\n",
        "\n",
        "# Numero de folds que usamos en k-fold Cross Validation para el hyperparameter tuning\n",
        "NUMBER_OF_FOLDS = 4\n",
        "\n",
        "# Margen para la funcion de perdida\n",
        "MARGIN = 0.001\n",
        "\n",
        "# Dimension del embedding que calculamos\n",
        "EMBEDDING_DIMENSION = 2\n",
        "\n",
        "# Learning rate para el entrenamiento con triples dificiles online\n",
        "ONLINE_LEARNING_RATE = 0.01\n",
        "\n",
        "# Numero de vecinos a usar en la adaptacion a clasificador\n",
        "NUMBER_NEIGHBOURS = 3\n",
        "\n",
        "# Semilla aleatoria para aquellas partes en las que fijamos el estado aleatorio\n",
        "RANDOM_SEED = 123456789\n",
        "\n",
        "# Controla si queremos evitar toda la parte del entrenamiento\n",
        "# con triples aleatorios\n",
        "# TODO -- antes de entregar poner a False\n",
        "SKIP_RANDOM_TRIPLETS_TRAINING = True\n",
        "\n",
        "# Controla si queremos entrenar el modelo y usarlo o \n",
        "# no entrenar nada y cargar el modelo entrenado de disco\n",
        "# Cuando es False, entrenamos y ademas actualizamos el modelo \n",
        "# en disco\n",
        "# TODO -- poner esto a False\n",
        "USE_CACHED_MODEL = False\n",
        "MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")\n",
        "\n",
        "# Controla si queremos saltarnos o no el hyperparameter \n",
        "# tuning para el entrenamiento online \n",
        "SKIP_HYPERPARAMTER_TUNING = True\n",
        "\n",
        "# Batch Triplet Loss Function\n",
        "# This way we can choose among \"hard\", \"all\"\n",
        "BATCH_TRIPLET_LOSS_FUNCTION = \"hard\"\n",
        "\n",
        "# Wether or not use softplus loss function instead of vanilla triplet loss\n",
        "USE_SOFTPLUS_LOSS = False\n",
        "\n",
        "# Count all sumamnds in the mean loss or only those summands greater than zero\n",
        "USE_GT_ZERO_MEAN_LOSS = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a1a0d0",
      "metadata": {
        "id": "01a1a0d0"
      },
      "source": [
        "# Autorización si estamos usando Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "b1264a2a",
      "metadata": {
        "id": "b1264a2a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0b027eea-859e-4df6-e4a3-4446accf6071"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "if RUNNING_ENV == \"remote\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1327dec",
      "metadata": {
        "id": "d1327dec"
      },
      "source": [
        "# Importando los módulos que vamos a usar"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "fc75086e",
      "metadata": {
        "id": "fc75086e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# Para poder usar ResNet18 preentrenado\n",
        "import torchvision.models as models \n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "import gc\n",
        "import functools\n",
        "import math\n",
        "import seaborn as sns\n",
        "\n",
        "# Todas las piezas concretas que usamos de sklearn\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "# Cargamos en el Notebook todos los ficheros .py que definen nuestra propia libreria\n",
        "# Usamos esta libreria para escribir el codigo base necesario para llevar a cabo ciertas\n",
        "# tareas del notebook (como el bucle de entrenamiento) que no tienen interes mostrar\n",
        "# en este notebook\n",
        "!cp -r \"$LIB_PATH\"/* .\n",
        "\n",
        "# Ahora que hemos cargado estos ficheros en el Notebook, importamos lo necesario\n",
        "# de nuestra propia libreria\n",
        "import core\n",
        "import time\n",
        "import copy\n",
        "import board\n",
        "import filesystem\n",
        "import metrics\n",
        "import loss_functions\n",
        "import embedding_to_classifier\n",
        "from train_loggers import ClassificationLogger, SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger\n",
        "from models import *\n",
        "from visualizations import *\n",
        "from tqdm.notebook import tqdm\n",
        "from core import train_model_offline, train_model_online\n",
        "from models import ResNet18\n",
        "from loss_functions import MeanTripletBatchTripletLoss, BatchHardTripletLoss, BatchAllTripletLoss\n",
        "from embedding_to_classifier import EmbeddingToClassifier"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e330fb",
      "metadata": {
        "id": "a1e330fb"
      },
      "source": [
        "# Funciones comunes que vamos a usar en el notebook"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "8eb846cd",
      "metadata": {
        "id": "8eb846cd"
      },
      "outputs": [],
      "source": [
        "def show_learning_curve(training_history: dict):\n",
        "    # Tomamos las dos funciones de perdida\n",
        "    loss = training_history['loss']\n",
        "    val_loss = training_history['val_loss']\n",
        "\n",
        "    # Pasamos los elementos a cpu, que es con lo que trabaja\n",
        "    # matplotlib\n",
        "    loss = [loss_el.cpu() for loss_el in loss]\n",
        "    val_loss = [val_loss_el.cpu() for val_loss_el in val_loss]\n",
        "    \n",
        "    # Mostramos las graficas\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "    \n",
        "def try_to_clean_memory(): \n",
        "    torch.cuda.empty_cache() \n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Triplets "
      ],
      "metadata": {
        "id": "wDEKdrG0oNZ_"
      },
      "id": "wDEKdrG0oNZ_"
    },
    {
      "cell_type": "markdown",
      "id": "2c1847bb",
      "metadata": {
        "id": "2c1847bb"
      },
      "source": [
        "## Carga del conjunto de datos\n",
        "\n",
        "- Cargamos los datos de entrenamiento y test\n",
        "- Además, separamos train en train y validación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "140a184c",
      "metadata": {
        "id": "140a184c"
      },
      "outputs": [],
      "source": [
        "# Transformaciones que queremos aplicar al cargar los datos\n",
        "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
        "])\n",
        "\n",
        "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
        "# preparado para descargar\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "# Separamos train en train y validacion\n",
        "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
        "\n",
        "# Data loaders para acceder a los datos\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = DATALOADER_BACH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size = DATALOADER_BACH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  train_dataset,\n",
        "  batch_size = DATALOADER_BACH_SIZE,\n",
        "  shuffle = True,\n",
        "  num_workers = NUM_WORKERS,\n",
        "  pin_memory = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44717f5",
      "metadata": {
        "id": "e44717f5"
      },
      "source": [
        "## Análisis Exploratorio de datos"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88baf997",
      "metadata": {
        "id": "88baf997"
      },
      "source": [
        "Mostramos algunas imágenes con sus clases para asegurar que hemos cargado correctamente las imágenes del conjunto de datos:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ebc478f5",
      "metadata": {
        "id": "ebc478f5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "b01cc232-42c0-4a18-e0e7-6152d34ec542"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase obtenida es: 7\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase obtenida es: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase obtenida es: 3\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase obtenida es: 5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "La clase obtenida es: 1\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ],
      "source": [
        "imgs_to_show = 5\n",
        "\n",
        "for _ in range(imgs_to_show):\n",
        "\n",
        "    # Cargamos un batch de imagenes\n",
        "    images, images_classes = next(iter(train_loader))\n",
        "\n",
        "    # Nos quedamos con la primera imagen del batch\n",
        "    img, img_class = images[0], images_classes[0]\n",
        "\n",
        "    # Mostramos alguna informacion de la imagen\n",
        "    print(f\"La clase obtenida es: {img_class}\")\n",
        "\n",
        "    # Re-escalamos y mostramos la imagen\n",
        "    img = img.reshape((28, 28))\n",
        "    show_img(img, color_format_range = (-1.0, 1.0))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "076aaf87",
      "metadata": {
        "id": "076aaf87"
      },
      "source": [
        "Mostramos ahora unas cuantas imágenes de forma simultánea:"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcc5d89",
      "metadata": {
        "id": "dfcc5d89"
      },
      "source": [
        "Mostramos ahora los tamaños del dataset:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "b6a35e9b",
      "metadata": {
        "id": "b6a35e9b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "86e64b1c-861f-4d4f-a8a9-aceb20abd1bd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tenemos 48000 imágenes de entrenamiento\n",
            "Tenemos 10000 imágenes de test\n"
          ]
        }
      ],
      "source": [
        "print(f\"Tenemos {len(train_dataset)} imágenes de entrenamiento\")\n",
        "print(f\"Tenemos {len(test_dataset)} imágenes de test\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c5029063",
      "metadata": {
        "id": "c5029063"
      },
      "source": [
        "## Generación de triples\n",
        "\n",
        "- Para entrenar la red siamesa, necesitamos dar triples con los que computar el *triplet loss*\n",
        "- Por ello, es necesaria una fase previa de *triplets mining*\n",
        "- En todos los casos, crearemos *Datasets* de *Pytorch* para representar la creación de los triples\n",
        "- Hacemos esto basándonos el la [documentación oficial de Pytorch](https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a772b2e2",
      "metadata": {
        "id": "a772b2e2"
      },
      "source": [
        "### Generación de triples aleatorios\n",
        "\n",
        "- Es la forma más sencilla y directa para generar triples\n",
        "- Usaremos esta generación como baseline para más tarde realizar comparaciones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "59f54f66",
      "metadata": {
        "id": "59f54f66"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import RandomSampler\n",
        "\n",
        "class RandomTriplets(Dataset):\n",
        "    \"\"\"\n",
        "    Dataset en el que los elementos son triples obtenidos de forma aleatoria\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, base_data: Dataset, custom_len: int, transform = None):\n",
        "        self.base_data = base_data\n",
        "        self.custom_len = custom_len\n",
        "        self.transform = transform\n",
        "        self.random_sampler = RandomSampler(self.base_data, replacement=True, num_samples=1, generator=None)\n",
        "        \n",
        "        # Por motivos de eficiencia, pre-computamos una lista de listas, de forma\n",
        "        # que tengamos disponibles las listas con las posiciones de cada clase por\n",
        "        # separado.\n",
        "        self.posiciones_clases = self.__precompute_list_of_classes()\n",
        "\n",
        "    def __len__(self):\n",
        "        \"\"\"\n",
        "        Devolvemos el tamaño del dataset\n",
        "        Como estamos generando triples aleatorios, devolvemos el tamaño definido\n",
        "        por parametro\n",
        "        \"\"\"\n",
        "        return self.custom_len\n",
        "\n",
        "    def __getitem__(self, idx) -> Tuple[np.ndarray, np.ndarray, np.ndarray]:\n",
        "        \"\"\"\n",
        "        Funcion que es llamada cuando se hace dataset[idx]\n",
        "        En vez de devolver una imagen (que es lo comun en esta clase dataset), \n",
        "        devolvemos un triple (anchor, positive, negative) aleatorio\n",
        "        \"\"\"\n",
        "\n",
        "        # Hacemos esto por temas de eficiencia\n",
        "        if torch.is_tensor(idx):\n",
        "            idx = idx.tolist()\n",
        "\n",
        "        # Tomamos una imagen aleatoria que sera el ancla\n",
        "        anchor, anchor_class = self.base_data[next(iter(self.random_sampler))]\n",
        "\n",
        "        # Tomamos una imagen de la misma clase, que sera la positiva, de forma aleatoria\n",
        "        random_index = np.random.choice(self.posiciones_clases[anchor_class])\n",
        "        positive, positive_class = self.base_data[random_index]\n",
        "\n",
        "        # Tomamos una imagen de otra clase, que sera la negativa\n",
        "        # Empiezo tomando una clase que no sea la del anchor\n",
        "        posible_classes = list(range(10))\n",
        "        posible_classes.remove(anchor_class)\n",
        "        negative_class = np.random.choice(posible_classes)\n",
        "\n",
        "        # Ahora tomamos un indice aleatorio de esta clase negativa\n",
        "        random_index = np.random.choice(self.posiciones_clases[negative_class])\n",
        "        negative, negative_class = self.base_data[random_index]\n",
        "        \n",
        "        # Generamos ahora el triple\n",
        "        triplet = [anchor, positive, negative]\n",
        "\n",
        "        # Aplicamos la transformacion dada al dataset al ejemplo que devolvemos\n",
        "        if self.transform:\n",
        "            triplet = [self.transform(np.array(img)) for img in triplet]\n",
        "\n",
        "        return triplet\n",
        "\n",
        "    def __precompute_list_of_classes(self) -> List[List[int]]:\n",
        "        \"\"\"\n",
        "        Calcula la lista con las listas de posiciones de cada clase por separado\n",
        "        \"\"\"\n",
        "        # Inicializamos la lista de listas\n",
        "        posiciones_clases = [[] for _ in range(10)]\n",
        "\n",
        "        # Recorremos el dataset y colocamos los indices donde corresponde\n",
        "        for idx, element in enumerate(self.base_data):\n",
        "            _, img_class = element\n",
        "            posiciones_clases[img_class].append(idx)\n",
        "\n",
        "        return posiciones_clases\n",
        "\n",
        "    \n",
        "class CustomReshape(object):\n",
        "    \"\"\"Pasamos la imagen de (28, 1, 28) a (28, 28)\"\"\"\n",
        "\n",
        "    def __call__(self, image):\n",
        "        image = image.reshape(28, 28)\n",
        "        return image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2b4cbf68",
      "metadata": {
        "id": "2b4cbf68"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
        "   \n",
        "    # Antes de modificar la base de datos para convertirla a triples\n",
        "    # la guardamos, porque mas adelante nos hara falta\n",
        "    old_train_dataset = train_dataset\n",
        "    old_test_dataset = test_dataset\n",
        "\n",
        "    # Necesitamos hacer reshape de las imagenes para que\n",
        "    # sean (28, 28) y no (28, 1, 28)\n",
        "    transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "\n",
        "        # Hacemos reshape de las imagenes para\n",
        "        # que sean tensores (28, 28)\n",
        "        CustomReshape(),\n",
        "    ])\n",
        "\n",
        "    # Generamos los triples aleatorios para training\n",
        "    random_triplets_train = RandomTriplets(\n",
        "        base_data = train_dataset,\n",
        "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
        "        transform = transform,\n",
        "    )\n",
        "\n",
        "    train_loader = torch.utils.data.DataLoader(\n",
        "        random_triplets_train,\n",
        "        batch_size = DATALOADER_BACH_SIZE,\n",
        "        shuffle = True,\n",
        "        num_workers = NUM_WORKERS,\n",
        "        pin_memory = True,\n",
        "    )\n",
        "\n",
        "    # Generamos los triples aleatorios para validacion\n",
        "    random_triplets_validation = RandomTriplets(\n",
        "        base_data = validation_dataset,\n",
        "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
        "        transform = transform,\n",
        "    )\n",
        "\n",
        "    validation_loader = torch.utils.data.DataLoader(\n",
        "        random_triplets_validation,\n",
        "        batch_size = DATALOADER_BACH_SIZE,\n",
        "        shuffle = True,\n",
        "        num_workers = NUM_WORKERS,\n",
        "        pin_memory = True,\n",
        "    )\n",
        "\n",
        "    # Generamos los triples aleatorios para test\n",
        "    random_triplets_test = RandomTriplets(\n",
        "        base_data = test_dataset,\n",
        "        custom_len = RANDOM_TRIPLETS_DATA_SIZE,\n",
        "        transform = transform,\n",
        "    )\n",
        "\n",
        "    test_loader = torch.utils.data.DataLoader(\n",
        "        random_triplets_test,\n",
        "        batch_size = DATALOADER_BACH_SIZE,\n",
        "        shuffle = True,\n",
        "        num_workers = NUM_WORKERS,\n",
        "        pin_memory = True,\n",
        "    )\n",
        "\n",
        "    # Visualizamos algunos triples aleatorios para comprobar el funcionamiento\n",
        "    custom_triplet = random_triplets_train[2]\n",
        "    for i in custom_triplet :\n",
        "        show_img(i, color_format_range = (-1.0, 1.0))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b0c7d0",
      "metadata": {
        "id": "58b0c7d0"
      },
      "source": [
        "## Entrenamiento del modelo"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "569ed64d",
      "metadata": {
        "id": "569ed64d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6e61057c-ec03-488c-b0a0-9eaf89fc645b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightModel(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc): Linear(in_features=3200, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = LightModel(EMBEDDING_DIMENSION)\n",
        "\n",
        "# TODO -- fijar bien los parametros\n",
        "parameters = dict()\n",
        "parameters[\"epochs\"] = TRAINING_EPOCHS_RANDOM\n",
        "parameters[\"lr\"] = RANDOM_TRIPLET_LEARNING_RATE\n",
        "parameters[\"criterion\"] = MeanTripletBatchTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS)\n",
        "\n",
        "# Definimos el logger que queremos para el entrenamiento\n",
        "logger = TripletLoggerOffline(\n",
        "    net = net,\n",
        "    iterations = 10 * DATALOADER_BACH_SIZE,\n",
        "    loss_func = parameters[\"criterion\"],\n",
        ")\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5bc94a2d",
      "metadata": {
        "id": "5bc94a2d"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
        "\n",
        "    training_history = train_model_offline(\n",
        "        net = net,\n",
        "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
        "        parameters = parameters,\n",
        "        train_loader = train_loader,\n",
        "        validation_loader = validation_loader,\n",
        "        name = \"SiameseNetwork\",\n",
        "        logger = logger,\n",
        "        snapshot_iterations = None\n",
        "    )"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "f312e365",
      "metadata": {
        "id": "f312e365"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False: show_learning_curve(training_history)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5cedfac4",
      "metadata": {
        "id": "5cedfac4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd0e1e13-3a04-420a-a9ea-351e4adddc3d"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "LightModel(\n",
              "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
              "  (fc): Linear(in_features=3200, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "# A partir de este punto ya no volvemos a entrenar el modelo\n",
        "# Asi que lo ponemos en modo evaluacion para que no lleve \n",
        "# la cuenta de los gradientes\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "967f2783",
      "metadata": {
        "id": "967f2783"
      },
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "- Mostramos algunas métricas fundamentales sobre el conjunto de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "c60be0c4",
      "metadata": {
        "id": "c60be0c4"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
        "    with torch.no_grad(): core.test_model(net, test_loader, parameters[\"criterion\"])"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We use the adapter to classification task to have more metrics about the model:"
      ],
      "metadata": {
        "id": "oOPlWZWYu7cR"
      },
      "id": "oOPlWZWYu7cR"
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "29f14b37",
      "metadata": {
        "id": "29f14b37",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
        "        # preparado para descargar\n",
        "        train_dataset = old_train_dataset\n",
        "        test_dataset = old_test_dataset\n",
        "    \n",
        "        # Data loaders para acceder a los datos\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_dataset,\n",
        "            batch_size = DATALOADER_BACH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "        test_loader = torch.utils.data.DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size = DATALOADER_BACH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "    \n",
        "    \n",
        "        classifier = EmbeddingToClassifier(\n",
        "            net, \n",
        "            k = NUMBER_NEIGHBOURS, \n",
        "            data_loader = train_loader,\n",
        "            embedding_dimension = EMBEDDING_DIMENSION\n",
        "        )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ae61cab8",
      "metadata": {
        "id": "ae61cab8"
      },
      "source": [
        "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "f2b77a7a",
      "metadata": {
        "id": "f2b77a7a"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        # Hacemos esto y no `in test_dataset[:max_iterations]`\n",
        "        # para no tener que tomar todo el dataset y quedarnos con\n",
        "        # una parte de el, que es un proceso mucho mas lento que usar\n",
        "        # el iterador que da `in test_dataset` y parar con el contador\n",
        "        counter = 0\n",
        "        max_iterations = 20\n",
        "    \n",
        "        for img, img_class in test_dataset:\n",
        "            predicted_class = classifier.predict(img)\n",
        "            print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
        "    \n",
        "            # Actualizamos el contador\n",
        "            counter += 1\n",
        "            if counter == max_iterations: break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d4def6fb",
      "metadata": {
        "id": "d4def6fb"
      },
      "source": [
        "## Plot del embedding\n",
        "\n",
        "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
        "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "47c93584",
      "metadata": {
        "id": "47c93584"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False: \n",
        "    with torch.no_grad(): classifier.scatter_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bfcedc5e",
      "metadata": {
        "id": "bfcedc5e"
      },
      "source": [
        "## Evaluación del clasificador obtenido\n",
        "\n",
        "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "id": "ef6bcfa1",
      "metadata": {
        "id": "ef6bcfa1"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "\n",
        "def calculate_roc_auc(true_labels_prob: np.array, predicted_labels_prob: np.array) -> float:\n",
        "    \"\"\"\n",
        "    Calcula el area bajo la curva ROC, dadas las etiquetas verdaderas y las\n",
        "    etiqeutas predichas por un modelo\n",
        "    \n",
        "    Las listas de etiquetas deben ser etiquetas probabilisticas\n",
        "    \"\"\"\n",
        "    return roc_auc_score(true_labels_prob, predicted_labels_prob, multi_class = \"ovo\")\n",
        "    \n",
        "def calculate_accuracy(true_labels: np.array, predicted_labels: np.array) -> float:\n",
        "    \"\"\"\n",
        "    Calcula el accuracy, dadas las etiquetas verdaderas y las\n",
        "    etiqeutas predichas por un modelo\n",
        "    \"\"\"\n",
        "    return accuracy_score(true_labels, predicted_labels)\n",
        "\n",
        "def calculate_silhouette(x, y):\n",
        "    \"\"\"Calcula el indice de silhouette para el embedding calculado por el modelo\"\"\"\n",
        "    return silhouette_score(x, y)\n",
        "\n",
        "def evaluate_model(model, train_loader, test_loader) -> dict:\n",
        "    \"\"\"\n",
        "    Evalua, usando distintas metricas, el modelo que hemos entrenado\n",
        "    Tambien evaluamos el embedding obtenido, no solo el clasificador\n",
        "    \n",
        "    Devuelve un diccionario con todas las metricas calculadas con el modelo\n",
        "    \"\"\"\n",
        "    \n",
        "    # Diccionario en el que vamos a almacenar todas las metricas\n",
        "    metrics = dict()\n",
        "    \n",
        "    # Tomamos los arrays en formato adecuado para calcular ciertas metricas\n",
        "    x_train, y_train = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
        "    x_test, y_test = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = test_loader, embedding_dimension = EMBEDDING_DIMENSION).prepare_data_for_sklearn()\n",
        "    \n",
        "    # Empezamos usando el modelo para realizar las predicciones+\n",
        "    # Usamos predicciones probabilisticas pues estas son fundamentales para\n",
        "    # la metrica roc auc\n",
        "    train_predicted_labels_prob = model.predict_proba_using_embedding(x_train)\n",
        "    test_predicted_labels_prob = model.predict_proba_using_embedding(x_test)\n",
        "\n",
        "    # Tomamos ahora las etiqeutas sin probabilidad\n",
        "    train_predicted_labels= model.predict_using_embedding(x_train)\n",
        "    test_predicted_labels = model.predict_using_embedding(x_test)\n",
        "    \n",
        "    # Tomamos las metricas de accuracy\n",
        "    metrics[\"train_acc\"] = calculate_accuracy(y_train, train_predicted_labels)\n",
        "    metrics[\"test_acc\"] = calculate_accuracy(y_test, test_predicted_labels)\n",
        "    \n",
        "    # Tomamos las areas bajo la curva ROC\n",
        "    metrics[\"train_roc_auc\"] = calculate_roc_auc(y_train, train_predicted_labels_prob)\n",
        "    metrics[\"test_roc_auc\"] = calculate_roc_auc(y_test, test_predicted_labels_prob)\n",
        "    \n",
        "    # Tomamos el indice de silhouette\n",
        "    metrics[\"train_silhouette\"] = calculate_silhouette(x_train, y_train)\n",
        "    metrics[\"test_silhouette\"] = calculate_silhouette(x_test, y_test)\n",
        "    \n",
        "    #Calculamos Matriz de confusion\n",
        "    metrics[\"train_confusion_matrix\"]=confusion_matrix(y_train, train_predicted_labels)\n",
        "    metrics[\"test_confusion_matrix\"]=confusion_matrix(y_test, test_predicted_labels)\n",
        "    \n",
        "    # Devolvemos las metricas en formato diccionario, que nos va a ser comodo para\n",
        "    # pasarlas a tablas y para mostrar muchas metricas simultaneamente\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def plot_confusion_matrix(train_confusion_matrix,test_confusion_matrix):\n",
        "    \"\"\"\n",
        "    Mostramos graficamente (mapa de calor) la matriz de confusion\n",
        "    \n",
        "    Podriamos mostrar directamente los valores de la matriz, pero al estar\n",
        "    trabajando con 25 clases es mas dificil de interpretar\n",
        "    \"\"\"\n",
        "    \n",
        "    # Mostramos graficamente las dos matrices de confusion\n",
        "    color_palette = sns.color_palette(\"viridis\", as_cmap=True)\n",
        "    print(\"Matriz de confusion en TRAIN\")\n",
        "    sns.heatmap(train_confusion_matrix, annot = False, cmap = color_palette, fmt=\"d\")\n",
        "    plt.show()\n",
        "    \n",
        "    print(\"Matriz de confusion en TEST\")\n",
        "    sns.heatmap(test_confusion_matrix, annot = False, cmap = color_palette)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "f7dc2fcb",
      "metadata": {
        "id": "f7dc2fcb"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos ejecutar esta seccion o no \n",
        "if SKIP_RANDOM_TRIPLETS_TRAINING is False:\n",
        "    with torch.no_grad():\n",
        "    \n",
        "        classifier.embedder.set_permute(False)\n",
        "    \n",
        "        metrics = evaluate_model(classifier, train_loader, test_loader)\n",
        "        pprint(metrics)\n",
        "    \n",
        "        classifier.embedder.set_permute(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Online Batch Triplets"
      ],
      "metadata": {
        "id": "OXxDVsblof-O"
      },
      "id": "OXxDVsblof-O"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Choose the loss function to use \n",
        "\n",
        "- We have so many combinations for loss functions that is not feasible to use one Colab section for each\n",
        "- Combinations depend on:\n",
        "    1. Batch hard vs Batch all \n",
        "    2. Classical triplet loss vs Softplus loss\n",
        "    3. All summands mean vs Only > 0 summands mean"
      ],
      "metadata": {
        "id": "IDZBu1P-o4TM"
      },
      "id": "IDZBu1P-o4TM"
    },
    {
      "cell_type": "code",
      "source": [
        "batch_loss_function = None\n",
        "if BATCH_TRIPLET_LOSS_FUNCTION == \"hard\":\n",
        "    batch_loss_function = BatchHardTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean = USE_GT_ZERO_MEAN_LOSS) \n",
        "if BATCH_TRIPLET_LOSS_FUNCTION == \"all\":\n",
        "    batch_loss_function = BatchAllTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean =  USE_GT_ZERO_MEAN_LOSS) \n",
        "\n",
        "# Sanity check\n",
        "if batch_loss_function is None:\n",
        "    raise Exception(f\"BATCH_TRIPLET_LOSS global parameter got unexpected value: {BATCH_TRIPLET_LOSS_FUNCTION}\")"
      ],
      "metadata": {
        "id": "lU4wWC-GpHdI"
      },
      "id": "lU4wWC-GpHdI",
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "7378947c",
      "metadata": {
        "id": "7378947c"
      },
      "source": [
        "## Volvemos a cargar los datos\n",
        "\n",
        "- Con esta función de pérdida ya no necesitamos calcular de forma offline los triples\n",
        "- Así que volvemos a cargar el dataset original, sin el pre-cómputo de triples"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "id": "8425cd40",
      "metadata": {
        "id": "8425cd40"
      },
      "outputs": [],
      "source": [
        "# Transformaciones que queremos aplicar al cargar los datos\n",
        "# Ahora solo pasamos las imagenes a tensores, pero podriamos hacer aqui normalizaciones\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    # TODO -- aqui podemos añadir la normaliazcion de datos\n",
        "])\n",
        "\n",
        "# Cargamos el dataset usando torchvision, que ya tiene el conjunto\n",
        "# preparado para descargar\n",
        "train_dataset = torchvision.datasets.MNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = True,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.MNIST(\n",
        "    root = DATA_PATH,\n",
        "    train = False,\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "# Separamos train en train y validacion\n",
        "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
        "\n",
        "# Data loaders para acceder a los datos\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = ONLINE_BATCH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size = ONLINE_BATCH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS,\n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size = ONLINE_BATCH_SIZE,\n",
        "  shuffle = True,\n",
        "  num_workers = NUM_WORKERS,\n",
        "  pin_memory = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5593607e",
      "metadata": {
        "id": "5593607e"
      },
      "source": [
        "## Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "id": "867bd047",
      "metadata": {
        "id": "867bd047"
      },
      "outputs": [],
      "source": [
        "def custom_cross_validation(net, parameters, train_dataset, k):\n",
        "    \"\"\"Funcion propia para hacer k-fold cross validation de una red convolucional\"\"\"\n",
        "\n",
        "    # Definimos la forma en la que vamos a hacer el split de los folds\n",
        "    ss = ShuffleSplit(n_splits=k, test_size=0.25, random_state=RANDOM_SEED)\n",
        "    \n",
        "    # Lista en la que guardamos las perdidas encontradas en cada fold\n",
        "    losses = []\n",
        "\n",
        "    # Iteramos usando el split que nos da sklearn\n",
        "    for train_index, validation_index in ss.split(train_dataset):\n",
        "        \n",
        "        # Tenemos los indices de los elementos, asi que tomamos los dos datasets\n",
        "        # usando dichos indices\n",
        "        train_fold = [train_dataset[idx] for idx in train_index]\n",
        "        validation_fold = [train_dataset[idx] for idx in validation_index]\n",
        "\n",
        "        # Transformamos los datasets en dataloaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_fold,\n",
        "            batch_size = ONLINE_BATCH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "        validation_loader = torch.utils.data.DataLoader(\n",
        "            validation_fold,\n",
        "            batch_size = ONLINE_BATCH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "\n",
        "        # Entrenamos la red\n",
        "        _ = train_model_online(\n",
        "            net = net,\n",
        "            path = os.path.join(BASE_PATH, \"tmp\"),\n",
        "            parameters = parameters,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            name = \"SiameseNetworkOnline\",\n",
        "            logger = SilentLogger(),\n",
        "            snapshot_iterations = None\n",
        "        )\n",
        "\n",
        "        # Evaluamos la red en el fold de validacion\n",
        "        net.eval()\n",
        "        loss = metrics.calculate_mean_triplet_loss_online(net, validation_loader, parameters[\"criterion\"], 1.0)\n",
        "        loss = float(loss) # Pasamos el tensor de un unico elemento a un float simple\n",
        "\n",
        "        # Añadimos el loss a nuestra lista\n",
        "        losses.append(loss)\n",
        "    \n",
        "    # Devolvemos el array en formato numpy para que sea mas comodo trabajar con ella\n",
        "    return np.array(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "id": "292f5ea4",
      "metadata": {
        "id": "292f5ea4"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos realizar el hyperparameater tuning o no\n",
        "if SKIP_HYPERPARAMTER_TUNING is False:\n",
        "\n",
        "    # Para controlar que parametros ya hemos explorado y queremos saltar\n",
        "    already_explored_parameters = [\n",
        "        # Embedding dimension, learning rate, margin\n",
        "        (2, 0.0001, 0.01),\n",
        "        (2, 0.0001, 1),\n",
        "        (3, 0.0001),\n",
        "    ]\n",
        "\n",
        "    # Parametros que queremos mover\n",
        "    #margin_values = [0.01, 0.1, 1.0]\n",
        "    # TODO -- volver a poner todos los valores\n",
        "    margin_values = [1.0]\n",
        "    learning_rate_values = [0.0001, 0.001, 0.01]\n",
        "    embedding_dimension_values = [2, 3, 4]\n",
        "    \n",
        "    # Parametros que fijamos de antemano\n",
        "    epochs = HYPERPARAMETER_TUNING_EPOCHS\n",
        "    \n",
        "    # Llevamos la cuenta de los mejores parametros y el mejor error encontrados hasta\n",
        "    # el momento\n",
        "    best_loss = None\n",
        "    best_parameters = {\n",
        "        \"embedding_dimension\": None,\n",
        "        \"lr\": None,\n",
        "        \"margin\": None\n",
        "    }\n",
        "    \n",
        "    # Exploramos las combinaciones de parametros\n",
        "    for margin in margin_values:\n",
        "        for learning_rate in learning_rate_values:\n",
        "            for embedding_dimension in embedding_dimension_values:\n",
        "        \n",
        "                print(f\"Optimizando para margin: {margin}, lr: {learning_rate}, embedding_dim: {embedding_dimension}\")\n",
        "\n",
        "                # Comprobamos si tenemos que saltarnos el calculo de algun valor\n",
        "                # porque ya se haya hecho\n",
        "                if (embedding_dimension, learning_rate, margin) in already_explored_parameters:\n",
        "                    print(\"\\tSaltando este calculo porque ya se realizo\")\n",
        "                    continue\n",
        "                \n",
        "                # Definimos el modelo que queremos optimizar\n",
        "                net = ResNet18(embedding_dimension)\n",
        "                \n",
        "                # En este caso, al no estar trabajando con los minibatches\n",
        "                # (los usamos directamente como nos los da pytorch), no tenemos\n",
        "                # que manipular los tensores\n",
        "                net.set_permute(False)\n",
        "                \n",
        "                parameters = dict()\n",
        "                parameters[\"epochs\"] = epochs\n",
        "                parameters[\"lr\"] = learning_rate\n",
        "                parameters[\"criterion\"] = BatchHardTripletLoss(margin)\n",
        "                logger = SilentLogger()\n",
        "    \n",
        "                # Usamos nuestra propia funcion de cross validation para validar el modelo\n",
        "                losses = custom_cross_validation(net, parameters, train_dataset, k = NUMBER_OF_FOLDS) \n",
        "                print(f\"El loss conseguido es {losses.mean()}\")\n",
        "                print(\"\")\n",
        "            \n",
        "                # Comprobamos si hemos mejorado la funcion de perdida\n",
        "                # En cuyo caso, actualizamos nuestra estructura de datos y, sobre todo, mostramos\n",
        "                # por pantalla los nuevos mejores parametros\n",
        "                basic_condition = math.isnan(losses.mean()) is False             # Si es NaN no entramos al if\n",
        "                enter_condition = best_loss is None or losses.mean() < best_loss # Entramos al if si mejoramos la perdida\n",
        "                compound_condition = basic_condition and enter_condition\n",
        "                if compound_condition:\n",
        "                \n",
        "                    # Actualizamos nuestra estructura de datos\n",
        "                    best_loss = losses.mean()\n",
        "                    best_parameters = {\n",
        "                        \"embedding_dimension\": embedding_dimension,\n",
        "                        \"lr\": learning_rate,\n",
        "                        \"margin\": margin,\n",
        "                    }\n",
        "            \n",
        "                    # Mostramos el cambio encontrado\n",
        "                    print(\"==> ENCONTRADOS NUEVOS MEJORES PARAMETROS\")\n",
        "                    print(f\"Mejores parametros: {best_parameters}\")\n",
        "                    print(f\"Mejor loss: {best_loss}\")\n",
        "            \n",
        "                \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1ee8ba",
      "metadata": {
        "id": "3e1ee8ba"
      },
      "source": [
        "## Entrenamiento online"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "id": "d7a2afd9",
      "metadata": {
        "id": "d7a2afd9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c514514b-1c16-456b-fab0-92bcd5e5b72c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LightModel(\n",
            "  (conv1): Conv2d(1, 4, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(4, 8, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv3): Conv2d(8, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv4): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc): Linear(in_features=3200, out_features=2, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "net = LightModel(EMBEDDING_DIMENSION)\n",
        "\n",
        "# En este caso, al no estar trabajando con los minibatches\n",
        "# (los usamos directamente como nos los da pytorch), no tenemos\n",
        "# que manipular los tensores\n",
        "net.set_permute(False)\n",
        "\n",
        "# Parameters of the training\n",
        "parameters = dict()\n",
        "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
        "parameters[\"lr\"] = ONLINE_LEARNING_RATE\n",
        "\n",
        "# We use the loss function that depends on the global parameter BATCH_TRIPLET_LOSS_FUNCTION\n",
        "parameters[\"criterion\"] = batch_loss_function\n",
        "\n",
        "# Definimos el logger que queremos para el entrenamiento\n",
        "logger = TripletLoggerOnline(\n",
        "    net = net,\n",
        "    iterations = -1,\n",
        "    loss_func = parameters[\"criterion\"],\n",
        "    train_percentage = 0.01,\n",
        "    validation_percentage = 1.0,\n",
        ")\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "id": "aa722a06",
      "metadata": {
        "id": "aa722a06",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 588
        },
        "outputId": "a0823c2f-6bbf-46ea-f5c9-42b3a0abbc14"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "==> Training on device cuda:0\n",
            "\n",
            "[0 / 0]\n",
            "[0 / 512]\n",
            "[0 / 1024]\n",
            "[0 / 1536]\n",
            "[0 / 2048]\n",
            "[0 / 2560]\n",
            "[0 / 3072]\n",
            "[0 / 3584]\n",
            "[0 / 4096]\n",
            "[0 / 9]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-25-6be2c6a5e290>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0mname\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"SiameseNetworkOnline\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m         \u001b[0msnapshot_iterations\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     ))\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/core.py\u001b[0m in \u001b[0;36mtrain_model_online\u001b[0;34m(net, path, parameters, train_loader, validation_loader, name, logger, snapshot_iterations)\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mcurrent_seen_batches\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m                 \u001b[0;31m# Log and return loss from training and validation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mtraining_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog_process\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m                 \u001b[0;31m# Save loss of training and validation sets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/train_loggers.py\u001b[0m in \u001b[0;36mlog_process\u001b[0;34m(self, train_loader, validation_loader, epoch, iteration)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m             \u001b[0;31m# Funcion de perdida en validacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 246\u001b[0;31m             \u001b[0mmean_val_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmetric\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalidation_max_examples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    247\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    248\u001b[0m         \u001b[0;31m# Volvemos a poner la red en modo entrenamiento\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/metrics.py\u001b[0m in \u001b[0;36mcalculate_mean_triplet_loss_online\u001b[0;34m(net, data_loader, loss_function, max_examples)\u001b[0m\n\u001b[1;32m    155\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         \u001b[0;31m# Calculate loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 157\u001b[0;31m         \u001b[0macumulated_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0;31m# Update seen examples\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1101\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1102\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1103\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1104\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/loss_functions.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, embeddings, labels)\u001b[0m\n\u001b[1;32m    265\u001b[0m             negative_distances = [\n\u001b[1;32m    266\u001b[0m                 \u001b[0mdistance_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_of_negatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m             ]\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/loss_functions.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    265\u001b[0m             negative_distances = [\n\u001b[1;32m    266\u001b[0m                 \u001b[0mdistance_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membeddings\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnegative\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 267\u001b[0;31m                 \u001b[0;32mfor\u001b[0m \u001b[0mnegative\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_of_negatives\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mimg_label\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    268\u001b[0m             ]\n\u001b[1;32m    269\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/loss_functions.py\u001b[0m in \u001b[0;36mdistance_function\u001b[0;34m(first, second)\u001b[0m\n\u001b[1;32m     15\u001b[0m     \u001b[0mBasic\u001b[0m \u001b[0mdistance\u001b[0m \u001b[0mfunction\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mIt\u001b[0m\u001b[0;31m'\u001b[0m\u001b[0ms\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mbase\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mall\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0mimplemented\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mthis\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \"\"\"\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mfirst\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0msecond\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# Entrenamos solo si lo especifica\n",
        "# el parametro que controla el uso de cache\n",
        "if USE_CACHED_MODEL is False:\n",
        "\n",
        "    # Para saber cuanto tarda\n",
        "    ts = time.time()\n",
        "\n",
        "    torch.jit.script(\n",
        "    training_history = train_model_online(\n",
        "        net = net,\n",
        "        path = os.path.join(BASE_PATH, \"tmp\"),\n",
        "        parameters = parameters,\n",
        "        train_loader = train_loader,\n",
        "        validation_loader = validation_loader,\n",
        "        name = \"SiameseNetworkOnline\",\n",
        "        logger = logger,\n",
        "        snapshot_iterations = None\n",
        "    ))\n",
        "\n",
        "    # Calculamos cuanto ha tardado\n",
        "    te = time.time()\n",
        "    print(f\"Ha tardado {te - ts}\")\n",
        "    \n",
        "    # Actualizamos la cache del modelo\n",
        "    filesystem.save_model(net, MODEL_CACHE_FOLDER, \"online_model_cached\")\n",
        "    \n",
        "# Nos saltamos el entrenamiento y cargamos el modelo desde la cache\n",
        "else:\n",
        "       \n",
        "    net = filesystem.load_model(\n",
        "        os.path.join(MODEL_CACHE_FOLDER, \"online_model_cached\"), \n",
        "        lambda: ResNet18(EMBEDDING_DIMENSION)\n",
        "    )\n",
        "    \n",
        "    # Tenemos que cargar la red en la memoria correspondiente\n",
        "    device = core.get_device()\n",
        "    net.to(device)\n",
        "    \n",
        "# A partir de este punto no hacemos entrenamiento\n",
        "# asi que ponemos la red en modo evaluacion para que \n",
        "# no vaya almacenando los gradientes\n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae35995",
      "metadata": {
        "id": "3ae35995"
      },
      "outputs": [],
      "source": [
        "# Entrenamos solo si lo especifica\n",
        "# el parametro que controla el uso de cache\n",
        "if USE_CACHED_MODEL is False:\n",
        "        \n",
        "    # Al principio tenemos un pico de error que hace \n",
        "    # que la grafica no sea legible. Ignoramos las primeras metricas\n",
        "    # de error para que la grafica sea legible\n",
        "    cut = 5\n",
        "    training_history[\"loss\"] = training_history[\"loss\"][cut:]\n",
        "    training_history[\"val_loss\"] = training_history[\"val_loss\"][cut:]\n",
        "\n",
        "    show_learning_curve(training_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68b8fef",
      "metadata": {
        "id": "d68b8fef"
      },
      "source": [
        "## Evaluación del modelo\n",
        "\n",
        "- Mostramos algunas métricas fundamentales sobre el conjunto de"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5210628c",
      "metadata": {
        "id": "5210628c"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    net.set_permute(False)\n",
        "    \n",
        "    core.test_model_online(net, test_loader, parameters[\"criterion\"], online = True)\n",
        "    \n",
        "    net.set_permute(True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e58d6f",
      "metadata": {
        "id": "f2e58d6f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    try_to_clean_memory()\n",
        "    classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader, embedding_dimension = EMBEDDING_DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e8ad35f7",
      "metadata": {
        "id": "e8ad35f7"
      },
      "source": [
        "Evaluamos este clasificador en un conjunto pequeño de imágenes de test. Más adelante tomamos métricas de dicho clasificador:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46acde43",
      "metadata": {
        "id": "46acde43"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    # Hacemos esto y no `in test_dataset[:max_iterations]`\n",
        "    # para no tener que tomar todo el dataset y quedarnos con\n",
        "    # una parte de el, que es un proceso mucho mas lento que usar\n",
        "    # el iterador que da `in test_dataset` y parar con el contador\n",
        "    counter = 0\n",
        "    max_iterations = 20\n",
        "    \n",
        "    for img, img_class in test_dataset:\n",
        "        predicted_class = classifier.predict(img)\n",
        "        print(f\"Etiqueta verdadera: {img_class}, etiqueta predicha: {predicted_class[0]}\")\n",
        "    \n",
        "        # Actualizamos el contador\n",
        "        counter += 1\n",
        "        if counter == max_iterations: break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fa44a0",
      "metadata": {
        "id": "58fa44a0"
      },
      "source": [
        "## Plot del embedding\n",
        "\n",
        "- Aprovechamos el cálculo realizado en la clase que realiza la adaptación a clasificación para mostrar gráficamente el embedding calculado\n",
        "- Esta gráfica solo la visualizamos cuando el embedding tiene dimensión 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab56fa9",
      "metadata": {
        "id": "8ab56fa9"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    classifier.scatter_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83cea66d",
      "metadata": {
        "id": "83cea66d"
      },
      "source": [
        "## Evaluación del clasificador obtenido\n",
        "\n",
        "- Ahora que hemos adaptado el modelo para usarlo como clasificador, podemos consultar ciertas métricas de clasificación"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4ae22",
      "metadata": {
        "id": "27b4ae22"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    try_to_clean_memory()\n",
        "    classifier.embedder.set_permute(False)\n",
        "    \n",
        "    metrics = evaluate_model(classifier, train_loader, test_loader)\n",
        "    pprint(metrics)\n",
        "    \n",
        "    classifier.embedder.set_permute(True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Notebook.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}