{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "SCUXXp1GrTvw",
      "metadata": {
        "id": "SCUXXp1GrTvw"
      },
      "source": [
        "# Labeled Faces in the Wild (LFW)\n",
        "\n",
        "- From the Notebook used for *MNIST dataset*, we try to solve identification for *Labeled Faces in the Wild dataset*\n",
        "    - We're loading the data from the pytorch implementation of this dataset\n",
        "    - But more information about this dataset can be found [the official website](http://vis-www.cs.umass.edu/lfw/)\n",
        "    - In that website, you can download the dataset in its original form\n",
        "- **Main differences from MNIST notebook**\n",
        "    - Obviously, the dataset is not the same\n",
        "    - Also, as we're solving a much harder dataset, and online triplet loss has been tested in *MNIST notebook*, we get rid of random triplets experiments "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58b07228",
      "metadata": {
        "id": "58b07228"
      },
      "source": [
        "\n",
        "# Global Parameters of the Notebook"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f955a4a9-088e-43ea-820e-05846a99463f",
      "metadata": {
        "id": "f955a4a9-088e-43ea-820e-05846a99463f"
      },
      "source": [
        "## Paths\n",
        "\n",
        "- Parameters related to data / model / lib paths"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "51aad6c9-1243-4de4-b799-e0a9ab8e7149",
      "metadata": {
        "id": "51aad6c9-1243-4de4-b799-e0a9ab8e7149"
      },
      "outputs": [],
      "source": [
        "# Lib to define paths\n",
        "import os\n",
        "\n",
        "# Define if we are running the notebook in our computer (\"local\")\n",
        "# or in Google Colab (\"remote\")\n",
        "RUNNING_ENV = \"remote\"\n",
        "\n",
        "# Base path for the rest of paths defined in the notebook\n",
        "BASE_PATH = \"./\" if RUNNING_ENV == \"local\" else \"/content/drive/MyDrive/Colab Notebooks/\"\n",
        "\n",
        "# Path to our lib dir\n",
        "LIB_PATH = os.path.join(BASE_PATH, \"lib\")\n",
        "\n",
        "# Path where we store training / test data\n",
        "DATA_PATH = os.path.join(BASE_PATH, \"data\")\n",
        "\n",
        "# Dir with all cached models \n",
        "# This cached models can be loaded from disk when training is skipped\n",
        "MODEL_CACHE_FOLDER = os.path.join(BASE_PATH, \"cached_models\")\n",
        "\n",
        "# Cache for the augmented dataset\n",
        "AUGMENTED_DATASET_CACHE_FILE = os.path.join(BASE_PATH, \"cached_augmented_dataset.pt\")\n",
        "\n",
        "# File where the logs are written\n",
        "LOGGING_FILE = os.path.join(BASE_PATH, \"training.log\")\n",
        "\n",
        "# Binary file where the stats of the profiling are saved\n",
        "PROFILE_SAVE_FILE = os.path.join(BASE_PATH, \"training_profile.stat\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28",
      "metadata": {
        "id": "8c97a39c-0f5a-4ad7-b2b6-6d6ac3d9ea28"
      },
      "source": [
        "## ML parameters\n",
        "\n",
        "- Parameters related to machine learning\n",
        "- For example, batch sizes, learning rates, ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "021a8a47-2dbb-4ef7-8045-496ff228a4ab",
      "metadata": {
        "id": "021a8a47-2dbb-4ef7-8045-496ff228a4ab"
      },
      "outputs": [],
      "source": [
        "# Parameters of P-K sampling\n",
        "P = 100   # Number of classes used in each minibatch\n",
        "K = 2     # Number of images sampled for each selected class\n",
        "\n",
        "# Batch size for online training \n",
        "# We can use `P * K` as batch size. Thus, minibatches will be\n",
        "# as we expect in P-K sampling. \n",
        "# \n",
        "# But we can use `n * P * K`. Thus, minibatches will be n P-K sampling\n",
        "# minibatche concatenated together\n",
        "# Be careful when doing this because it can be really slow, and there is no\n",
        "# clear reason to do this\n",
        "ONLINE_BATCH_SIZE = P * K\n",
        "\n",
        "# Epochs for hard triplets, online training \n",
        "TRAINING_EPOCHS = 1\n",
        "\n",
        "# Learning rate for hard triplets, online training\n",
        "ONLINE_LEARNING_RATE = 0.01\n",
        "\n",
        "# How many single elements we want to see before logging \n",
        "# It has to be a multiple of P * K, otherwise `should_log` would return always \n",
        "# false as `it % LOGGING_ITERATIONS != 0` always\n",
        "#\n",
        "# `LOGGING_ITERATIONS = P * K * n` means we log after seeing `n` P-K sampled\n",
        "# minibatches\n",
        "LOGGING_ITERATIONS = P * K * 20\n",
        "\n",
        "# Which percentage of the training and validation set we want to use for the logging\n",
        "ONLINE_LOGGER_TRAIN_PERCENTAGE = 1 / 5\n",
        "ONLINE_LOGGER_VALIDATION_PERCENTAGE = 1 / 3\n",
        "\n",
        "# Choose which model we're going to use\n",
        "# Can be \"ResNet18\", \"LightModel\" or \"LFWResNet18\"\n",
        "NET_MODEL = \"LFWResNet18\"\n",
        "\n",
        "# Epochs used in k-Fold Cross validation \n",
        "# k-Fold Cross validation used for parameter exploration\n",
        "HYPERPARAMETER_TUNING_EPOCHS = 7\n",
        "\n",
        "# Number of folds used in k-fold Cross Validation\n",
        "NUMBER_OF_FOLDS = 4\n",
        "\n",
        "# Margin used in the loss function\n",
        "MARGIN = 1.0\n",
        "\n",
        "# Dim of the embedding calculated by the network\n",
        "EMBEDDING_DIMENSION = 5\n",
        "\n",
        "# Number of neighbours considered in K-NN\n",
        "# K-NN used for transforming embedding task to classification task \n",
        "NUMBER_NEIGHBOURS = 3\n",
        "\n",
        "# Batch Triplet Loss Function\n",
        "# This way we can choose among \"hard\", \"all\"\n",
        "BATCH_TRIPLET_LOSS_FUNCTION = \"hard\"\n",
        "\n",
        "# Wether or not use softplus loss function instead of vanilla triplet loss\n",
        "USE_SOFTPLUS_LOSS = False\n",
        "\n",
        "# Count all sumamnds in the mean loss or only those summands greater than zero\n",
        "USE_GT_ZERO_MEAN_LOSS = True\n",
        "\n",
        "# Wether or not use lazy computations in the data augmentation\n",
        "LAZY_DATA_AUGMENTATION = True"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "_O0TCQMuc7-w",
      "metadata": {
        "id": "_O0TCQMuc7-w"
      },
      "source": [
        "## Section parameters"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "066f9a63-cc85-4203-bd4e-29d49eb64339",
      "metadata": {
        "id": "066f9a63-cc85-4203-bd4e-29d49eb64339"
      },
      "source": [
        "- Flags to choose if some sections will run or not\n",
        "- This way we can skip some heavy computations when not needed "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40ef107c-a812-4abb-97d2-8d375cd0e685",
      "metadata": {
        "id": "40ef107c-a812-4abb-97d2-8d375cd0e685"
      },
      "outputs": [],
      "source": [
        "# Skip hyper parameter tuning for online training\n",
        "SKIP_HYPERPARAMTER_TUNING = True\n",
        "\n",
        "# Skip training and use a cached model\n",
        "# Useful for testing the embedding -> classifier transformation\n",
        "# Thus, when False training is not computed and a cached model\n",
        "# is loaded from disk\n",
        "# Cached models are stored in `MODEL_CACHE_FOLDER`\n",
        "USE_CACHED_MODEL = False\n",
        "\n",
        "# Skip data augmentation and use the cached augmented dataset\n",
        "USE_CACHED_AUGMENTED_DATASET = False\n",
        "\n",
        "# Most of the time we're not exploring the data, but doing\n",
        "# either hyperparameter settings or training of the model\n",
        "# So if we skip this step we can start the process faster\n",
        "SKIP_EXPLORATORY_DATA_ANALYSYS = True\n",
        "\n",
        "# Wether or not profile the training \n",
        "# This should be False most of the times\n",
        "# Note that profiling adds a significant overhead to the training\n",
        "PROFILE_TRAINING = False"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Dfnhm9Pt1OCf",
      "metadata": {
        "id": "Dfnhm9Pt1OCf"
      },
      "source": [
        "## WANDB Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "nl1eCSLX1P3p",
      "metadata": {
        "id": "nl1eCSLX1P3p"
      },
      "outputs": [],
      "source": [
        "from datetime import datetime\n",
        "\n",
        "# Name for the project\n",
        "# One project groups different runs\n",
        "WANDB_PROJECT_NAME = \"Labeled Faces in the Wild (LFW) Dataset\"\n",
        "\n",
        "# Name for this concrete run \n",
        "# I don't care too much about it, because wandb tracks the parameters we use \n",
        "# in this run (see \"Configuration for Weights and Biases\" section)\n",
        "WANDB_RUN_NAME = str(datetime.now())"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f109c94c-8595-49d2-a328-9ef9233adf27",
      "metadata": {
        "id": "f109c94c-8595-49d2-a328-9ef9233adf27"
      },
      "source": [
        "## Others"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f735abd1",
      "metadata": {
        "id": "f735abd1"
      },
      "outputs": [],
      "source": [
        "# Number of workers we want to use \n",
        "# We can have less, equal or greater num of workers than CPUs\n",
        "# In the following forum:\n",
        "#   https://discuss.pytorch.org/t/guidelines-for-assigning-num-workers-to-dataloader/813/4\n",
        "# they recomend to explore this parameter, growing it until system RAM saturates\n",
        "# Using a value greater than 2 makes pytorch tell us that this value is not optimal\n",
        "# So sticking with what pytorch tells uss\n",
        "NUM_WORKERS = 2\n",
        "\n",
        "# Fix random seed to make reproducible results\n",
        "RANDOM_SEED = 123456789"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01a1a0d0",
      "metadata": {
        "id": "01a1a0d0"
      },
      "source": [
        "# Auth forGoogle Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b1264a2a",
      "metadata": {
        "id": "b1264a2a"
      },
      "outputs": [],
      "source": [
        "if RUNNING_ENV == \"remote\":\n",
        "    from google.colab import drive\n",
        "    drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "bblM9skNz8_J",
      "metadata": {
        "id": "bblM9skNz8_J"
      },
      "source": [
        "# Pre-installations\n",
        "\n",
        "- Some packages are not installed in the Colab Enviroment\n",
        "- So install them if we're running in Colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "69yRzLZaz8t7",
      "metadata": {
        "id": "69yRzLZaz8t7"
      },
      "outputs": [],
      "source": [
        "if RUNNING_ENV == \"remote\":\n",
        "    !pip install wandb"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d1327dec",
      "metadata": {
        "id": "d1327dec"
      },
      "source": [
        "# Importing the modules we are going to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fc75086e",
      "metadata": {
        "id": "fc75086e"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "import torchvision\n",
        "import torchvision.datasets as datasets\n",
        "\n",
        "# For using pre-trained ResNets\n",
        "import torchvision.models as models \n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import os\n",
        "import logging\n",
        "from datetime import datetime\n",
        "from pprint import pprint\n",
        "import gc\n",
        "import functools\n",
        "import math\n",
        "import seaborn as sns\n",
        "from collections import Counter\n",
        "import time\n",
        "import copy\n",
        "import cProfile\n",
        "\n",
        "import wandb\n",
        "\n",
        "# All concrete pieces we're using form sklearn\n",
        "from sklearn.metrics import roc_auc_score, accuracy_score, silhouette_score\n",
        "from sklearn.model_selection import ShuffleSplit\n",
        "\n",
        "\n",
        "from tqdm import tqdm\n",
        "from typing import List\n",
        "\n",
        "# Load in the notebook all .py files that make our personal lib\n",
        "# This way we keep notebook code as small as possible, and only pertinent \n",
        "# to the concrete task that this notebook solves (generic and reusable code\n",
        "# goes to personal lib)\n",
        "# Also, \n",
        "!mkdir -p ./src/lib/\n",
        "!cp -r \"$LIB_PATH\"/* ./src/lib/\n",
        "\n",
        "# Now that files are loaded, we can import pieces of code\n",
        "import src.lib.core as core\n",
        "import src.lib.trainers as trainers\n",
        "import src.lib.filesystem as filesystem\n",
        "import src.lib.metrics as metrics\n",
        "import src.lib.loss_functions as loss_functions\n",
        "import src.lib.embedding_to_classifier as embedding_to_classifier\n",
        "import src.lib.sampler as sampler\n",
        "import src.lib.utils as utils\n",
        "import src.lib.data_augmentation as data_augmentation\n",
        "\n",
        "from src.lib.trainers import train_model_offline, train_model_online\n",
        "from src.lib.train_loggers import SilentLogger, TripletLoggerOffline, TripletLoggerOnline, TrainLogger, CompoundLogger, IntraClusterLogger, InterClusterLogger\n",
        "from src.lib.models import *\n",
        "from src.lib.visualizations import *\n",
        "from src.lib.models import ResNet18, LFWResNet18\n",
        "from src.lib.loss_functions import MeanTripletBatchTripletLoss, BatchHardTripletLoss, BatchAllTripletLoss\n",
        "from src.lib.embedding_to_classifier import EmbeddingToClassifier\n",
        "from src.lib.sampler import CustomSampler\n",
        "from src.lib.data_augmentation import AugmentatedDataset, LazyAugmentatedDataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "KyePreRCLI2x",
      "metadata": {
        "id": "KyePreRCLI2x"
      },
      "source": [
        "# Configuration of the logger\n",
        "\n",
        "- Here we set the configuration for all logging done \n",
        "- In lib, `logging.getLogger(\"MAIN_LOGGER\")` is used everywhere, so we get it, configure it once, and use that config everywhere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tiO4YfykLIdi",
      "metadata": {
        "id": "tiO4YfykLIdi"
      },
      "outputs": [],
      "source": [
        "# Get the logger that is used everywhere\n",
        "file_logger = logging.getLogger(\"MAIN_LOGGER\")\n",
        "\n",
        "# Configure it\n",
        "file_logger.propagate = False # Avoid propagatint to upper logger, which logs to \n",
        "                         # the console\n",
        "file_logger.setLevel(logging.DEBUG)\n",
        "formatter = logging.Formatter(\"%(asctime)s::%(levelname)s::%(funcName)s::> %(message)s\")\n",
        "file_handler = logging.FileHandler(LOGGING_FILE)\n",
        "file_handler.setFormatter(formatter)\n",
        "file_logger.addHandler(file_handler)\n",
        "\n",
        "# 'application' code\n",
        "file_logger.debug('debug message')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ePE7z9rVzCza",
      "metadata": {
        "id": "ePE7z9rVzCza"
      },
      "source": [
        "# Configuration for Weigths and Biases\n",
        "\n",
        "- We're going to use `wandb` for tracking the training of the models\n",
        "- In this section, we configure `wandb`, mainly selecting which parameters of the notebook are we going to track"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "kUP3capL7BhL",
      "metadata": {
        "id": "kUP3capL7BhL"
      },
      "outputs": [],
      "source": [
        "# Select which parameters of the notebook we're going to track in wand\n",
        "# This has to be done before `wandb.init()` in order to pass this dict to \n",
        "# `wandb.init`\n",
        "# \n",
        "# I could create a config dict in \"Global Parameters of the Notebook\" and pass it\n",
        "# rightaway. Or use directly wandb.config.SOMETHING everywhere. We don't do this \n",
        "# because of the following reasons:\n",
        "# \n",
        "# 1. We don't want to track all parameters (ie. section parameters, dir paths...)\n",
        "# 2. At this moment, we're not 100% sure that wandb is the right tool, so we are\n",
        "#    looking for loose coupling\n",
        "\n",
        "wandb_config_dict = {}\n",
        "\n",
        "\n",
        "wandb_config_dict[\"P\"] = P \n",
        "wandb_config_dict[\"K\"] = K\n",
        "wandb_config_dict[\"ONLINE_BATCH_SIZE\"] = ONLINE_BATCH_SIZE\n",
        "wandb_config_dict[\"TRAINING_EPOCHS\"] = TRAINING_EPOCHS\n",
        "wandb_config_dict[\"ONLINE_LEARNING_RATE\"] = ONLINE_LEARNING_RATE\n",
        "wandb_config_dict[\"LOGGING_ITERATIONS\"] = LOGGING_ITERATIONS\n",
        "wandb_config_dict[\"ONLINE_LOGGER_TRAIN_PERCENTAGE\"] = ONLINE_LOGGER_TRAIN_PERCENTAGE\n",
        "wandb_config_dict[\"ONLINE_LOGGER_VALIDATION_PERCENTAGE\"] = ONLINE_LOGGER_VALIDATION_PERCENTAGE\n",
        "wandb_config_dict[\"NET_MODEL\"] = NET_MODEL\n",
        "wandb_config_dict[\"HYPERPARAMETER_TUNING_EPOCHS\"] = HYPERPARAMETER_TUNING_EPOCHS\n",
        "wandb_config_dict[\"NUMBER_OF_FOLDS\"] = NUMBER_OF_FOLDS\n",
        "wandb_config_dict[\"MARGIN\"] = MARGIN\n",
        "wandb_config_dict[\"EMBEDDING_DIMENSION\"] = EMBEDDING_DIMENSION\n",
        "wandb_config_dict[\"NUMBER_NEIGHBOURS\"] = NUMBER_NEIGHBOURS\n",
        "wandb_config_dict[\"BATCH_TRIPLET_LOSS_FUNCTION\"] = BATCH_TRIPLET_LOSS_FUNCTION\n",
        "wandb_config_dict[\"USE_SOFTPLUS_LOSS\"] = USE_SOFTPLUS_LOSS\n",
        "wandb_config_dict[\"USE_GT_ZERO_MEAN_LOSS\"] = USE_GT_ZERO_MEAN_LOSS\n",
        "wandb_config_dict[\"PROFILE_TRAINING\"] = PROFILE_TRAINING"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "HjQOqYORzK28",
      "metadata": {
        "id": "HjQOqYORzK28"
      },
      "outputs": [],
      "source": [
        "# Init the wandb tracker\n",
        "# We need to do this before \n",
        "wandb.init(\n",
        "    project = WANDB_PROJECT_NAME, \n",
        "    name = WANDB_RUN_NAME, \n",
        "    config = wandb_config_dict\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "yb9CtFO17BaT",
      "metadata": {
        "id": "yb9CtFO17BaT"
      },
      "outputs": [],
      "source": [
        "# Set env variable to allow wandb to save the code of the notebook\n",
        "%env WANDB_NOTEBOOK_NAME=WANDB_RUN_NAME"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a1e330fb",
      "metadata": {
        "id": "a1e330fb"
      },
      "source": [
        "# Functions that we are going to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8eb846cd",
      "metadata": {
        "id": "8eb846cd"
      },
      "outputs": [],
      "source": [
        "def show_learning_curve(training_history: dict):\n",
        "    # Take two learning curves\n",
        "    loss = training_history['loss']\n",
        "    val_loss = training_history['val_loss']\n",
        "\n",
        "    # Move the lists to cpu, because that's what matplotlib needs\n",
        "    loss = [loss_el.cpu() for loss_el in loss]\n",
        "    val_loss = [val_loss_el.cpu() for val_loss_el in val_loss]\n",
        "    \n",
        "    # Show graphics\n",
        "    plt.plot(loss)\n",
        "    plt.plot(val_loss)\n",
        "    plt.legend(['Training loss', 'Validation loss'])\n",
        "    plt.show()\n",
        "    \n",
        "def try_to_clean_memory(): \n",
        "    torch.cuda.empty_cache() \n",
        "    gc.collect()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Ix5G-x3u7CIw",
      "metadata": {
        "id": "Ix5G-x3u7CIw"
      },
      "source": [
        "# Importing and preparing the data"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2c1847bb",
      "metadata": {
        "id": "2c1847bb"
      },
      "source": [
        "## Dataset loading\n",
        "\n",
        "- As mentioned before, we're using the `pytorch` implementation for this dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "140a184c",
      "metadata": {
        "id": "140a184c"
      },
      "outputs": [],
      "source": [
        "# Transformations that we want to apply when loading the data\n",
        "# Now we are only transforming images to tensors (pythorch only works with tensors)\n",
        "# But we can apply here some normalizations\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((250, 250)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(\n",
        "         (0.5, 0.5, 0.5),\n",
        "         (0.5, 0.5, 0.5)\n",
        "     ),\n",
        "])\n",
        "\n",
        "# Load the dataset\n",
        "# torchvision has a method to download and load the dataset\n",
        "# TODO -- look what's the difference between this dataset and LFWPairs\n",
        "train_dataset = torchvision.datasets.LFWPeople(\n",
        "    root = DATA_PATH,\n",
        "    split = \"train\",\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "test_dataset = torchvision.datasets.LFWPeople(\n",
        "    root = DATA_PATH,\n",
        "    split = \"test\",\n",
        "    download = True,\n",
        "    transform = transform,\n",
        ")\n",
        "\n",
        "# Train -> train / validation split\n",
        "train_dataset, validation_dataset = core.split_train_test(train_dataset, 0.8)\n",
        "\n",
        "\n",
        "# Train dataset and validation dataset split resulting in two `Subset` objects\n",
        "# We want to work only with `Dataset` objects, so get only that part\n",
        "train_dataset = train_dataset.dataset\n",
        "validation_dataset = validation_dataset.dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7378947c",
      "metadata": {
        "id": "7378947c"
      },
      "source": [
        "## Use our custom sampler\n",
        "\n",
        "- We want to use custom `Sampler` to do $P-K$ sampling\n",
        "    - For each minibatch, select $P$ random classes. For each selected class, select $K$ random images\n",
        "- Thus, each minibatch has a size of $P \\times K$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8425cd40",
      "metadata": {
        "id": "8425cd40"
      },
      "outputs": [],
      "source": [
        "# New dataloaders that use our custom sampler\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size = ONLINE_BATCH_SIZE,\n",
        "    num_workers = NUM_WORKERS, \n",
        "    pin_memory = True,\n",
        "    sampler = CustomSampler(P, K, train_dataset)\n",
        ")\n",
        "\n",
        "# TODO -- here I don't know if use default sampler or custom sampler\n",
        "validation_loader = torch.utils.data.DataLoader(\n",
        "    validation_dataset,\n",
        "    batch_size = ONLINE_BATCH_SIZE,\n",
        "    shuffle = True,\n",
        "    num_workers = NUM_WORKERS, \n",
        "    pin_memory = True,\n",
        ")\n",
        "\n",
        "# TODO -- here I don't know if use default sampler or custom sampler\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "  test_dataset,\n",
        "  batch_size = ONLINE_BATCH_SIZE,\n",
        "  shuffle = True,\n",
        "  num_workers = NUM_WORKERS,\n",
        "  pin_memory = True,\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e44717f5",
      "metadata": {
        "id": "e44717f5"
      },
      "source": [
        "# Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "88baf997",
      "metadata": {
        "id": "88baf997"
      },
      "source": [
        "## Show some images from the dataset\n",
        "\n",
        "Show some images with their classes, to verify that data loading was properly done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebc478f5",
      "metadata": {
        "id": "ebc478f5"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "\n",
        "    imgs_to_show = 5\n",
        "    \n",
        "    for i, (img, label) in enumerate(train_dataset):\n",
        "    \n",
        "        # Show information about the image before plotting it\n",
        "        print(f\"Img label is: {label}\")\n",
        "    \n",
        "        # Plot the img\n",
        "        # TODO -- figure out a function to do propper img showing\n",
        "        #img = img.reshape((250, 250, 3))\n",
        "        #show_img(img, color_format_range = (-1.0, 1.0))\n",
        "        plt.imshow(img.permute(1, 2, 0))\n",
        "        plt.show()\n",
        "    \n",
        "        # Stop the loop\n",
        "        if i == imgs_to_show:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dfcc5d89",
      "metadata": {
        "id": "dfcc5d89"
      },
      "source": [
        "## Show the sizes of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b6a35e9b",
      "metadata": {
        "id": "b6a35e9b"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    print(f\"Train dataset: {len(train_dataset)}\")\n",
        "    print(f\"Validation dataset: {len(validation_dataset)}\")\n",
        "    print(f\"Test dataset: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "mCRzFpR-7TIm",
      "metadata": {
        "id": "mCRzFpR-7TIm"
      },
      "source": [
        "## Show some of the triplets that we generated with our custom sampler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ZhzOTti07SDq",
      "metadata": {
        "id": "ZhzOTti07SDq"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    # We show the triplets associated to only two classes\n",
        "    triplets_to_show = 2 * K\n",
        "    \n",
        "    # TODO -- plot these images grouped in rows \n",
        "    \n",
        "    counter = 0\n",
        "    finished = False\n",
        "    for (imgs, labels) in train_loader:\n",
        "        if finished is True:\n",
        "            break\n",
        "    \n",
        "        for img, label in zip(imgs, labels):\n",
        "            print(f\"Current label is {label}\")\n",
        "            plt.imshow(img.permute(1, 2, 0))\n",
        "            plt.show()\n",
        "            counter = counter + 1\n",
        "    \n",
        "            if counter >= triplets_to_show:\n",
        "                finished = True\n",
        "                break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "eSv8e_CIFpK3",
      "metadata": {
        "id": "eSv8e_CIFpK3"
      },
      "source": [
        "## Explore how many images each class has\n",
        "\n",
        "Not all the classes have the same amount of images associated. So we plot the distribution of how many images has each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UWA4MOmGFosg",
      "metadata": {
        "id": "UWA4MOmGFosg"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    plot_how_many_images_per_class(\n",
        "        train_dataset, \n",
        "        cut = 25, \n",
        "        fig_size = (10, 10)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WNibFmW-T_oe",
      "metadata": {
        "id": "WNibFmW-T_oe"
      },
      "source": [
        "We can see that most of the classes have only one image associated. Almost all classes have less than 10 images associated. So we have to develop a mechanism to do `P-K` sampling with a big enough value of `K`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "rEnKkEqjAYBo",
      "metadata": {
        "id": "rEnKkEqjAYBo"
      },
      "source": [
        "## Let's see how many classes have at least `K` images\n",
        "\n",
        "- This is important, because it shows us how many classes can be used in training using *P-K* sampling without any modification to the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dpDwph8VAjxN",
      "metadata": {
        "id": "dpDwph8VAjxN"
      },
      "outputs": [],
      "source": [
        "def how_many_classes_have_at_least_K_images(dataset: torch.utils.data.Dataset, K: int):\n",
        "    # Get the dict with class -> number of images of that class\n",
        "    how_many_images_per_class = Counter(dataset.targets)\n",
        "\n",
        "    # Get a list of classes that have at least P images\n",
        "    # Use list comprehension with filtering over prev Counter dict \n",
        "    classes_with_at_least_K_images = [\n",
        "        curr_class \n",
        "        for curr_class, curr_value in how_many_images_per_class.items() \n",
        "        if curr_value >= K\n",
        "    ]\n",
        "\n",
        "    # Show some stats\n",
        "    n = len(classes_with_at_least_K_images)\n",
        "    print(f\"There are {n} classes with at least {K} images\")\n",
        "    print(f\"That represents the {n / len(how_many_images_per_class) * 100:.2f}% of the initial classes\")\n",
        "    \n",
        "\n",
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    how_many_classes_have_at_least_K_images(train_dataset, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Epy7BbIf7KFk",
      "metadata": {
        "id": "Epy7BbIf7KFk"
      },
      "source": [
        "# Data augmentation\n",
        "\n",
        "- As we've seen before, the main problem with this dataset is that most of the classes have only one or two images associated\n",
        "- So we're going to apply data augmentation to have at least a minimun number of images per class\n",
        "\n",
        "**Alternatives to do this**:\n",
        "\n",
        "1. Use `pytorch` transformations\n",
        "    - The problem is that this doesn't grow the size of the dataset\n",
        "    - Instead, it calls randomly the transformation for each image, at each epoch\n",
        "    - So at each epoch we have the same number of images, but sometimes transformed an sometimes not\n",
        "    - This type of data augmentation doesn't solve our problem\n",
        "2. Use `albumentation` library\n",
        "    - Same would happen, as can be seen in the [official docs](https://albumentations.ai/docs/examples/pytorch_classification/)\n",
        "3. Perform data augmentation manually\n",
        "    - As we couldn't find a ready-to-use solution, this seems the way to go\n",
        "    - Make ourselves the code to perform the data augmentation \n",
        "\n",
        "So, the **process** is going to be:\n",
        "\n",
        "1. Iterate over all classes of the dataset\n",
        "2. If that class has less than `K` images, perform data augmentation to get at least that number of images\n",
        "3. Wrap it on a `Dataset` class for ease of use"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WsqW3faJ-QNL",
      "metadata": {
        "id": "WsqW3faJ-QNL"
      },
      "source": [
        "## Augmentation of the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "xaf12x5v90Mf",
      "metadata": {
        "id": "xaf12x5v90Mf"
      },
      "outputs": [],
      "source": [
        "# Use the cached dataset\n",
        "if USE_CACHED_AUGMENTED_DATASET == True:\n",
        "    train_dataset_augmented = torch.load(AUGMENTED_DATASET_CACHE_FILE)\n",
        "\n",
        "# We have to do the data augmentation if we mark that we want to do it (section parameter)\n",
        "# Or if the cached dataset was done for other number of images (ie. for 4 when\n",
        "# now we want 32)\n",
        "if USE_CACHED_AUGMENTED_DATASET == False or train_dataset_augmented.min_number_of_images != K:\n",
        "\n",
        "    # Select the data augmentation mechanism\n",
        "    AugmentationClass = LazyAugmentatedDataset if LAZY_DATA_AUGMENTATION is True else AugmentatedDataset\n",
        "\n",
        "    train_dataset_augmented = AugmentationClass(\n",
        "        base_dataset = train_dataset,\n",
        "        min_number_of_images = K,\n",
        "    \n",
        "        # Remember that the trasformation has to be random type\n",
        "        # Otherwise, we could end with a lot of repeated images\n",
        "        transform = transforms.Compose([\n",
        "            transforms.RandomResizedCrop(size=(250, 250)),\n",
        "            transforms.RandomRotation(degrees=(0, 180)),\n",
        "            transforms.RandomAutocontrast(),\n",
        "        ])\n",
        "    \n",
        "    )\n",
        "\n",
        "    # Save the augmented dataset to cache\n",
        "    torch.save(train_dataset_augmented, AUGMENTED_DATASET_CACHE_FILE)\n",
        "\n",
        "# Now put a loader in front of the augmented dataset\n",
        "train_loader_augmented = torch.utils.data.DataLoader(\n",
        "    train_dataset_augmented,\n",
        "    batch_size = ONLINE_BATCH_SIZE,\n",
        "    num_workers = NUM_WORKERS, \n",
        "    pin_memory = True,\n",
        "    sampler = CustomSampler(P, K, train_dataset_augmented)\n",
        ")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "zsMHtZHg4Zw2",
      "metadata": {
        "id": "zsMHtZHg4Zw2"
      },
      "source": [
        "## Remove previous datasets\n",
        "\n",
        "- If we're not doing hyperparameter tuning, we don't need to hold previous dataset and dataloader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "UXOBqBqqQaHZ",
      "metadata": {
        "id": "UXOBqBqqQaHZ"
      },
      "outputs": [],
      "source": [
        "if SKIP_HYPERPARAMTER_TUNING is True:\n",
        "    # We are not using the old dataset and dataloader\n",
        "    # So delete them to try to have as much RAM as we can \n",
        "    # Otherwise, train will crash due to lack of RAM\n",
        "    del train_dataset\n",
        "    del train_loader\n",
        "    \n",
        "    try_to_clean_memory()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "XRRxfZOQ-SPq",
      "metadata": {
        "id": "XRRxfZOQ-SPq"
      },
      "source": [
        "## Repeat some basic EDA on augmented dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "-5w6XVoB-jxu",
      "metadata": {
        "id": "-5w6XVoB-jxu"
      },
      "source": [
        "### Show some images from the dataset\n",
        "\n",
        "Show some images with their classes, to verify that data loading was properly done:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "VsB4N41q-jxv",
      "metadata": {
        "id": "VsB4N41q-jxv"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    imgs_to_show = 5\n",
        "    \n",
        "    for i, (img, label) in enumerate(train_dataset_augmented):\n",
        "    \n",
        "        # Show information about the image before plotting it\n",
        "        print(f\"Img label is: {label}\")\n",
        "    \n",
        "        # Plot the img\n",
        "        # TODO -- figure out a function to do propper img showing\n",
        "        #img = img.reshape((250, 250, 3))\n",
        "        #show_img(img, color_format_range = (-1.0, 1.0))\n",
        "        plt.imshow(img.permute(1, 2, 0))\n",
        "        plt.show()\n",
        "    \n",
        "        # Stop the loop\n",
        "        if i == imgs_to_show:\n",
        "            break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "RRwDdkev-jxv",
      "metadata": {
        "id": "RRwDdkev-jxv"
      },
      "source": [
        "### Show the sizes of the datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "BAjeFwAr-jxv",
      "metadata": {
        "id": "BAjeFwAr-jxv"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    print(f\"Train dataset: {len(train_dataset_augmented)}\")\n",
        "    print(f\"Validation dataset: {len(validation_dataset)}\")\n",
        "    print(f\"Test dataset: {len(test_dataset)}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vYaPLgMd-jxw",
      "metadata": {
        "id": "vYaPLgMd-jxw"
      },
      "source": [
        "### Show some of the triplets that we generated with our custom sampler:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "tvAawNLT-jxw",
      "metadata": {
        "id": "tvAawNLT-jxw"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    # We show the triplets associated to only two classes\n",
        "    triplets_to_show = 2 * K\n",
        "    \n",
        "    # TODO -- plot these images grouped in rows \n",
        "    \n",
        "    counter = 0\n",
        "    finished = False\n",
        "    for (imgs, labels) in train_loader_augmented:\n",
        "        if finished is True:\n",
        "            break\n",
        "    \n",
        "        for img, label in zip(imgs, labels):\n",
        "            print(f\"Current label is {label}\")\n",
        "            plt.imshow(img.permute(1, 2, 0))\n",
        "            plt.show()\n",
        "            counter = counter + 1\n",
        "    \n",
        "            if counter >= triplets_to_show:\n",
        "                finished = True\n",
        "                break\n",
        "    "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "D80g8DYM-jxw",
      "metadata": {
        "id": "D80g8DYM-jxw"
      },
      "source": [
        "### Explore how many images each class has\n",
        "\n",
        "Not all the classes have the same amount of images associated. So we plot the distribution of how many images has each class:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "X_ncq8SD-jxw",
      "metadata": {
        "id": "X_ncq8SD-jxw"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    plot_how_many_images_per_class(\n",
        "        train_dataset_augmented, \n",
        "        cut = 25, \n",
        "        fig_size = (10, 10)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "GXk4wDBi-jxw",
      "metadata": {
        "id": "GXk4wDBi-jxw"
      },
      "source": [
        "We can see that most of the classes have only one image associated. Almost all classes have less than 10 images associated. So we have to develop a mechanism to do `P-K` sampling with a big enough value of `K`."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "spSDmpqi-jxx",
      "metadata": {
        "id": "spSDmpqi-jxx"
      },
      "source": [
        "### Let's see how many classes have at least `P` images\n",
        "\n",
        "- This is important, because it shows us how many classes can be used in training using *P-K* sampling without any modification to the pipeline"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "qLZXLx7N-jxx",
      "metadata": {
        "id": "qLZXLx7N-jxx"
      },
      "outputs": [],
      "source": [
        "if SKIP_EXPLORATORY_DATA_ANALYSYS is False:\n",
        "    how_many_classes_have_at_least_K_images(train_dataset_augmented, K)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "IDZBu1P-o4TM",
      "metadata": {
        "id": "IDZBu1P-o4TM"
      },
      "source": [
        "# Choose the loss function to use \n",
        "\n",
        "- We have so many combinations for loss functions that is not feasible to use one Colab section for each\n",
        "- Combinations depend on:\n",
        "    1. Batch hard vs Batch all \n",
        "    2. Classical triplet loss vs Softplus loss\n",
        "    3. All summands mean vs Only > 0 summands mean\n",
        "- This election is done in *Global Parameters of the Notebook* section"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "lU4wWC-GpHdI",
      "metadata": {
        "id": "lU4wWC-GpHdI"
      },
      "outputs": [],
      "source": [
        "batch_loss_function = None\n",
        "if BATCH_TRIPLET_LOSS_FUNCTION == \"hard\":\n",
        "    batch_loss_function = BatchHardTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean = USE_GT_ZERO_MEAN_LOSS) \n",
        "if BATCH_TRIPLET_LOSS_FUNCTION == \"all\":\n",
        "    batch_loss_function = BatchAllTripletLoss(MARGIN, use_softplus = USE_SOFTPLUS_LOSS, use_gt_than_zero_mean =  USE_GT_ZERO_MEAN_LOSS) \n",
        "\n",
        "# Sanity check\n",
        "if batch_loss_function is None:\n",
        "    raise Exception(f\"BATCH_TRIPLET_LOSS global parameter got unexpected value: {BATCH_TRIPLET_LOSS_FUNCTION}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5593607e",
      "metadata": {
        "id": "5593607e"
      },
      "source": [
        "# Hyperparameter tuning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "867bd047",
      "metadata": {
        "id": "867bd047"
      },
      "outputs": [],
      "source": [
        "# TODO -- translate to english\n",
        "# TODO -- move to lib\n",
        "def custom_cross_validation(\n",
        "        net: torch.nn.Module, \n",
        "        parameters, \n",
        "        train_dataset, \n",
        "        k\n",
        "    ):\n",
        "    \"\"\"\n",
        "    Perform k-fold cross validation \n",
        "    \"\"\"\n",
        "\n",
        "    # Definimos la forma en la que vamos a hacer el split de los folds\n",
        "    ss = ShuffleSplit(n_splits=k, test_size=0.25, random_state=RANDOM_SEED)\n",
        "    \n",
        "    # Lista en la que guardamos las perdidas encontradas en cada fold\n",
        "    losses = []\n",
        "\n",
        "    # Iteramos usando el split que nos da sklearn\n",
        "    for train_index, validation_index in ss.split(train_dataset):\n",
        "        \n",
        "        # Tenemos los indices de los elementos, asi que tomamos los dos datasets\n",
        "        # usando dichos indices\n",
        "        train_fold = [train_dataset[idx] for idx in train_index]\n",
        "        validation_fold = [train_dataset[idx] for idx in validation_index]\n",
        "\n",
        "        # Transformamos los datasets en dataloaders\n",
        "        train_loader = torch.utils.data.DataLoader(\n",
        "            train_fold,\n",
        "            batch_size = ONLINE_BATCH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "        validation_loader = torch.utils.data.DataLoader(\n",
        "            validation_fold,\n",
        "            batch_size = ONLINE_BATCH_SIZE,\n",
        "            shuffle = True,\n",
        "            num_workers = NUM_WORKERS,\n",
        "            pin_memory = True,\n",
        "        )\n",
        "\n",
        "        # Entrenamos la red\n",
        "        _ = train_model_online(\n",
        "            net = net,\n",
        "            path = os.path.join(BASE_PATH, \"tmp\"),\n",
        "            parameters = parameters,\n",
        "            train_loader = train_loader,\n",
        "            validation_loader = validation_loader,\n",
        "            name = \"SiameseNetworkOnline\",\n",
        "            logger = SilentLogger(),\n",
        "            snapshot_iterations = None\n",
        "        )\n",
        "\n",
        "        # Evaluamos la red en el fold de validacion\n",
        "        net.eval()\n",
        "        loss = metrics.calculate_mean_triplet_loss_online(net, validation_loader, parameters[\"criterion\"], 1.0)\n",
        "        loss = float(loss) # Pasamos el tensor de un unico elemento a un float simple\n",
        "\n",
        "        # AÃ±adimos el loss a nuestra lista\n",
        "        losses.append(loss)\n",
        "    \n",
        "    # Devolvemos el array en formato numpy para que sea mas comodo trabajar con ella\n",
        "    return np.array(losses)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "292f5ea4",
      "metadata": {
        "id": "292f5ea4"
      },
      "outputs": [],
      "source": [
        "# Controlamos si queremos realizar el hyperparameater tuning o no\n",
        "if SKIP_HYPERPARAMTER_TUNING is False:\n",
        "\n",
        "    # Para controlar que parametros ya hemos explorado y queremos saltar\n",
        "    already_explored_parameters = [\n",
        "        # Embedding dimension, learning rate, margin\n",
        "        (2, 0.0001, 0.01),\n",
        "        (2, 0.0001, 1),\n",
        "        (3, 0.0001),\n",
        "    ]\n",
        "\n",
        "    # Parametros que queremos mover\n",
        "    #margin_values = [0.01, 0.1, 1.0]\n",
        "    # TODO -- volver a poner todos los valores\n",
        "    margin_values = [1.0]\n",
        "    learning_rate_values = [0.0001, 0.001, 0.01]\n",
        "    embedding_dimension_values = [2, 3, 4]\n",
        "    \n",
        "    # Parametros que fijamos de antemano\n",
        "    epochs = HYPERPARAMETER_TUNING_EPOCHS\n",
        "    \n",
        "    # Llevamos la cuenta de los mejores parametros y el mejor error encontrados hasta\n",
        "    # el momento\n",
        "    best_loss = None\n",
        "    best_parameters = {\n",
        "        \"embedding_dimension\": None,\n",
        "        \"lr\": None,\n",
        "        \"margin\": None\n",
        "    }\n",
        "    \n",
        "    # Exploramos las combinaciones de parametros\n",
        "    for margin in margin_values:\n",
        "        for learning_rate in learning_rate_values:\n",
        "            for embedding_dimension in embedding_dimension_values:\n",
        "        \n",
        "                print(f\"Optimizando para margin: {margin}, lr: {learning_rate}, embedding_dim: {embedding_dimension}\")\n",
        "\n",
        "                # Comprobamos si tenemos que saltarnos el calculo de algun valor\n",
        "                # porque ya se haya hecho\n",
        "                if (embedding_dimension, learning_rate, margin) in already_explored_parameters:\n",
        "                    print(\"\\tSaltando este calculo porque ya se realizo\")\n",
        "                    continue\n",
        "                \n",
        "                # Definimos el modelo que queremos optimizar\n",
        "                net = ResNet18(embedding_dimension)\n",
        "                \n",
        "                # En este caso, al no estar trabajando con los minibatches\n",
        "                # (los usamos directamente como nos los da pytorch), no tenemos\n",
        "                # que manipular los tensores\n",
        "                net.set_permute(False)\n",
        "                \n",
        "                parameters = dict()\n",
        "                parameters[\"epochs\"] = epochs\n",
        "                parameters[\"lr\"] = learning_rate\n",
        "                parameters[\"criterion\"] = BatchHardTripletLoss(margin)\n",
        "                logger = SilentLogger()\n",
        "    \n",
        "                # Usamos nuestra propia funcion de cross validation para validar el modelo\n",
        "                losses = custom_cross_validation(net, parameters, train_dataset, k = NUMBER_OF_FOLDS) \n",
        "                print(f\"El loss conseguido es {losses.mean()}\")\n",
        "                print(\"\")\n",
        "            \n",
        "                # Comprobamos si hemos mejorado la funcion de perdida\n",
        "                # En cuyo caso, actualizamos nuestra estructura de datos y, sobre todo, mostramos\n",
        "                # por pantalla los nuevos mejores parametros\n",
        "                basic_condition = math.isnan(losses.mean()) is False             # Si es NaN no entramos al if\n",
        "                enter_condition = best_loss is None or losses.mean() < best_loss # Entramos al if si mejoramos la perdida\n",
        "                compound_condition = basic_condition and enter_condition\n",
        "                if compound_condition:\n",
        "                \n",
        "                    # Actualizamos nuestra estructura de datos\n",
        "                    best_loss = losses.mean()\n",
        "                    best_parameters = {\n",
        "                        \"embedding_dimension\": embedding_dimension,\n",
        "                        \"lr\": learning_rate,\n",
        "                        \"margin\": margin,\n",
        "                    }\n",
        "            \n",
        "                    # Mostramos el cambio encontrado\n",
        "                    print(\"==> ENCONTRADOS NUEVOS MEJORES PARAMETROS\")\n",
        "                    print(f\"Mejores parametros: {best_parameters}\")\n",
        "                    print(f\"Mejor loss: {best_loss}\")\n",
        "            \n",
        "                \n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3e1ee8ba",
      "metadata": {
        "id": "3e1ee8ba"
      },
      "source": [
        "# Training of the model"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "hwfeRAseEQKJ",
      "metadata": {
        "id": "hwfeRAseEQKJ"
      },
      "source": [
        "## Selecting the network and tweaking some parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d7a2afd9",
      "metadata": {
        "id": "d7a2afd9"
      },
      "outputs": [],
      "source": [
        "net = None\n",
        "if NET_MODEL == \"ResNet18\":\n",
        "    net = ResNet18(EMBEDDING_DIMENSION)\n",
        "elif NET_MODEL == \"LightModel\":\n",
        "    net = LightModel(EMBEDDING_DIMENSION)\n",
        "elif NET_MODEL == \"LFWResNet18\":\n",
        "    net = LFWResNet18(EMBEDDING_DIMENSION)\n",
        "else:\n",
        "    raise Exception(\"Parameter 'NET_MODEL' has not a valid value\")\n",
        "\n",
        "# The custom sampler takes care of minibatch management\n",
        "# Thus, we don't have to make manipulations on them\n",
        "net.set_permute(False)\n",
        "\n",
        "# Training parameters\n",
        "parameters = dict()\n",
        "parameters[\"epochs\"] = TRAINING_EPOCHS\n",
        "parameters[\"lr\"] = ONLINE_LEARNING_RATE\n",
        "\n",
        "# We use the loss function that depends on the global parameter BATCH_TRIPLET_LOSS_FUNCTION\n",
        "# We selected this loss func in *Choose the loss function to use* section\n",
        "parameters[\"criterion\"] = batch_loss_function\n",
        "\n",
        "print(net)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Iw4QHUvhEVH2",
      "metadata": {
        "id": "Iw4QHUvhEVH2"
      },
      "source": [
        "## Defining the loggers we want to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3sXGOrUu2OTB",
      "metadata": {
        "id": "3sXGOrUu2OTB"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define the loggers we want to use \n",
        "triplet_loss_logger = TripletLoggerOnline(\n",
        "    net = net,\n",
        "    iterations = LOGGING_ITERATIONS,\n",
        "    loss_func = parameters[\"criterion\"],\n",
        "    train_percentage = ONLINE_LOGGER_TRAIN_PERCENTAGE,\n",
        "    validation_percentage = ONLINE_LOGGER_VALIDATION_PERCENTAGE,\n",
        "    greater_than_zero = USE_GT_ZERO_MEAN_LOSS,\n",
        ")\n",
        "\n",
        "cluster_sizes_logger = IntraClusterLogger(\n",
        "    net = net,\n",
        "    iterations = LOGGING_ITERATIONS,\n",
        "    train_percentage = ONLINE_LOGGER_TRAIN_PERCENTAGE,\n",
        "    validation_percentage = ONLINE_LOGGER_VALIDATION_PERCENTAGE,\n",
        ")\n",
        "\n",
        "intercluster_metrics_logger = InterClusterLogger(\n",
        "    net = net,\n",
        "    iterations = LOGGING_ITERATIONS,\n",
        "    train_percentage = ONLINE_LOGGER_TRAIN_PERCENTAGE,\n",
        "    validation_percentage = ONLINE_LOGGER_VALIDATION_PERCENTAGE,\n",
        ")\n",
        "\n",
        "# Combine them in a single logger\n",
        "logger = CompoundLogger([\n",
        "    triplet_loss_logger,\n",
        "    cluster_sizes_logger,\n",
        "    intercluster_metrics_logger\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "WTFKWlsiEaQo",
      "metadata": {
        "id": "WTFKWlsiEaQo"
      },
      "source": [
        "## Running the training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa722a06",
      "metadata": {
        "id": "aa722a06"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "# Check if we want to skip training\n",
        "if USE_CACHED_MODEL is False:\n",
        "\n",
        "    # To measure the time it takes to train\n",
        "    ts = time.time()\n",
        "    \n",
        "    # Run the training with or without profiling\n",
        "    if PROFILE_TRAINING is True:\n",
        "        training_history = cProfile.run(\n",
        "            f\"\"\"train_model_online(\n",
        "                net = net,\n",
        "                path = os.path.join(BASE_PATH, 'tmp'),\n",
        "                parameters = parameters,\n",
        "                train_loader = train_loader_augmented,\n",
        "                validation_loader = validation_loader,\n",
        "                name = NET_MODEL,\n",
        "                logger = logger,\n",
        "                snapshot_iterations = None\n",
        "            )\"\"\",\n",
        "            PROFILE_SAVE_FILE\n",
        "        )\n",
        "\n",
        "    else:\n",
        "\n",
        "        training_history = train_model_online(\n",
        "            net = net,\n",
        "            path = os.path.join(BASE_PATH, \"tmp\"),\n",
        "            parameters = parameters,\n",
        "            train_loader = train_loader_augmented,\n",
        "            validation_loader = validation_loader,\n",
        "            name = NET_MODEL,\n",
        "            logger = logger,\n",
        "            snapshot_iterations = None\n",
        "        )\n",
        "    \n",
        "    # Compute how long it took\n",
        "    te = time.time()\n",
        "    print(f\"It took {te - ts} seconds to train\")\n",
        "    \n",
        "    # Update the model cache\n",
        "    filesystem.save_model(net, MODEL_CACHE_FOLDER, \"online_model_cached\")\n",
        "    \n",
        "# In case we skipped training, load the model from cache\n",
        "else:\n",
        "    \n",
        "    # Load the model from cache\n",
        "    net = filesystem.load_model(\n",
        "        os.path.join(MODEL_CACHE_FOLDER, \"online_model_cached\"), \n",
        "        lambda: LFWResNet18(EMBEDDING_DIMENSION)\n",
        "    )\n",
        "    \n",
        "    # Load the network in corresponding mem device (cpu -> ram, gpu -> gpu mem\n",
        "    device = core.get_device()\n",
        "    net.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0473016a-7a1b-4cd6-9c69-d9326f9f3180",
      "metadata": {
        "id": "0473016a-7a1b-4cd6-9c69-d9326f9f3180"
      },
      "outputs": [],
      "source": [
        "# From this point, we won't perform training on the model\n",
        "# So eval mode is set for better performance \n",
        "net.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3ae35995",
      "metadata": {
        "id": "3ae35995"
      },
      "outputs": [],
      "source": [
        "# Check if we have to skip this section\n",
        "if USE_CACHED_MODEL is False:\n",
        "        \n",
        "    # In the start, we have too much error that goes down very fast\n",
        "    # So we ignore first 5 values for the metric, improving legibility\n",
        "    # cut = 5\n",
        "    # training_history[\"loss\"] = training_history[\"loss\"][cut:]\n",
        "    # training_history[\"val_loss\"] = training_history[\"val_loss\"][cut:]\n",
        "    \n",
        "    show_learning_curve(training_history)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d68b8fef",
      "metadata": {
        "id": "d68b8fef"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "f6aaa8b0-37ba-4bbe-9de2-169badb66db6",
      "metadata": {
        "id": "f6aaa8b0-37ba-4bbe-9de2-169badb66db6"
      },
      "source": [
        "Show the \"criterion\" metric on test set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5210628c",
      "metadata": {
        "id": "5210628c"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    net.set_permute(False)\n",
        "    \n",
        "    core.test_model_online(net, test_loader, parameters[\"criterion\"], online = True)\n",
        "    \n",
        "    net.set_permute(True)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "583be3ea-7684-4312-b6af-4462ed77cdda",
      "metadata": {
        "id": "583be3ea-7684-4312-b6af-4462ed77cdda"
      },
      "source": [
        "Now take the classifier from the embedding and use it to compute some classification metrics:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f2e58d6f",
      "metadata": {
        "id": "f2e58d6f",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    \n",
        "    # Try to clean memory, because we can easily run out of memory\n",
        "    # This provoke the notebook to crash, and all in-memory objects to be lost\n",
        "    try_to_clean_memory()\n",
        "    \n",
        "    # With hopefully enough memory, try to convert the embedding to a classificator\n",
        "    classifier = EmbeddingToClassifier(net, k = NUMBER_NEIGHBOURS, data_loader = train_loader_augmented, embedding_dimension = EMBEDDING_DIMENSION)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a4363deb-186f-4834-8207-e54099c58abd",
      "metadata": {
        "id": "a4363deb-186f-4834-8207-e54099c58abd"
      },
      "source": [
        "We evaluate this classifier by watching how it works over a small test set. Later we take some metrics from this classifier to evaluate it more precisely.   "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "46acde43",
      "metadata": {
        "id": "46acde43"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "\n",
        "    # Shoow only `max_iterations` classifications\n",
        "    counter = 0\n",
        "    max_iterations = 20\n",
        "\n",
        "    for img, img_class in test_dataset:\n",
        "        predicted_class = classifier.predict(img)\n",
        "        print(f\"True label: {img_class}, predicted label: {predicted_class[0]}, correct: {img_class == predicted_class[0]}\")\n",
        "\n",
        "        counter += 1\n",
        "        if counter == max_iterations: break"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "58fa44a0",
      "metadata": {
        "id": "58fa44a0"
      },
      "source": [
        "# Plot of the embedding\n",
        "\n",
        "- If the dimension of the embedding is 2, then we can plot how the transformation to a classificator works:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8ab56fa9",
      "metadata": {
        "id": "8ab56fa9"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    classifier.scatter_plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "83cea66d",
      "metadata": {
        "id": "83cea66d"
      },
      "source": [
        "# Evaluating the obtained classifier \n",
        "\n",
        "- Now that we adapted our network to a classification task, we can compute some classification metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "27b4ae22",
      "metadata": {
        "id": "27b4ae22"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    try_to_clean_memory()\n",
        "    classifier.embedder.set_permute(False)\n",
        "    \n",
        "    metrics = evaluate_model(classifier, train_loader, test_loader)\n",
        "    pprint(metrics)\n",
        "    \n",
        "    classifier.embedder.set_permute(True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "a1e330fb"
      ],
      "provenance": [],
      "toc_visible": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}